<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Time Series Forecasting ¬∑ UnifiedMetrics.jl</title><meta name="title" content="Time Series Forecasting ¬∑ UnifiedMetrics.jl"/><meta property="og:title" content="Time Series Forecasting ¬∑ UnifiedMetrics.jl"/><meta property="twitter:title" content="Time Series Forecasting ¬∑ UnifiedMetrics.jl"/><meta name="description" content="Documentation for UnifiedMetrics.jl."/><meta property="og:description" content="Documentation for UnifiedMetrics.jl."/><meta property="twitter:description" content="Documentation for UnifiedMetrics.jl."/><meta property="og:url" content="https://taf-society.github.io/UnifiedMetrics.jl/time_series/"/><meta property="twitter:url" content="https://taf-society.github.io/UnifiedMetrics.jl/time_series/"/><link rel="canonical" href="https://taf-society.github.io/UnifiedMetrics.jl/time_series/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">UnifiedMetrics.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../getting_started/">Getting Started</a></li><li><a class="tocitem" href="../choosing_metrics/">Choosing the Right Metric</a></li><li class="is-active"><a class="tocitem" href>Time Series Forecasting</a><ul class="internal"><li><a class="tocitem" href="#Why-Time-Series-Metrics-Are-Different"><span>Why Time Series Metrics Are Different</span></a></li><li><a class="tocitem" href="#Metrics-at-a-Glance"><span>Metrics at a Glance</span></a></li><li><a class="tocitem" href="#Choosing-the-Right-Time-Series-Metric"><span>Choosing the Right Time Series Metric</span></a></li><li><a class="tocitem" href="#Scaled-Error-Metrics"><span>Scaled Error Metrics</span></a></li><li><a class="tocitem" href="#Bias-Detection-Metrics"><span>Bias Detection Metrics</span></a></li><li><a class="tocitem" href="#Benchmark-Comparison-Metrics"><span>Benchmark Comparison Metrics</span></a></li><li><a class="tocitem" href="#Percentage-Based-Metrics"><span>Percentage-Based Metrics</span></a></li><li><a class="tocitem" href="#Directional-Accuracy"><span>Directional Accuracy</span></a></li><li><a class="tocitem" href="#Prediction-Interval-Metrics"><span>Prediction Interval Metrics</span></a></li><li><a class="tocitem" href="#Autocorrelation-Preservation"><span>Autocorrelation Preservation</span></a></li><li><a class="tocitem" href="#Complete-Evaluation-Framework"><span>Complete Evaluation Framework</span></a></li><li><a class="tocitem" href="#Common-Pitfalls-and-Solutions"><span>Common Pitfalls and Solutions</span></a></li><li><a class="tocitem" href="#References-and-Further-Reading"><span>References and Further Reading</span></a></li></ul></li><li><span class="tocitem">Other Metrics</span><ul><li><a class="tocitem" href="../regression/">Regression</a></li><li><a class="tocitem" href="../classification/">Classification</a></li><li><a class="tocitem" href="../binary_classification/">Binary Classification</a></li><li><a class="tocitem" href="../information_retrieval/">Information Retrieval</a></li></ul></li><li><a class="tocitem" href="../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Time Series Forecasting</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Time Series Forecasting</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/taf-society/UnifiedMetrics.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands">ÔÇõ</span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/taf-society/UnifiedMetrics.jl/blob/main/docs/src/time_series.md" title="Edit source on GitHub"><span class="docs-icon fa-solid">ÔÅÑ</span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Time-Series-Forecasting-Metrics"><a class="docs-heading-anchor" href="#Time-Series-Forecasting-Metrics">Time Series Forecasting Metrics</a><a id="Time-Series-Forecasting-Metrics-1"></a><a class="docs-heading-anchor-permalink" href="#Time-Series-Forecasting-Metrics" title="Permalink"></a></h1><p>A comprehensive guide to evaluating time series forecasting models.</p><h2 id="Why-Time-Series-Metrics-Are-Different"><a class="docs-heading-anchor" href="#Why-Time-Series-Metrics-Are-Different">Why Time Series Metrics Are Different</a><a id="Why-Time-Series-Metrics-Are-Different-1"></a><a class="docs-heading-anchor-permalink" href="#Why-Time-Series-Metrics-Are-Different" title="Permalink"></a></h2><p>Time series evaluation has unique challenges that standard regression metrics don&#39;t address:</p><ol><li><strong>Scale Dependence</strong>: MAE of 10 means nothing without context - is that good or bad?</li><li><strong>Benchmark Comparison</strong>: How does your model compare to simple baselines (naive, seasonal naive)?</li><li><strong>Temporal Structure</strong>: Errors may be autocorrelated, biased, or directionally wrong</li><li><strong>Probabilistic Forecasts</strong>: Modern forecasting produces prediction intervals, not just point forecasts</li><li><strong>Multiple Horizons</strong>: Accuracy often degrades as forecast horizon increases</li></ol><p>UnifiedMetrics.jl provides 13 specialized metrics to address these challenges.</p><h2 id="Metrics-at-a-Glance"><a class="docs-heading-anchor" href="#Metrics-at-a-Glance">Metrics at a Glance</a><a id="Metrics-at-a-Glance-1"></a><a class="docs-heading-anchor-permalink" href="#Metrics-at-a-Glance" title="Permalink"></a></h2><table><tr><th style="text-align: right">Metric</th><th style="text-align: right">Category</th><th style="text-align: right">Range</th><th style="text-align: right">Key Insight</th></tr><tr><td style="text-align: right"><code>mase</code></td><td style="text-align: right">Scaled Error</td><td style="text-align: right">[0, ‚àû)</td><td style="text-align: right">Is model better than naive?</td></tr><tr><td style="text-align: right"><code>msse</code></td><td style="text-align: right">Scaled Error</td><td style="text-align: right">[0, ‚àû)</td><td style="text-align: right">Squared version of MASE</td></tr><tr><td style="text-align: right"><code>rmsse</code></td><td style="text-align: right">Scaled Error</td><td style="text-align: right">[0, ‚àû)</td><td style="text-align: right">Same scale as data</td></tr><tr><td style="text-align: right"><code>tracking_signal</code></td><td style="text-align: right">Bias</td><td style="text-align: right">(-‚àû, ‚àû)</td><td style="text-align: right">Is forecast systematically off?</td></tr><tr><td style="text-align: right"><code>forecast_bias</code></td><td style="text-align: right">Bias</td><td style="text-align: right">(-‚àû, ‚àû)</td><td style="text-align: right">Average over/under prediction</td></tr><tr><td style="text-align: right"><code>theil_u1</code></td><td style="text-align: right">Benchmark</td><td style="text-align: right">[0, 1]</td><td style="text-align: right">Normalized inequality</td></tr><tr><td style="text-align: right"><code>theil_u2</code></td><td style="text-align: right">Benchmark</td><td style="text-align: right">[0, ‚àû)</td><td style="text-align: right">Comparison to naive</td></tr><tr><td style="text-align: right"><code>wape</code></td><td style="text-align: right">Percentage</td><td style="text-align: right">[0, ‚àû)</td><td style="text-align: right">Weighted percentage error</td></tr><tr><td style="text-align: right"><code>directional_accuracy</code></td><td style="text-align: right">Direction</td><td style="text-align: right">[0, 1]</td><td style="text-align: right">Up/down prediction accuracy</td></tr><tr><td style="text-align: right"><code>coverage_probability</code></td><td style="text-align: right">Intervals</td><td style="text-align: right">[0, 1]</td><td style="text-align: right">Interval calibration</td></tr><tr><td style="text-align: right"><code>winkler_score</code></td><td style="text-align: right">Intervals</td><td style="text-align: right">[0, ‚àû)</td><td style="text-align: right">Interval sharpness + calibration</td></tr><tr><td style="text-align: right"><code>pinball_loss_series</code></td><td style="text-align: right">Quantile</td><td style="text-align: right">[0, ‚àû)</td><td style="text-align: right">Quantile forecast accuracy</td></tr><tr><td style="text-align: right"><code>autocorrelation_error</code></td><td style="text-align: right">Structure</td><td style="text-align: right">[0, ‚àû)</td><td style="text-align: right">Temporal pattern preservation</td></tr></table><hr/><h2 id="Choosing-the-Right-Time-Series-Metric"><a class="docs-heading-anchor" href="#Choosing-the-Right-Time-Series-Metric">Choosing the Right Time Series Metric</a><a id="Choosing-the-Right-Time-Series-Metric-1"></a><a class="docs-heading-anchor-permalink" href="#Choosing-the-Right-Time-Series-Metric" title="Permalink"></a></h2><h3 id="Decision-Flowchart"><a class="docs-heading-anchor" href="#Decision-Flowchart">Decision Flowchart</a><a id="Decision-Flowchart-1"></a><a class="docs-heading-anchor-permalink" href="#Decision-Flowchart" title="Permalink"></a></h3><pre><code class="nohighlight hljs">What do you need to evaluate?
‚îÇ
‚îú‚îÄ‚ñ∫ Point Forecast Accuracy
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚ñ∫ Need to compare across different series/scales?
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚ñ∫ YES ‚îÄ‚îÄ‚ñ∫ mase() or rmsse()
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚ñ∫ NO ‚îÄ‚îÄ‚ñ∫ mae() or rmse() from regression metrics
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚ñ∫ Need percentage-based reporting?
‚îÇ       ‚îÇ
‚îÇ       ‚îú‚îÄ‚ñ∫ Data has zeros? ‚îÄ‚îÄ‚ñ∫ wape()
‚îÇ       ‚îÇ
‚îÇ       ‚îî‚îÄ‚ñ∫ No zeros ‚îÄ‚îÄ‚ñ∫ mape() from regression metrics
‚îÇ
‚îú‚îÄ‚ñ∫ Forecast Bias Detection
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚ñ∫ Real-time monitoring ‚îÄ‚îÄ‚ñ∫ tracking_signal()
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚ñ∫ One-time evaluation ‚îÄ‚îÄ‚ñ∫ forecast_bias()
‚îÇ
‚îú‚îÄ‚ñ∫ Benchmark Comparison
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚ñ∫ Is model better than naive forecast? ‚îÄ‚îÄ‚ñ∫ theil_u2() or mase()
‚îÇ
‚îú‚îÄ‚ñ∫ Direction Prediction (Trading)
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚ñ∫ directional_accuracy()
‚îÇ
‚îî‚îÄ‚ñ∫ Probabilistic Forecasts
    ‚îÇ
    ‚îú‚îÄ‚ñ∫ Prediction intervals ‚îÄ‚îÄ‚ñ∫ coverage_probability() + winkler_score()
    ‚îÇ
    ‚îî‚îÄ‚ñ∫ Quantile forecasts ‚îÄ‚îÄ‚ñ∫ pinball_loss_series()</code></pre><h3 id="Metric-Selection-by-Use-Case"><a class="docs-heading-anchor" href="#Metric-Selection-by-Use-Case">Metric Selection by Use Case</a><a id="Metric-Selection-by-Use-Case-1"></a><a class="docs-heading-anchor-permalink" href="#Metric-Selection-by-Use-Case" title="Permalink"></a></h3><table><tr><th style="text-align: right">Use Case</th><th style="text-align: right">Primary Metric</th><th style="text-align: right">Secondary Metrics</th></tr><tr><td style="text-align: right">M-competition style evaluation</td><td style="text-align: right"><code>mase</code></td><td style="text-align: right"><code>rmsse</code>, <code>mape</code></td></tr><tr><td style="text-align: right">Supply chain forecasting</td><td style="text-align: right"><code>wape</code></td><td style="text-align: right"><code>mase</code>, <code>forecast_bias</code></td></tr><tr><td style="text-align: right">Demand forecasting</td><td style="text-align: right"><code>mase</code></td><td style="text-align: right"><code>tracking_signal</code>, <code>coverage_probability</code></td></tr><tr><td style="text-align: right">Financial/trading</td><td style="text-align: right"><code>directional_accuracy</code></td><td style="text-align: right"><code>theil_u2</code></td></tr><tr><td style="text-align: right">Weather forecasting</td><td style="text-align: right"><code>rmsse</code></td><td style="text-align: right"><code>coverage_probability</code>, <code>winkler_score</code></td></tr><tr><td style="text-align: right">Real-time monitoring</td><td style="text-align: right"><code>tracking_signal</code></td><td style="text-align: right"><code>forecast_bias</code></td></tr><tr><td style="text-align: right">Model selection</td><td style="text-align: right"><code>mase</code></td><td style="text-align: right"><code>theil_u2</code>, <code>winkler_score</code></td></tr></table><hr/><h2 id="Scaled-Error-Metrics"><a class="docs-heading-anchor" href="#Scaled-Error-Metrics">Scaled Error Metrics</a><a id="Scaled-Error-Metrics-1"></a><a class="docs-heading-anchor-permalink" href="#Scaled-Error-Metrics" title="Permalink"></a></h2><p>The most important innovation in time series evaluation. These metrics compare your forecast error to the error of a naive benchmark, making them <strong>scale-independent</strong> and <strong>interpretable</strong>.</p><h3 id="MASE-(Mean-Absolute-Scaled-Error)"><a class="docs-heading-anchor" href="#MASE-(Mean-Absolute-Scaled-Error)">MASE (Mean Absolute Scaled Error)</a><a id="MASE-(Mean-Absolute-Scaled-Error)-1"></a><a class="docs-heading-anchor-permalink" href="#MASE-(Mean-Absolute-Scaled-Error)" title="Permalink"></a></h3><pre><code class="language-julia hljs">mase(actual, predicted; m=1)</code></pre><p>Compute the Mean Absolute Scaled Error. See <a href="../api/#API-Reference">API Reference</a> for full documentation.</p><h4 id="Why-MASE-is-the-Gold-Standard"><a class="docs-heading-anchor" href="#Why-MASE-is-the-Gold-Standard">Why MASE is the Gold Standard</a><a id="Why-MASE-is-the-Gold-Standard-1"></a><a class="docs-heading-anchor-permalink" href="#Why-MASE-is-the-Gold-Standard" title="Permalink"></a></h4><ol><li><strong>Scale-independent</strong>: Compare forecasts across products, regions, or time periods with different scales</li><li><strong>Interpretable threshold</strong>: MASE &lt; 1 means better than naive, MASE &gt; 1 means worse</li><li><strong>Handles zeros</strong>: Unlike MAPE, works with intermittent demand</li><li><strong>Symmetric</strong>: Treats over- and under-forecasting equally</li><li><strong>Recommended</strong>: Official metric of M3 and M4 forecasting competitions</li></ol><h4 id="Understanding-the-Seasonal-Period-m"><a class="docs-heading-anchor" href="#Understanding-the-Seasonal-Period-m">Understanding the Seasonal Period <code>m</code></a><a id="Understanding-the-Seasonal-Period-m-1"></a><a class="docs-heading-anchor-permalink" href="#Understanding-the-Seasonal-Period-m" title="Permalink"></a></h4><p>The <code>m</code> parameter defines what &quot;naive forecast&quot; means:</p><table><tr><th style="text-align: right">Your Data</th><th style="text-align: right">Seasonality</th><th style="text-align: right">m Value</th><th style="text-align: right">Naive Forecast</th></tr><tr><td style="text-align: right">Daily sales</td><td style="text-align: right">Weekly pattern</td><td style="text-align: right">7</td><td style="text-align: right">Same day last week</td></tr><tr><td style="text-align: right">Daily sales</td><td style="text-align: right">No clear pattern</td><td style="text-align: right">1</td><td style="text-align: right">Yesterday&#39;s value</td></tr><tr><td style="text-align: right">Weekly data</td><td style="text-align: right">Yearly pattern</td><td style="text-align: right">52</td><td style="text-align: right">Same week last year</td></tr><tr><td style="text-align: right">Monthly data</td><td style="text-align: right">Yearly pattern</td><td style="text-align: right">12</td><td style="text-align: right">Same month last year</td></tr><tr><td style="text-align: right">Quarterly data</td><td style="text-align: right">Yearly pattern</td><td style="text-align: right">4</td><td style="text-align: right">Same quarter last year</td></tr><tr><td style="text-align: right">Hourly data</td><td style="text-align: right">Daily pattern</td><td style="text-align: right">24</td><td style="text-align: right">Same hour yesterday</td></tr></table><p><strong>Example:</strong></p><pre><code class="language-julia hljs"># Monthly retail sales with yearly seasonality
actual = [100, 95, 110, 120, 140, 160, 155, 150, 130, 115, 105, 180,  # Year 1
          105, 98, 115, 125, 145, 165, 160, 155, 135, 120, 110, 190]  # Year 2

predicted = [102, 97, 112, 118, 142, 158, 157, 152, 132, 117, 107, 178,
             107, 100, 117, 123, 147, 163, 162, 157, 137, 122, 112, 188]

# Compare to seasonal naive (same month last year)
mase(actual, predicted, m=12)  # Yearly seasonality

# Compare to simple naive (previous month)
mase(actual, predicted, m=1)   # Usually higher - seasonal naive is a tougher benchmark</code></pre><h4 id="MASE-Interpretation-Guide"><a class="docs-heading-anchor" href="#MASE-Interpretation-Guide">MASE Interpretation Guide</a><a id="MASE-Interpretation-Guide-1"></a><a class="docs-heading-anchor-permalink" href="#MASE-Interpretation-Guide" title="Permalink"></a></h4><table><tr><th style="text-align: right">MASE Value</th><th style="text-align: right">Interpretation</th><th style="text-align: right">Action</th></tr><tr><td style="text-align: right">&lt; 0.5</td><td style="text-align: right">Excellent</td><td style="text-align: right">Model is production-ready</td></tr><tr><td style="text-align: right">0.5 - 0.8</td><td style="text-align: right">Good</td><td style="text-align: right">Model adds significant value</td></tr><tr><td style="text-align: right">0.8 - 1.0</td><td style="text-align: right">Acceptable</td><td style="text-align: right">Model slightly beats naive</td></tr><tr><td style="text-align: right">1.0</td><td style="text-align: right">Break-even</td><td style="text-align: right">Model equals naive benchmark</td></tr><tr><td style="text-align: right">1.0 - 1.5</td><td style="text-align: right">Poor</td><td style="text-align: right">Model worse than naive</td></tr><tr><td style="text-align: right">&gt; 1.5</td><td style="text-align: right">Very Poor</td><td style="text-align: right">Investigate model issues</td></tr></table><h3 id="MSSE-and-RMSSE"><a class="docs-heading-anchor" href="#MSSE-and-RMSSE">MSSE and RMSSE</a><a id="MSSE-and-RMSSE-1"></a><a class="docs-heading-anchor-permalink" href="#MSSE-and-RMSSE" title="Permalink"></a></h3><pre><code class="language-julia hljs">msse(actual, predicted; m=1)
rmsse(actual, predicted; m=1)</code></pre><p>Squared scaled error metrics. See <a href="../api/#API-Reference">API Reference</a> for full documentation.</p><h4 id="When-to-Use-RMSSE-vs-MASE"><a class="docs-heading-anchor" href="#When-to-Use-RMSSE-vs-MASE">When to Use RMSSE vs MASE</a><a id="When-to-Use-RMSSE-vs-MASE-1"></a><a class="docs-heading-anchor-permalink" href="#When-to-Use-RMSSE-vs-MASE" title="Permalink"></a></h4><ul><li><strong>RMSSE</strong>: Penalizes large errors more heavily (like RMSE vs MAE)</li><li><strong>MASE</strong>: More robust to outliers</li><li><strong>M5 competition</strong> used RMSSE as the primary metric</li></ul><pre><code class="language-julia hljs">actual = [100, 110, 105, 200, 120]  # Note: 200 is an outlier
predicted = [102, 108, 107, 150, 118]

mase(actual, predicted)   # Less affected by the large error at position 4
rmsse(actual, predicted)  # More affected by the large error</code></pre><hr/><h2 id="Bias-Detection-Metrics"><a class="docs-heading-anchor" href="#Bias-Detection-Metrics">Bias Detection Metrics</a><a id="Bias-Detection-Metrics-1"></a><a class="docs-heading-anchor-permalink" href="#Bias-Detection-Metrics" title="Permalink"></a></h2><p>Systematic bias is a common problem in forecasting. A model might have good overall accuracy but consistently over- or under-predict.</p><h3 id="Tracking-Signal"><a class="docs-heading-anchor" href="#Tracking-Signal">Tracking Signal</a><a id="Tracking-Signal-1"></a><a class="docs-heading-anchor-permalink" href="#Tracking-Signal" title="Permalink"></a></h3><pre><code class="language-julia hljs">tracking_signal(actual, predicted)</code></pre><p>Monitor forecast bias over time. See <a href="../api/#API-Reference">API Reference</a> for full documentation.</p><h4 id="Real-Time-Bias-Monitoring"><a class="docs-heading-anchor" href="#Real-Time-Bias-Monitoring">Real-Time Bias Monitoring</a><a id="Real-Time-Bias-Monitoring-1"></a><a class="docs-heading-anchor-permalink" href="#Real-Time-Bias-Monitoring" title="Permalink"></a></h4><p>The tracking signal is designed for <strong>continuous monitoring</strong> of forecast performance:</p><pre><code class="language-julia hljs"># Monitor forecast bias over time
function monitor_forecast(actual_stream, predicted_stream)
    for t in eachindex(actual_stream)
        actual_so_far = actual_stream[1:t]
        predicted_so_far = predicted_stream[1:t]

        ts = tracking_signal(actual_so_far, predicted_so_far)

        if abs(ts) &gt; 4
            println(&quot;‚ö†Ô∏è  Period $t: Tracking signal = $(round(ts, digits=2))&quot;)
            if ts &gt; 0
                println(&quot;   Model is under-forecasting. Consider adjusting upward.&quot;)
            else
                println(&quot;   Model is over-forecasting. Consider adjusting downward.&quot;)
            end
        end
    end
end</code></pre><h4 id="Control-Chart-Interpretation"><a class="docs-heading-anchor" href="#Control-Chart-Interpretation">Control Chart Interpretation</a><a id="Control-Chart-Interpretation-1"></a><a class="docs-heading-anchor-permalink" href="#Control-Chart-Interpretation" title="Permalink"></a></h4><table><tr><th style="text-align: right">Tracking Signal</th><th style="text-align: right">Status</th><th style="text-align: right">Action</th></tr><tr><td style="text-align: right">-4 to +4</td><td style="text-align: right">In control</td><td style="text-align: right">Continue monitoring</td></tr><tr><td style="text-align: right">¬±4 to ¬±6</td><td style="text-align: right">Warning</td><td style="text-align: right">Investigate recent forecasts</td></tr><tr><td style="text-align: right">Beyond ¬±6</td><td style="text-align: right">Out of control</td><td style="text-align: right">Recalibrate model immediately</td></tr></table><h3 id="Forecast-Bias"><a class="docs-heading-anchor" href="#Forecast-Bias">Forecast Bias</a><a id="Forecast-Bias-1"></a><a class="docs-heading-anchor-permalink" href="#Forecast-Bias" title="Permalink"></a></h3><pre><code class="language-julia hljs">forecast_bias(actual, predicted)</code></pre><p>Compute the average forecast error. See <a href="../api/#API-Reference">API Reference</a> for full documentation.</p><h4 id="Bias-vs-Tracking-Signal"><a class="docs-heading-anchor" href="#Bias-vs-Tracking-Signal">Bias vs Tracking Signal</a><a id="Bias-vs-Tracking-Signal-1"></a><a class="docs-heading-anchor-permalink" href="#Bias-vs-Tracking-Signal" title="Permalink"></a></h4><table><tr><th style="text-align: right">Metric</th><th style="text-align: right">Use Case</th><th style="text-align: right">Output</th></tr><tr><td style="text-align: right"><code>forecast_bias</code></td><td style="text-align: right">One-time evaluation</td><td style="text-align: right">Average error (in original units)</td></tr><tr><td style="text-align: right"><code>tracking_signal</code></td><td style="text-align: right">Continuous monitoring</td><td style="text-align: right">Normalized ratio (unitless)</td></tr></table><pre><code class="language-julia hljs">actual = [100, 110, 105, 115, 120]
predicted = [95, 105, 100, 110, 115]  # Consistently under-predicting by ~5

forecast_bias(actual, predicted)    # Returns 5.0 (average under-prediction)
tracking_signal(actual, predicted)  # Returns ~5.0 (normalized, indicates bias)</code></pre><hr/><h2 id="Benchmark-Comparison-Metrics"><a class="docs-heading-anchor" href="#Benchmark-Comparison-Metrics">Benchmark Comparison Metrics</a><a id="Benchmark-Comparison-Metrics-1"></a><a class="docs-heading-anchor-permalink" href="#Benchmark-Comparison-Metrics" title="Permalink"></a></h2><h3 id="Theil&#39;s-U-Statistics"><a class="docs-heading-anchor" href="#Theil&#39;s-U-Statistics">Theil&#39;s U Statistics</a><a id="Theil&#39;s-U-Statistics-1"></a><a class="docs-heading-anchor-permalink" href="#Theil&#39;s-U-Statistics" title="Permalink"></a></h3><pre><code class="language-julia hljs">theil_u1(actual, predicted)
theil_u2(actual, predicted; m=1)</code></pre><p>Benchmark comparison metrics. See <a href="../api/#API-Reference">API Reference</a> for full documentation.</p><h4 id="Understanding-Theil&#39;s-U1-vs-U2"><a class="docs-heading-anchor" href="#Understanding-Theil&#39;s-U1-vs-U2">Understanding Theil&#39;s U1 vs U2</a><a id="Understanding-Theil&#39;s-U1-vs-U2-1"></a><a class="docs-heading-anchor-permalink" href="#Understanding-Theil&#39;s-U1-vs-U2" title="Permalink"></a></h4><table><tr><th style="text-align: right">Statistic</th><th style="text-align: right">Range</th><th style="text-align: right">Interpretation</th></tr><tr><td style="text-align: right"><strong>U1</strong></td><td style="text-align: right">[0, 1]</td><td style="text-align: right">0 = perfect, 1 = worst possible</td></tr><tr><td style="text-align: right"><strong>U2</strong></td><td style="text-align: right">[0, ‚àû)</td><td style="text-align: right">&lt; 1 = better than naive, &gt; 1 = worse than naive</td></tr></table><p><strong>U2 is more commonly used</strong> because it directly answers: &quot;Is my model better than just using the last value?&quot;</p><pre><code class="language-julia hljs">actual = [100, 110, 105, 115, 120, 125]
predicted = [98, 108, 107, 113, 118, 123]

# Is this forecast better than naive?
u2 = theil_u2(actual, predicted)
println(&quot;Theil U2: $u2&quot;)
println(u2 &lt; 1 ? &quot;Model beats naive forecast&quot; : &quot;Naive forecast is better&quot;)</code></pre><hr/><h2 id="Percentage-Based-Metrics"><a class="docs-heading-anchor" href="#Percentage-Based-Metrics">Percentage-Based Metrics</a><a id="Percentage-Based-Metrics-1"></a><a class="docs-heading-anchor-permalink" href="#Percentage-Based-Metrics" title="Permalink"></a></h2><h3 id="WAPE-(Weighted-Absolute-Percentage-Error)"><a class="docs-heading-anchor" href="#WAPE-(Weighted-Absolute-Percentage-Error)">WAPE (Weighted Absolute Percentage Error)</a><a id="WAPE-(Weighted-Absolute-Percentage-Error)-1"></a><a class="docs-heading-anchor-permalink" href="#WAPE-(Weighted-Absolute-Percentage-Error)" title="Permalink"></a></h3><pre><code class="language-julia hljs">wape(actual, predicted)</code></pre><p>Weighted percentage error metric. See <a href="../api/#API-Reference">API Reference</a> for full documentation.</p><h4 id="WAPE-vs-MAPE"><a class="docs-heading-anchor" href="#WAPE-vs-MAPE">WAPE vs MAPE</a><a id="WAPE-vs-MAPE-1"></a><a class="docs-heading-anchor-permalink" href="#WAPE-vs-MAPE" title="Permalink"></a></h4><table><tr><th style="text-align: right">Metric</th><th style="text-align: right">Formula</th><th style="text-align: right">Handles Zeros?</th><th style="text-align: right">Weighting</th></tr><tr><td style="text-align: right">MAPE</td><td style="text-align: right">mean(|error| / |actual|)</td><td style="text-align: right">No (undefined)</td><td style="text-align: right">Equal weight</td></tr><tr><td style="text-align: right">WAPE</td><td style="text-align: right">sum(|error|) / sum(|actual|)</td><td style="text-align: right">Yes</td><td style="text-align: right">Weighted by actual</td></tr></table><p><strong>WAPE is preferred for:</strong></p><ul><li>Intermittent demand (many zeros)</li><li>Aggregated reporting (total error as % of total actual)</li><li>Supply chain metrics</li></ul><pre><code class="language-julia hljs"># Intermittent demand with zeros
actual = [0, 10, 0, 0, 20, 5, 0, 15]
predicted = [1, 8, 2, 0, 18, 6, 1, 14]

# MAPE would be undefined due to zeros
# mape(actual, predicted)  # Don&#39;t use!

# WAPE works fine
wape(actual, predicted)  # Returns meaningful percentage</code></pre><hr/><h2 id="Directional-Accuracy"><a class="docs-heading-anchor" href="#Directional-Accuracy">Directional Accuracy</a><a id="Directional-Accuracy-1"></a><a class="docs-heading-anchor-permalink" href="#Directional-Accuracy" title="Permalink"></a></h2><pre><code class="language-julia hljs">directional_accuracy(actual, predicted)</code></pre><p>Measures how often the model predicts the correct direction of change. See <a href="../api/#API-Reference">API Reference</a> for full documentation.</p><h4 id="When-Direction-Matters-More-Than-Magnitude"><a class="docs-heading-anchor" href="#When-Direction-Matters-More-Than-Magnitude">When Direction Matters More Than Magnitude</a><a id="When-Direction-Matters-More-Than-Magnitude-1"></a><a class="docs-heading-anchor-permalink" href="#When-Direction-Matters-More-Than-Magnitude" title="Permalink"></a></h4><p>In many applications, predicting the <strong>direction</strong> of change is more valuable than predicting the exact value:</p><ul><li><strong>Trading</strong>: Buy/sell signals depend on up/down prediction</li><li><strong>Inventory</strong>: Increase/decrease stock based on demand direction</li><li><strong>Capacity planning</strong>: Scale up/down based on trend direction</li></ul><pre><code class="language-julia hljs"># Stock price forecasting
actual_prices = [100.0, 102.0, 101.0, 103.0, 102.5, 104.0, 103.0, 105.0]
predicted_prices = [99.0, 101.5, 102.0, 102.5, 103.0, 103.5, 104.0, 104.5]

# MAE might look good...
mae(actual_prices, predicted_prices)  # ~1.0

# But what about direction?
da = directional_accuracy(actual_prices, predicted_prices)
println(&quot;Directional Accuracy: $(round(da * 100, digits=1))%&quot;)
println(da &gt; 0.5 ? &quot;Model has predictive value for direction&quot; : &quot;Model fails to predict direction&quot;)</code></pre><h4 id="Directional-Accuracy-Benchmarks"><a class="docs-heading-anchor" href="#Directional-Accuracy-Benchmarks">Directional Accuracy Benchmarks</a><a id="Directional-Accuracy-Benchmarks-1"></a><a class="docs-heading-anchor-permalink" href="#Directional-Accuracy-Benchmarks" title="Permalink"></a></h4><table><tr><th style="text-align: right">DA Value</th><th style="text-align: right">Interpretation</th></tr><tr><td style="text-align: right">&gt; 60%</td><td style="text-align: right">Good directional forecasting</td></tr><tr><td style="text-align: right">50-60%</td><td style="text-align: right">Marginal predictive value</td></tr><tr><td style="text-align: right">~50%</td><td style="text-align: right">No better than coin flip</td></tr><tr><td style="text-align: right">&lt; 50%</td><td style="text-align: right">Worse than random (consider inverting)</td></tr></table><hr/><h2 id="Prediction-Interval-Metrics"><a class="docs-heading-anchor" href="#Prediction-Interval-Metrics">Prediction Interval Metrics</a><a id="Prediction-Interval-Metrics-1"></a><a class="docs-heading-anchor-permalink" href="#Prediction-Interval-Metrics" title="Permalink"></a></h2><p>Modern forecasting produces <strong>probabilistic forecasts</strong> with prediction intervals, not just point predictions. These metrics evaluate interval quality.</p><h3 id="Coverage-Probability"><a class="docs-heading-anchor" href="#Coverage-Probability">Coverage Probability</a><a id="Coverage-Probability-1"></a><a class="docs-heading-anchor-permalink" href="#Coverage-Probability" title="Permalink"></a></h3><pre><code class="language-julia hljs">coverage_probability(actual, lower, upper)</code></pre><p>Compute the proportion of actual values within prediction intervals. See <a href="../api/#API-Reference">API Reference</a> for full documentation.</p><h4 id="Calibration-Assessment"><a class="docs-heading-anchor" href="#Calibration-Assessment">Calibration Assessment</a><a id="Calibration-Assessment-1"></a><a class="docs-heading-anchor-permalink" href="#Calibration-Assessment" title="Permalink"></a></h4><p>A well-calibrated 95% prediction interval should contain the actual value ~95% of the time:</p><pre><code class="language-julia hljs">actual = [100, 110, 105, 115, 120, 125, 130, 128, 135, 140]

# Your model&#39;s 95% prediction intervals
lower_95 = [92, 102, 97, 107, 112, 117, 122, 120, 127, 132]
upper_95 = [108, 118, 113, 123, 128, 133, 138, 136, 143, 148]

coverage = coverage_probability(actual, lower_95, upper_95)
println(&quot;95% Interval Coverage: $(round(coverage * 100, digits=1))%&quot;)

if coverage &lt; 0.90
    println(&quot;‚ö†Ô∏è  Under-coverage: Intervals too narrow&quot;)
elseif coverage &gt; 0.99
    println(&quot;‚ö†Ô∏è  Over-coverage: Intervals too wide (but valid)&quot;)
else
    println(&quot;‚úì Well-calibrated&quot;)
end</code></pre><h3 id="Winkler-Score"><a class="docs-heading-anchor" href="#Winkler-Score">Winkler Score</a><a id="Winkler-Score-1"></a><a class="docs-heading-anchor-permalink" href="#Winkler-Score" title="Permalink"></a></h3><pre><code class="language-julia hljs">winkler_score(actual, lower, upper; alpha=0.05)</code></pre><p>Evaluate prediction intervals for sharpness and calibration. See <a href="../api/#API-Reference">API Reference</a> for full documentation.</p><h4 id="Why-Winkler-Score?"><a class="docs-heading-anchor" href="#Why-Winkler-Score?">Why Winkler Score?</a><a id="Why-Winkler-Score?-1"></a><a class="docs-heading-anchor-permalink" href="#Why-Winkler-Score?" title="Permalink"></a></h4><p>Coverage alone doesn&#39;t tell the whole story. Two models can have the same coverage but different interval widths:</p><ul><li><strong>Model A</strong>: 95% coverage with wide intervals (less useful)</li><li><strong>Model B</strong>: 95% coverage with narrow intervals (more useful)</li></ul><p>Winkler score rewards <strong>sharp</strong> (narrow) intervals while penalizing <strong>miscoverage</strong>:</p><pre><code class="language-julia hljs">actual = [100, 110, 105]

# Model A: Wide intervals (always covers, but not useful)
lower_a = [80, 90, 85]
upper_a = [120, 130, 125]

# Model B: Narrow intervals (same coverage, more useful)
lower_b = [95, 105, 100]
upper_b = [105, 115, 110]

# Both have 100% coverage
coverage_probability(actual, lower_a, upper_a)  # 1.0
coverage_probability(actual, lower_b, upper_b)  # 1.0

# But Winkler score prefers narrower intervals
winkler_score(actual, lower_a, upper_a, alpha=0.05)  # Higher (worse)
winkler_score(actual, lower_b, upper_b, alpha=0.05)  # Lower (better)</code></pre><h3 id="Pinball-Loss-(Quantile-Loss)"><a class="docs-heading-anchor" href="#Pinball-Loss-(Quantile-Loss)">Pinball Loss (Quantile Loss)</a><a id="Pinball-Loss-(Quantile-Loss)-1"></a><a class="docs-heading-anchor-permalink" href="#Pinball-Loss-(Quantile-Loss)" title="Permalink"></a></h3><pre><code class="language-julia hljs">pinball_loss_series(actual, predicted; quantile=0.5)</code></pre><p>Evaluate quantile forecasts. See <a href="../api/#API-Reference">API Reference</a> for full documentation.</p><h4 id="Evaluating-Quantile-Forecasts"><a class="docs-heading-anchor" href="#Evaluating-Quantile-Forecasts">Evaluating Quantile Forecasts</a><a id="Evaluating-Quantile-Forecasts-1"></a><a class="docs-heading-anchor-permalink" href="#Evaluating-Quantile-Forecasts" title="Permalink"></a></h4><p>For probabilistic forecasts that output multiple quantiles:</p><pre><code class="language-julia hljs">actual = [100, 110, 105, 115, 120]

# Forecasts at different quantiles
forecast_p10 = [85, 95, 90, 100, 105]    # 10th percentile
forecast_p50 = [98, 108, 103, 113, 118]  # Median
forecast_p90 = [112, 122, 117, 127, 132] # 90th percentile

# Evaluate each quantile
for (q, forecast) in [(0.1, forecast_p10), (0.5, forecast_p50), (0.9, forecast_p90)]
    loss = pinball_loss_series(actual, forecast, quantile=q)
    println(&quot;P$(Int(q*100)) Pinball Loss: $(round(loss, digits=3))&quot;)
end</code></pre><hr/><h2 id="Autocorrelation-Preservation"><a class="docs-heading-anchor" href="#Autocorrelation-Preservation">Autocorrelation Preservation</a><a id="Autocorrelation-Preservation-1"></a><a class="docs-heading-anchor-permalink" href="#Autocorrelation-Preservation" title="Permalink"></a></h2><pre><code class="language-julia hljs">autocorrelation_error(actual, predicted; max_lag=10)</code></pre><p>Measure how well the forecast preserves the temporal structure. See <a href="../api/#API-Reference">API Reference</a> for full documentation.</p><h4 id="When-Temporal-Structure-Matters"><a class="docs-heading-anchor" href="#When-Temporal-Structure-Matters">When Temporal Structure Matters</a><a id="When-Temporal-Structure-Matters-1"></a><a class="docs-heading-anchor-permalink" href="#When-Temporal-Structure-Matters" title="Permalink"></a></h4><p>Some applications require forecasts that preserve the <strong>statistical properties</strong> of the original series:</p><ul><li>Simulation and scenario generation</li><li>Synthetic data for testing</li><li>Risk modeling (preserving volatility clustering)</li></ul><pre><code class="language-julia hljs"># Original series has strong autocorrelation
actual = cumsum(randn(100))  # Random walk

# Good forecast preserves autocorrelation structure
good_forecast = actual .+ randn(100) * 0.5  # Small noise

# Bad forecast destroys autocorrelation
bad_forecast = shuffle(actual)  # Shuffled - no temporal structure

autocorrelation_error(actual, good_forecast, max_lag=10)  # Low
autocorrelation_error(actual, bad_forecast, max_lag=10)   # High</code></pre><hr/><h2 id="Complete-Evaluation-Framework"><a class="docs-heading-anchor" href="#Complete-Evaluation-Framework">Complete Evaluation Framework</a><a id="Complete-Evaluation-Framework-1"></a><a class="docs-heading-anchor-permalink" href="#Complete-Evaluation-Framework" title="Permalink"></a></h2><h3 id="Recommended-Evaluation-Protocol"><a class="docs-heading-anchor" href="#Recommended-Evaluation-Protocol">Recommended Evaluation Protocol</a><a id="Recommended-Evaluation-Protocol-1"></a><a class="docs-heading-anchor-permalink" href="#Recommended-Evaluation-Protocol" title="Permalink"></a></h3><p>For comprehensive time series model evaluation, use this framework:</p><pre><code class="language-julia hljs">using UnifiedMetrics

function evaluate_forecast(actual, predicted, lower, upper; m=1, alpha=0.05)
    println(&quot;=&quot; ^ 60)
    println(&quot;TIME SERIES FORECAST EVALUATION REPORT&quot;)
    println(&quot;=&quot; ^ 60)

    # 1. Point Forecast Accuracy
    println(&quot;\nüìä POINT FORECAST ACCURACY&quot;)
    println(&quot;-&quot; ^ 40)
    println(&quot;MAE:  $(round(mae(actual, predicted), digits=3))&quot;)
    println(&quot;RMSE: $(round(rmse(actual, predicted), digits=3))&quot;)
    println(&quot;MAPE: $(round(mape(actual, predicted) * 100, digits=2))%&quot;)
    println(&quot;WAPE: $(round(wape(actual, predicted) * 100, digits=2))%&quot;)

    # 2. Scale-Independent Metrics
    println(&quot;\nüìè SCALE-INDEPENDENT METRICS&quot;)
    println(&quot;-&quot; ^ 40)
    m_val = mase(actual, predicted, m=m)
    println(&quot;MASE (m=$m):  $(round(m_val, digits=3))&quot;)
    println(&quot;RMSSE (m=$m): $(round(rmsse(actual, predicted, m=m), digits=3))&quot;)
    println(&quot;Theil U2:     $(round(theil_u2(actual, predicted, m=m), digits=3))&quot;)

    if m_val &lt; 1
        println(&quot;‚úì Model outperforms naive forecast&quot;)
    else
        println(&quot;‚ö† Model underperforms naive forecast&quot;)
    end

    # 3. Bias Analysis
    println(&quot;\nüéØ BIAS ANALYSIS&quot;)
    println(&quot;-&quot; ^ 40)
    fb = forecast_bias(actual, predicted)
    ts = tracking_signal(actual, predicted)
    println(&quot;Forecast Bias:    $(round(fb, digits=3))&quot;)
    println(&quot;Tracking Signal:  $(round(ts, digits=3))&quot;)

    if abs(ts) &gt; 4
        println(&quot;‚ö† Systematic bias detected!&quot;)
    else
        println(&quot;‚úì No significant bias&quot;)
    end

    # 4. Directional Accuracy
    println(&quot;\n‚ÜóÔ∏è DIRECTIONAL ACCURACY&quot;)
    println(&quot;-&quot; ^ 40)
    da = directional_accuracy(actual, predicted)
    println(&quot;Direction Accuracy: $(round(da * 100, digits=1))%&quot;)

    # 5. Prediction Intervals (if provided)
    if !isnothing(lower) &amp;&amp; !isnothing(upper)
        println(&quot;\nüìà PREDICTION INTERVAL QUALITY&quot;)
        println(&quot;-&quot; ^ 40)
        cov = coverage_probability(actual, lower, upper)
        wink = winkler_score(actual, lower, upper, alpha=alpha)
        expected_cov = 1 - alpha

        println(&quot;Expected Coverage: $(round(expected_cov * 100, digits=1))%&quot;)
        println(&quot;Actual Coverage:   $(round(cov * 100, digits=1))%&quot;)
        println(&quot;Winkler Score:     $(round(wink, digits=3))&quot;)

        if abs(cov - expected_cov) &lt; 0.05
            println(&quot;‚úì Intervals well-calibrated&quot;)
        elseif cov &lt; expected_cov
            println(&quot;‚ö† Under-coverage: intervals too narrow&quot;)
        else
            println(&quot;‚ö† Over-coverage: intervals too wide&quot;)
        end
    end

    println(&quot;\n&quot; * &quot;=&quot; ^ 60)
end

# Example usage
actual = [100.0, 110.0, 105.0, 115.0, 120.0, 125.0, 130.0, 128.0]
predicted = [98.0, 108.0, 107.0, 113.0, 118.0, 123.0, 128.0, 126.0]
lower = [90.0, 100.0, 99.0, 105.0, 110.0, 115.0, 120.0, 118.0]
upper = [106.0, 116.0, 115.0, 121.0, 126.0, 131.0, 136.0, 134.0]

evaluate_forecast(actual, predicted, lower, upper, m=1, alpha=0.05)</code></pre><h3 id="Multi-Series-Comparison"><a class="docs-heading-anchor" href="#Multi-Series-Comparison">Multi-Series Comparison</a><a id="Multi-Series-Comparison-1"></a><a class="docs-heading-anchor-permalink" href="#Multi-Series-Comparison" title="Permalink"></a></h3><p>When comparing forecasts across multiple time series:</p><pre><code class="language-julia hljs">function compare_models_across_series(series_data, models)
    results = Dict{String, Vector{Float64}}()

    for model_name in keys(models)
        results[model_name] = Float64[]
    end

    for (actual, model_forecasts) in series_data
        for (model_name, predicted) in model_forecasts
            push!(results[model_name], mase(actual, predicted))
        end
    end

    println(&quot;Model Comparison (MASE)&quot;)
    println(&quot;-&quot; ^ 40)
    for (model_name, mase_values) in results
        avg_mase = mean(mase_values)
        println(&quot;$model_name: $(round(avg_mase, digits=3)) (avg across $(length(mase_values)) series)&quot;)
    end
end</code></pre><hr/><h2 id="Common-Pitfalls-and-Solutions"><a class="docs-heading-anchor" href="#Common-Pitfalls-and-Solutions">Common Pitfalls and Solutions</a><a id="Common-Pitfalls-and-Solutions-1"></a><a class="docs-heading-anchor-permalink" href="#Common-Pitfalls-and-Solutions" title="Permalink"></a></h2><h3 id="Pitfall-1:-Using-MAPE-with-Zeros"><a class="docs-heading-anchor" href="#Pitfall-1:-Using-MAPE-with-Zeros">Pitfall 1: Using MAPE with Zeros</a><a id="Pitfall-1:-Using-MAPE-with-Zeros-1"></a><a class="docs-heading-anchor-permalink" href="#Pitfall-1:-Using-MAPE-with-Zeros" title="Permalink"></a></h3><p><strong>Problem</strong>: MAPE is undefined when actual values are zero (common in intermittent demand).</p><p><strong>Solution</strong>: Use WAPE or MASE instead.</p><pre><code class="language-julia hljs">actual = [0, 10, 0, 5, 0, 20]  # Intermittent demand
predicted = [1, 9, 1, 4, 1, 19]

# Don&#39;t do this:
# mape(actual, predicted)  # Returns Inf or NaN

# Do this instead:
wape(actual, predicted)
mase(actual, predicted)</code></pre><h3 id="Pitfall-2:-Ignoring-Seasonality-in-MASE"><a class="docs-heading-anchor" href="#Pitfall-2:-Ignoring-Seasonality-in-MASE">Pitfall 2: Ignoring Seasonality in MASE</a><a id="Pitfall-2:-Ignoring-Seasonality-in-MASE-1"></a><a class="docs-heading-anchor-permalink" href="#Pitfall-2:-Ignoring-Seasonality-in-MASE" title="Permalink"></a></h3><p><strong>Problem</strong>: Using m=1 when data has seasonality makes the benchmark too easy to beat.</p><p><strong>Solution</strong>: Set m to match your data&#39;s seasonal period.</p><pre><code class="language-julia hljs"># Monthly data with yearly seasonality
actual = repeat([100, 80, 90, 110, 130, 150, 160, 155, 140, 120, 100, 180], 2)
predicted = actual .+ randn(24) * 5

# This makes naive look bad (comparing to previous month)
mase(actual, predicted, m=1)  # Artificially low

# This is the correct comparison (same month last year)
mase(actual, predicted, m=12)  # More realistic assessment</code></pre><h3 id="Pitfall-3:-Only-Evaluating-Point-Forecasts"><a class="docs-heading-anchor" href="#Pitfall-3:-Only-Evaluating-Point-Forecasts">Pitfall 3: Only Evaluating Point Forecasts</a><a id="Pitfall-3:-Only-Evaluating-Point-Forecasts-1"></a><a class="docs-heading-anchor-permalink" href="#Pitfall-3:-Only-Evaluating-Point-Forecasts" title="Permalink"></a></h3><p><strong>Problem</strong>: Ignoring prediction intervals misses important information about forecast uncertainty.</p><p><strong>Solution</strong>: Always evaluate both point accuracy and interval quality.</p><pre><code class="language-julia hljs"># A model with great point accuracy but terrible intervals
actual = [100, 110, 105, 115, 120]
predicted = [100, 110, 105, 115, 120]  # Perfect point forecast!
lower = [99, 109, 104, 114, 119]       # Intervals way too narrow
upper = [101, 111, 106, 116, 121]

mae(actual, predicted)  # 0.0 - Perfect!
coverage_probability(actual, lower, upper)  # May be &lt; 0.95 - Problem!</code></pre><h3 id="Pitfall-4:-Not-Monitoring-for-Bias"><a class="docs-heading-anchor" href="#Pitfall-4:-Not-Monitoring-for-Bias">Pitfall 4: Not Monitoring for Bias</a><a id="Pitfall-4:-Not-Monitoring-for-Bias-1"></a><a class="docs-heading-anchor-permalink" href="#Pitfall-4:-Not-Monitoring-for-Bias" title="Permalink"></a></h3><p><strong>Problem</strong>: A model may have good overall accuracy but develop systematic bias over time.</p><p><strong>Solution</strong>: Use tracking signal for ongoing monitoring.</p><pre><code class="language-julia hljs"># Model starts good but develops bias
actual = [100, 102, 104, 106, 108, 110, 112, 114, 116, 118]
predicted = [100, 101, 102, 103, 104, 105, 106, 107, 108, 109]  # Increasing under-forecast

# Overall MAE looks okay
mae(actual, predicted)  # ~4.5

# But tracking signal reveals the problem
tracking_signal(actual, predicted)  # High positive value - systematic under-forecasting</code></pre><hr/><h2 id="References-and-Further-Reading"><a class="docs-heading-anchor" href="#References-and-Further-Reading">References and Further Reading</a><a id="References-and-Further-Reading-1"></a><a class="docs-heading-anchor-permalink" href="#References-and-Further-Reading" title="Permalink"></a></h2><h3 id="Academic-References"><a class="docs-heading-anchor" href="#Academic-References">Academic References</a><a id="Academic-References-1"></a><a class="docs-heading-anchor-permalink" href="#Academic-References" title="Permalink"></a></h3><ul><li><p>Hyndman, R.J., &amp; Koehler, A.B. (2006). &quot;Another look at measures of forecast accuracy.&quot; <em>International Journal of Forecasting</em>, 22(4), 679-688. (Introduced MASE)</p></li><li><p>Makridakis, S., Spiliotis, E., &amp; Assimakopoulos, V. (2020). &quot;The M4 Competition: 100,000 time series and 61 forecasting methods.&quot; <em>International Journal of Forecasting</em>, 36(1), 54-74.</p></li><li><p>Gneiting, T., &amp; Raftery, A.E. (2007). &quot;Strictly proper scoring rules, prediction, and estimation.&quot; <em>Journal of the American Statistical Association</em>, 102(477), 359-378. (Theory behind proper scoring rules)</p></li></ul><h3 id="Metric-Selection-Guidelines"><a class="docs-heading-anchor" href="#Metric-Selection-Guidelines">Metric Selection Guidelines</a><a id="Metric-Selection-Guidelines-1"></a><a class="docs-heading-anchor-permalink" href="#Metric-Selection-Guidelines" title="Permalink"></a></h3><ul><li><strong>M-competitions</strong>: Use MASE, sMAPE (symmetric MAPE), and RMSSE</li><li><strong>Supply chain</strong>: Use WAPE, MASE, and tracking signal</li><li><strong>Finance</strong>: Use directional accuracy, Theil&#39;s U2</li><li><strong>Probabilistic forecasting</strong>: Use coverage probability, Winkler score, CRPS</li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../choosing_metrics/">¬´ Choosing the Right Metric</a><a class="docs-footer-nextpage" href="../regression/">Regression ¬ª</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Friday 30 January 2026 11:19">Friday 30 January 2026</span>. Using Julia version 1.11.8.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
