<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Choosing the Right Metric · UnifiedMetrics.jl</title><meta name="title" content="Choosing the Right Metric · UnifiedMetrics.jl"/><meta property="og:title" content="Choosing the Right Metric · UnifiedMetrics.jl"/><meta property="twitter:title" content="Choosing the Right Metric · UnifiedMetrics.jl"/><meta name="description" content="Documentation for UnifiedMetrics.jl."/><meta property="og:description" content="Documentation for UnifiedMetrics.jl."/><meta property="twitter:description" content="Documentation for UnifiedMetrics.jl."/><meta property="og:url" content="https://taf-society.github.io/UnifiedMetrics.jl/choosing_metrics/"/><meta property="twitter:url" content="https://taf-society.github.io/UnifiedMetrics.jl/choosing_metrics/"/><link rel="canonical" href="https://taf-society.github.io/UnifiedMetrics.jl/choosing_metrics/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">UnifiedMetrics.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../getting_started/">Getting Started</a></li><li class="is-active"><a class="tocitem" href>Choosing the Right Metric</a><ul class="internal"><li><a class="tocitem" href="#Quick-Decision-Guide"><span>Quick Decision Guide</span></a></li><li><a class="tocitem" href="#regression-guide"><span>Regression Metrics</span></a></li><li><a class="tocitem" href="#classification-guide"><span>Classification Metrics</span></a></li><li><a class="tocitem" href="#binary-guide"><span>Binary Classification Metrics</span></a></li><li><a class="tocitem" href="#ir-guide"><span>Information Retrieval Metrics</span></a></li><li><a class="tocitem" href="#ts-guide"><span>Time Series Metrics</span></a></li><li><a class="tocitem" href="#Common-Mistakes-to-Avoid"><span>Common Mistakes to Avoid</span></a></li><li><a class="tocitem" href="#Metric-Selection-Summary-Table"><span>Metric Selection Summary Table</span></a></li></ul></li><li><a class="tocitem" href="../time_series/">Time Series Forecasting</a></li><li><span class="tocitem">Other Metrics</span><ul><li><a class="tocitem" href="../regression/">Regression</a></li><li><a class="tocitem" href="../classification/">Classification</a></li><li><a class="tocitem" href="../binary_classification/">Binary Classification</a></li><li><a class="tocitem" href="../information_retrieval/">Information Retrieval</a></li></ul></li><li><a class="tocitem" href="../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Choosing the Right Metric</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Choosing the Right Metric</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/taf-society/UnifiedMetrics.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/taf-society/UnifiedMetrics.jl/blob/main/docs/src/choosing_metrics.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Choosing-the-Right-Metric"><a class="docs-heading-anchor" href="#Choosing-the-Right-Metric">Choosing the Right Metric</a><a id="Choosing-the-Right-Metric-1"></a><a class="docs-heading-anchor-permalink" href="#Choosing-the-Right-Metric" title="Permalink"></a></h1><p>This guide helps you select the appropriate metric for your machine learning task. The right metric depends on your problem type, data characteristics, and business requirements.</p><h2 id="Quick-Decision-Guide"><a class="docs-heading-anchor" href="#Quick-Decision-Guide">Quick Decision Guide</a><a id="Quick-Decision-Guide-1"></a><a class="docs-heading-anchor-permalink" href="#Quick-Decision-Guide" title="Permalink"></a></h2><h3 id="What-type-of-problem-are-you-solving?"><a class="docs-heading-anchor" href="#What-type-of-problem-are-you-solving?">What type of problem are you solving?</a><a id="What-type-of-problem-are-you-solving?-1"></a><a class="docs-heading-anchor-permalink" href="#What-type-of-problem-are-you-solving?" title="Permalink"></a></h3><table><tr><th style="text-align: right">Problem Type</th><th style="text-align: right">Go to Section</th></tr><tr><td style="text-align: right">Predicting continuous values (prices, temperatures, etc.)</td><td style="text-align: right"><a href="#regression-guide">Regression Metrics</a></td></tr><tr><td style="text-align: right">Predicting categories (multi-class)</td><td style="text-align: right"><a href="#classification-guide">Classification Metrics</a></td></tr><tr><td style="text-align: right">Predicting yes/no outcomes</td><td style="text-align: right"><a href="#binary-guide">Binary Classification Metrics</a></td></tr><tr><td style="text-align: right">Ranking items (search, recommendations)</td><td style="text-align: right"><a href="#ir-guide">Information Retrieval Metrics</a></td></tr><tr><td style="text-align: right">Predicting future values in a sequence</td><td style="text-align: right"><a href="#ts-guide">Time Series Metrics</a></td></tr></table><hr/><h2 id="regression-guide"><a class="docs-heading-anchor" href="#regression-guide">Regression Metrics</a><a id="regression-guide-1"></a><a class="docs-heading-anchor-permalink" href="#regression-guide" title="Permalink"></a></h2><h3 id="Decision-Flowchart"><a class="docs-heading-anchor" href="#Decision-Flowchart">Decision Flowchart</a><a id="Decision-Flowchart-1"></a><a class="docs-heading-anchor-permalink" href="#Decision-Flowchart" title="Permalink"></a></h3><pre><code class="nohighlight hljs">START: Regression Problem
    |
    v
Do you need interpretable units?
    |
    +-- YES --&gt; Do you want to penalize large errors more?
    |               |
    |               +-- YES --&gt; Use RMSE
    |               |
    |               +-- NO --&gt; Use MAE
    |
    +-- NO --&gt; Do you need scale-independent comparison?
                    |
                    +-- YES --&gt; Are there zeros in actual values?
                    |               |
                    |               +-- YES --&gt; Use SMAPE or WMAPE
                    |               |
                    |               +-- NO --&gt; Use MAPE
                    |
                    +-- NO --&gt; Use R² (explained_variation)</code></pre><h3 id="When-to-Use-Each-Metric"><a class="docs-heading-anchor" href="#When-to-Use-Each-Metric">When to Use Each Metric</a><a id="When-to-Use-Each-Metric-1"></a><a class="docs-heading-anchor-permalink" href="#When-to-Use-Each-Metric" title="Permalink"></a></h3><table><tr><th style="text-align: right">Metric</th><th style="text-align: right">Use When</th><th style="text-align: right">Avoid When</th></tr><tr><td style="text-align: right"><strong>MAE</strong></td><td style="text-align: right">You want average error in original units; outliers should not dominate</td><td style="text-align: right">You need to heavily penalize large errors</td></tr><tr><td style="text-align: right"><strong>RMSE</strong></td><td style="text-align: right">Large errors are particularly bad; you want same units as target</td><td style="text-align: right">Outliers are present and acceptable</td></tr><tr><td style="text-align: right"><strong>MAPE</strong></td><td style="text-align: right">You need percentage errors for stakeholder communication</td><td style="text-align: right">Actual values contain zeros or near-zeros</td></tr><tr><td style="text-align: right"><strong>SMAPE</strong></td><td style="text-align: right">You need percentage errors and have zeros</td><td style="text-align: right">You need asymmetric error treatment</td></tr><tr><td style="text-align: right"><strong>R²</strong></td><td style="text-align: right">You want to know proportion of variance explained</td><td style="text-align: right">Comparing models on different datasets</td></tr><tr><td style="text-align: right"><strong>MASE</strong></td><td style="text-align: right">Comparing forecasts across different scales</td><td style="text-align: right">Non-time-series data</td></tr></table><h3 id="Detailed-Recommendations"><a class="docs-heading-anchor" href="#Detailed-Recommendations">Detailed Recommendations</a><a id="Detailed-Recommendations-1"></a><a class="docs-heading-anchor-permalink" href="#Detailed-Recommendations" title="Permalink"></a></h3><h4 id="For-General-Regression-Tasks"><a class="docs-heading-anchor" href="#For-General-Regression-Tasks">For General Regression Tasks</a><a id="For-General-Regression-Tasks-1"></a><a class="docs-heading-anchor-permalink" href="#For-General-Regression-Tasks" title="Permalink"></a></h4><p><strong>Primary metric</strong>: <code>rmse</code> or <code>mae</code></p><ul><li>Use <code>rmse</code> when large errors are costly (e.g., predicting house prices where a <span>$</span>100K error is much worse than ten <span>$</span>10K errors)</li><li>Use <code>mae</code> when all errors matter equally (e.g., predicting delivery times)</li></ul><pre><code class="language-julia hljs"># Standard evaluation
rmse(actual, predicted)  # Penalizes large errors
mae(actual, predicted)   # Treats all errors equally</code></pre><h4 id="For-Percentage-Based-Reporting"><a class="docs-heading-anchor" href="#For-Percentage-Based-Reporting">For Percentage-Based Reporting</a><a id="For-Percentage-Based-Reporting-1"></a><a class="docs-heading-anchor-permalink" href="#For-Percentage-Based-Reporting" title="Permalink"></a></h4><p><strong>Primary metric</strong>: <code>mape</code>, <code>smape</code>, or <code>wmape</code></p><pre><code class="language-julia hljs"># When actual values are always positive and non-zero
mape(actual, predicted)

# When actual values may be zero
smape(actual, predicted)  # Symmetric, bounded [0, 2]
wmape(actual, predicted)  # Weighted by actuals

# To detect systematic bias
mpe(actual, predicted)  # Positive = under-prediction</code></pre><h4 id="For-Model-Comparison"><a class="docs-heading-anchor" href="#For-Model-Comparison">For Model Comparison</a><a id="For-Model-Comparison-1"></a><a class="docs-heading-anchor-permalink" href="#For-Model-Comparison" title="Permalink"></a></h4><p><strong>Primary metric</strong>: <code>explained_variation</code> (R²) or <code>adjusted_r2</code></p><pre><code class="language-julia hljs"># Basic R²
explained_variation(actual, predicted)  # 1 = perfect, 0 = mean baseline

# When comparing models with different numbers of features
adjusted_r2(actual, predicted, n_features)</code></pre><h4 id="For-Robust-Models-(Outlier-Resistant)"><a class="docs-heading-anchor" href="#For-Robust-Models-(Outlier-Resistant)">For Robust Models (Outlier-Resistant)</a><a id="For-Robust-Models-(Outlier-Resistant)-1"></a><a class="docs-heading-anchor-permalink" href="#For-Robust-Models-(Outlier-Resistant)" title="Permalink"></a></h4><p><strong>Primary metric</strong>: <code>huber_loss</code> or <code>mdae</code></p><pre><code class="language-julia hljs"># Huber loss: quadratic for small errors, linear for large
huber_loss(actual, predicted, delta=1.0)

# Median Absolute Error: robust to outliers
mdae(actual, predicted)</code></pre><h4 id="For-Skewed-Target-Variables"><a class="docs-heading-anchor" href="#For-Skewed-Target-Variables">For Skewed Target Variables</a><a id="For-Skewed-Target-Variables-1"></a><a class="docs-heading-anchor-permalink" href="#For-Skewed-Target-Variables" title="Permalink"></a></h4><p><strong>Primary metric</strong>: <code>rmsle</code> or <code>msle</code></p><pre><code class="language-julia hljs"># For targets spanning multiple orders of magnitude (prices, populations)
rmsle(actual, predicted)  # Penalizes under-prediction more</code></pre><h4 id="For-Count-Data-or-GLMs"><a class="docs-heading-anchor" href="#For-Count-Data-or-GLMs">For Count Data or GLMs</a><a id="For-Count-Data-or-GLMs-1"></a><a class="docs-heading-anchor-permalink" href="#For-Count-Data-or-GLMs" title="Permalink"></a></h4><p><strong>Primary metric</strong>: <code>mean_poisson_deviance</code> or <code>mean_gamma_deviance</code></p><pre><code class="language-julia hljs"># For count data (website visits, number of purchases)
mean_poisson_deviance(actual, predicted)

# For positive continuous data with variance ~ mean²
mean_gamma_deviance(actual, predicted)</code></pre><hr/><h2 id="classification-guide"><a class="docs-heading-anchor" href="#classification-guide">Classification Metrics</a><a id="classification-guide-1"></a><a class="docs-heading-anchor-permalink" href="#classification-guide" title="Permalink"></a></h2><h3 id="Decision-Flowchart-2"><a class="docs-heading-anchor" href="#Decision-Flowchart-2">Decision Flowchart</a><a class="docs-heading-anchor-permalink" href="#Decision-Flowchart-2" title="Permalink"></a></h3><pre><code class="nohighlight hljs">START: Multi-class Classification
    |
    v
Is your dataset balanced?
    |
    +-- YES --&gt; Use accuracy() or ce()
    |
    +-- NO --&gt; Use balanced_accuracy() or cohens_kappa()
                    |
                    v
               Do you need a single summary metric?
                    |
                    +-- YES --&gt; For binary: mcc()
                    |           For ordinal: ScoreQuadraticWeightedKappa()
                    |
                    +-- NO --&gt; Use confusion_matrix() for detailed analysis</code></pre><h3 id="When-to-Use-Each-Metric-2"><a class="docs-heading-anchor" href="#When-to-Use-Each-Metric-2">When to Use Each Metric</a><a class="docs-heading-anchor-permalink" href="#When-to-Use-Each-Metric-2" title="Permalink"></a></h3><table><tr><th style="text-align: right">Metric</th><th style="text-align: right">Use When</th><th style="text-align: right">Avoid When</th></tr><tr><td style="text-align: right"><strong>accuracy</strong></td><td style="text-align: right">Classes are balanced; simple reporting needed</td><td style="text-align: right">Imbalanced datasets</td></tr><tr><td style="text-align: right"><strong>balanced_accuracy</strong></td><td style="text-align: right">Classes are imbalanced</td><td style="text-align: right">You need per-class details</td></tr><tr><td style="text-align: right"><strong>cohens_kappa</strong></td><td style="text-align: right">You want to account for chance agreement</td><td style="text-align: right">N/A</td></tr><tr><td style="text-align: right"><strong>mcc</strong></td><td style="text-align: right">Binary classification; best single metric</td><td style="text-align: right">Multi-class (use macro-averaged)</td></tr><tr><td style="text-align: right"><strong>confusion_matrix</strong></td><td style="text-align: right">You need detailed error analysis</td><td style="text-align: right">Simple summary is sufficient</td></tr></table><h3 id="Detailed-Recommendations-2"><a class="docs-heading-anchor" href="#Detailed-Recommendations-2">Detailed Recommendations</a><a class="docs-heading-anchor-permalink" href="#Detailed-Recommendations-2" title="Permalink"></a></h3><h4 id="For-Balanced-Datasets"><a class="docs-heading-anchor" href="#For-Balanced-Datasets">For Balanced Datasets</a><a id="For-Balanced-Datasets-1"></a><a class="docs-heading-anchor-permalink" href="#For-Balanced-Datasets" title="Permalink"></a></h4><pre><code class="language-julia hljs">accuracy(actual, predicted)  # Simple and interpretable</code></pre><h4 id="For-Imbalanced-Datasets"><a class="docs-heading-anchor" href="#For-Imbalanced-Datasets">For Imbalanced Datasets</a><a id="For-Imbalanced-Datasets-1"></a><a class="docs-heading-anchor-permalink" href="#For-Imbalanced-Datasets" title="Permalink"></a></h4><pre><code class="language-julia hljs"># Macro-averaged recall across classes
balanced_accuracy(actual, predicted)

# Accounts for chance agreement
cohens_kappa(actual, predicted)</code></pre><h4 id="For-Ordinal-Classification"><a class="docs-heading-anchor" href="#For-Ordinal-Classification">For Ordinal Classification</a><a id="For-Ordinal-Classification-1"></a><a class="docs-heading-anchor-permalink" href="#For-Ordinal-Classification" title="Permalink"></a></h4><p>When classes have a natural order (e.g., ratings 1-5):</p><pre><code class="language-julia hljs"># Penalizes predictions farther from true class
ScoreQuadraticWeightedKappa(actual, predicted, min_rating=1, max_rating=5)</code></pre><h4 id="For-Multi-Label-Classification"><a class="docs-heading-anchor" href="#For-Multi-Label-Classification">For Multi-Label Classification</a><a id="For-Multi-Label-Classification-1"></a><a class="docs-heading-anchor-permalink" href="#For-Multi-Label-Classification" title="Permalink"></a></h4><pre><code class="language-julia hljs"># Fraction of incorrect labels
hamming_loss(actual_matrix, predicted_matrix)</code></pre><hr/><h2 id="binary-guide"><a class="docs-heading-anchor" href="#binary-guide">Binary Classification Metrics</a><a id="binary-guide-1"></a><a class="docs-heading-anchor-permalink" href="#binary-guide" title="Permalink"></a></h2><h3 id="Decision-Flowchart-3"><a class="docs-heading-anchor" href="#Decision-Flowchart-3">Decision Flowchart</a><a class="docs-heading-anchor-permalink" href="#Decision-Flowchart-3" title="Permalink"></a></h3><pre><code class="nohighlight hljs">START: Binary Classification
    |
    v
What type of predictions do you have?
    |
    +-- Probabilities (0-1) --&gt; Do you need threshold-independent evaluation?
    |                               |
    |                               +-- YES --&gt; Use auc() or gini_coefficient()
    |                               |
    |                               +-- NO --&gt; What matters more?
    |                                               |
    |                                               +-- Calibration --&gt; brier_score() or logloss()
    |                                               |
    |                                               +-- Ranking --&gt; ks_statistic()
    |
    +-- Binary Labels (0/1) --&gt; What is your priority?
                                    |
                                    +-- Balance precision/recall --&gt; fbeta_score()
                                    |
                                    +-- Minimize false positives --&gt; precision()
                                    |
                                    +-- Minimize false negatives --&gt; recall()
                                    |
                                    +-- Single best metric --&gt; mcc()</code></pre><h3 id="When-to-Use-Each-Metric-3"><a class="docs-heading-anchor" href="#When-to-Use-Each-Metric-3">When to Use Each Metric</a><a class="docs-heading-anchor-permalink" href="#When-to-Use-Each-Metric-3" title="Permalink"></a></h3><table><tr><th style="text-align: right">Metric</th><th style="text-align: right">Use When</th><th style="text-align: right">Avoid When</th></tr><tr><td style="text-align: right"><strong>auc</strong></td><td style="text-align: right">Comparing models; threshold hasn&#39;t been chosen</td><td style="text-align: right">You need a specific operating point</td></tr><tr><td style="text-align: right"><strong>precision</strong></td><td style="text-align: right">False positives are costly (spam detection)</td><td style="text-align: right">Missing positives is worse</td></tr><tr><td style="text-align: right"><strong>recall</strong></td><td style="text-align: right">False negatives are costly (disease detection)</td><td style="text-align: right">False alarms are problematic</td></tr><tr><td style="text-align: right"><strong>fbeta_score</strong></td><td style="text-align: right">You need to balance precision and recall</td><td style="text-align: right">Clear priority for one over other</td></tr><tr><td style="text-align: right"><strong>mcc</strong></td><td style="text-align: right">Imbalanced data; need single summary metric</td><td style="text-align: right">You need threshold-independent metric</td></tr><tr><td style="text-align: right"><strong>brier_score</strong></td><td style="text-align: right">Probability calibration matters</td><td style="text-align: right">Ranking is more important</td></tr></table><h3 id="Detailed-Recommendations-3"><a class="docs-heading-anchor" href="#Detailed-Recommendations-3">Detailed Recommendations</a><a class="docs-heading-anchor-permalink" href="#Detailed-Recommendations-3" title="Permalink"></a></h3><h4 id="For-Model-Selection-(Before-Choosing-Threshold)"><a class="docs-heading-anchor" href="#For-Model-Selection-(Before-Choosing-Threshold)">For Model Selection (Before Choosing Threshold)</a><a id="For-Model-Selection-(Before-Choosing-Threshold)-1"></a><a class="docs-heading-anchor-permalink" href="#For-Model-Selection-(Before-Choosing-Threshold)" title="Permalink"></a></h4><pre><code class="language-julia hljs"># Area Under ROC Curve - threshold independent
auc(actual, predicted_scores)

# Gini coefficient (= 2*AUC - 1)
gini_coefficient(actual, predicted_scores)

# Maximum separation between classes
ks_statistic(actual, predicted_scores)</code></pre><h4 id="For-Probability-Calibration"><a class="docs-heading-anchor" href="#For-Probability-Calibration">For Probability Calibration</a><a id="For-Probability-Calibration-1"></a><a class="docs-heading-anchor-permalink" href="#For-Probability-Calibration" title="Permalink"></a></h4><p>When you need well-calibrated probabilities:</p><pre><code class="language-julia hljs"># Mean squared error of probabilities
brier_score(actual, predicted_probs)  # Lower is better

# Cross-entropy loss
logloss(actual, predicted_probs)  # Lower is better</code></pre><h4 id="For-Threshold-Based-Evaluation"><a class="docs-heading-anchor" href="#For-Threshold-Based-Evaluation">For Threshold-Based Evaluation</a><a id="For-Threshold-Based-Evaluation-1"></a><a class="docs-heading-anchor-permalink" href="#For-Threshold-Based-Evaluation" title="Permalink"></a></h4><p>After choosing a classification threshold:</p><pre><code class="language-julia hljs"># Convert probabilities to labels
predicted_labels = predicted_probs .&gt;= threshold

# When false positives are costly (spam filter, fraud detection)
precision(actual, predicted_labels)

# When false negatives are costly (disease screening, security threats)
recall(actual, predicted_labels)
sensitivity(actual, predicted_labels)  # Same as recall

# Balanced metric
fbeta_score(actual, predicted_labels)         # F1: equal weight
fbeta_score(actual, predicted_labels, beta=0.5)  # Favor precision
fbeta_score(actual, predicted_labels, beta=2.0)  # Favor recall</code></pre><h4 id="For-Medical/Diagnostic-Applications"><a class="docs-heading-anchor" href="#For-Medical/Diagnostic-Applications">For Medical/Diagnostic Applications</a><a id="For-Medical/Diagnostic-Applications-1"></a><a class="docs-heading-anchor-permalink" href="#For-Medical/Diagnostic-Applications" title="Permalink"></a></h4><pre><code class="language-julia hljs"># Sensitivity (true positive rate)
sensitivity(actual, predicted_labels)

# Specificity (true negative rate)
specificity(actual, predicted_labels)

# Youden&#39;s J (optimal threshold criterion)
youden_j(actual, predicted_labels)

# Likelihood ratios for clinical decision making
positive_likelihood_ratio(actual, predicted_labels)
negative_likelihood_ratio(actual, predicted_labels)
diagnostic_odds_ratio(actual, predicted_labels)</code></pre><h4 id="For-Imbalanced-Data"><a class="docs-heading-anchor" href="#For-Imbalanced-Data">For Imbalanced Data</a><a id="For-Imbalanced-Data-1"></a><a class="docs-heading-anchor-permalink" href="#For-Imbalanced-Data" title="Permalink"></a></h4><p>The single best metric for binary classification with imbalanced data:</p><pre><code class="language-julia hljs"># Matthews Correlation Coefficient: accounts for all quadrants of confusion matrix
mcc(actual, predicted_labels)  # Range: [-1, 1], 0 = random</code></pre><h4 id="For-Business-Applications"><a class="docs-heading-anchor" href="#For-Business-Applications">For Business Applications</a><a id="For-Business-Applications-1"></a><a class="docs-heading-anchor-permalink" href="#For-Business-Applications" title="Permalink"></a></h4><pre><code class="language-julia hljs"># Lift: how much better than random in top X%
lift(actual, predicted_scores, percentile=0.1)

# Gain: what % of positives captured in top X%
gain(actual, predicted_scores, percentile=0.1)</code></pre><hr/><h2 id="ir-guide"><a class="docs-heading-anchor" href="#ir-guide">Information Retrieval Metrics</a><a id="ir-guide-1"></a><a class="docs-heading-anchor-permalink" href="#ir-guide" title="Permalink"></a></h2><h3 id="Decision-Flowchart-4"><a class="docs-heading-anchor" href="#Decision-Flowchart-4">Decision Flowchart</a><a class="docs-heading-anchor-permalink" href="#Decision-Flowchart-4" title="Permalink"></a></h3><pre><code class="nohighlight hljs">START: Ranking/Retrieval Problem
    |
    v
Do you have graded relevance scores?
    |
    +-- YES --&gt; Use ndcg() or dcg()
    |
    +-- NO (binary relevance) --&gt; What matters more?
                                      |
                                      +-- Finding first relevant item --&gt; mrr()
                                      |
                                      +-- Finding all relevant items --&gt; recall_at_k()
                                      |
                                      +-- Precision of top results --&gt; precision_at_k()
                                      |
                                      +-- Balance of both --&gt; f1_at_k() or mapk()</code></pre><h3 id="When-to-Use-Each-Metric-4"><a class="docs-heading-anchor" href="#When-to-Use-Each-Metric-4">When to Use Each Metric</a><a class="docs-heading-anchor-permalink" href="#When-to-Use-Each-Metric-4" title="Permalink"></a></h3><table><tr><th style="text-align: right">Metric</th><th style="text-align: right">Use When</th><th style="text-align: right">Avoid When</th></tr><tr><td style="text-align: right"><strong>ndcg</strong></td><td style="text-align: right">Relevance is graded (0-5 stars)</td><td style="text-align: right">Binary relevance only</td></tr><tr><td style="text-align: right"><strong>mrr</strong></td><td style="text-align: right">Only first relevant result matters</td><td style="text-align: right">All relevant items matter</td></tr><tr><td style="text-align: right"><strong>map@k</strong></td><td style="text-align: right">Ranking quality across positions matters</td><td style="text-align: right">Only top-1 or top-k matters</td></tr><tr><td style="text-align: right"><strong>recall@k</strong></td><td style="text-align: right">Coverage of relevant items is priority</td><td style="text-align: right">Precision matters more</td></tr><tr><td style="text-align: right"><strong>precision@k</strong></td><td style="text-align: right">Quality of top results is priority</td><td style="text-align: right">Missing relevant items is costly</td></tr><tr><td style="text-align: right"><strong>hit_rate</strong></td><td style="text-align: right">At least one relevant in top-k is success</td><td style="text-align: right">Need finer granularity</td></tr></table><h3 id="Detailed-Recommendations-4"><a class="docs-heading-anchor" href="#Detailed-Recommendations-4">Detailed Recommendations</a><a class="docs-heading-anchor-permalink" href="#Detailed-Recommendations-4" title="Permalink"></a></h3><h4 id="For-Search-Engines"><a class="docs-heading-anchor" href="#For-Search-Engines">For Search Engines</a><a id="For-Search-Engines-1"></a><a class="docs-heading-anchor-permalink" href="#For-Search-Engines" title="Permalink"></a></h4><pre><code class="language-julia hljs"># Graded relevance (best for search)
ndcg(relevance_scores, k=10)

# Mean NDCG across queries
mean_ndcg(relevances_list, k=10)

# Mean Reciprocal Rank (how quickly users find what they want)
mrr(actual_list, predicted_list)</code></pre><h4 id="For-Recommendation-Systems"><a class="docs-heading-anchor" href="#For-Recommendation-Systems">For Recommendation Systems</a><a id="For-Recommendation-Systems-1"></a><a class="docs-heading-anchor-permalink" href="#For-Recommendation-Systems" title="Permalink"></a></h4><pre><code class="language-julia hljs"># Did we show at least one good item?
hit_rate(actual_list, predicted_list, k=10)

# How many relevant items did we show?
recall_at_k(actual, predicted, k=10)

# What fraction of shown items are relevant?
precision_at_k(actual, predicted, k=10)

# Balanced metric
f1_at_k(actual, predicted, k=10)

# Catalog coverage (diversity)
coverage(predicted_list, full_catalog)

# Novelty (recommending non-obvious items)
novelty(predicted_list, item_popularity)</code></pre><h4 id="For-E-commerce-/-Product-Search"><a class="docs-heading-anchor" href="#For-E-commerce-/-Product-Search">For E-commerce / Product Search</a><a id="For-E-commerce-/-Product-Search-1"></a><a class="docs-heading-anchor-permalink" href="#For-E-commerce-/-Product-Search" title="Permalink"></a></h4><pre><code class="language-julia hljs"># Average precision at k
apk(10, relevant_products, retrieved_products)

# Mean AP across queries
mapk(10, relevant_lists, retrieved_lists)</code></pre><hr/><h2 id="ts-guide"><a class="docs-heading-anchor" href="#ts-guide">Time Series Metrics</a><a id="ts-guide-1"></a><a class="docs-heading-anchor-permalink" href="#ts-guide" title="Permalink"></a></h2><h3 id="Decision-Flowchart-5"><a class="docs-heading-anchor" href="#Decision-Flowchart-5">Decision Flowchart</a><a class="docs-heading-anchor-permalink" href="#Decision-Flowchart-5" title="Permalink"></a></h3><pre><code class="nohighlight hljs">START: Time Series Forecasting
    |
    v
What aspect of forecast quality matters?
    |
    +-- Point forecast accuracy --&gt; Is scale-independent comparison needed?
    |                                   |
    |                                   +-- YES --&gt; mase() or theil_u2()
    |                                   |
    |                                   +-- NO --&gt; rmse() or mae()
    |
    +-- Directional accuracy --&gt; directional_accuracy()
    |
    +-- Forecast bias --&gt; tracking_signal() or forecast_bias()
    |
    +-- Prediction intervals --&gt; coverage_probability() or winkler_score()</code></pre><h3 id="When-to-Use-Each-Metric-5"><a class="docs-heading-anchor" href="#When-to-Use-Each-Metric-5">When to Use Each Metric</a><a class="docs-heading-anchor-permalink" href="#When-to-Use-Each-Metric-5" title="Permalink"></a></h3><table><tr><th style="text-align: right">Metric</th><th style="text-align: right">Use When</th><th style="text-align: right">Avoid When</th></tr><tr><td style="text-align: right"><strong>mase</strong></td><td style="text-align: right">Comparing across series with different scales</td><td style="text-align: right">Single series evaluation</td></tr><tr><td style="text-align: right"><strong>rmsse</strong></td><td style="text-align: right">Scale-independent; sensitive to large errors</td><td style="text-align: right">Outliers acceptable</td></tr><tr><td style="text-align: right"><strong>tracking_signal</strong></td><td style="text-align: right">Monitoring for systematic bias</td><td style="text-align: right">One-time evaluation</td></tr><tr><td style="text-align: right"><strong>directional_accuracy</strong></td><td style="text-align: right">Direction matters more than magnitude</td><td style="text-align: right">Magnitude accuracy critical</td></tr><tr><td style="text-align: right"><strong>winkler_score</strong></td><td style="text-align: right">Evaluating prediction intervals</td><td style="text-align: right">Point forecasts only</td></tr><tr><td style="text-align: right"><strong>theil_u2</strong></td><td style="text-align: right">Comparing to naive benchmark</td><td style="text-align: right">Absolute accuracy needed</td></tr></table><h3 id="Detailed-Recommendations-5"><a class="docs-heading-anchor" href="#Detailed-Recommendations-5">Detailed Recommendations</a><a class="docs-heading-anchor-permalink" href="#Detailed-Recommendations-5" title="Permalink"></a></h3><h4 id="For-Comparing-Forecasts-Across-Different-Series"><a class="docs-heading-anchor" href="#For-Comparing-Forecasts-Across-Different-Series">For Comparing Forecasts Across Different Series</a><a id="For-Comparing-Forecasts-Across-Different-Series-1"></a><a class="docs-heading-anchor-permalink" href="#For-Comparing-Forecasts-Across-Different-Series" title="Permalink"></a></h4><p>The M-competition recommended metrics:</p><pre><code class="language-julia hljs"># Mean Absolute Scaled Error (most recommended)
mase(actual, predicted, m=1)     # Non-seasonal
mase(actual, predicted, m=12)    # Monthly data with yearly seasonality
mase(actual, predicted, m=7)     # Daily data with weekly seasonality

# Root Mean Squared Scaled Error
rmsse(actual, predicted, m=1)</code></pre><p><strong>Interpretation</strong>:</p><ul><li>MASE &lt; 1: Better than naive forecast</li><li>MASE = 1: Same as naive forecast</li><li>MASE &gt; 1: Worse than naive forecast</li></ul><h4 id="For-Single-Series-Evaluation"><a class="docs-heading-anchor" href="#For-Single-Series-Evaluation">For Single Series Evaluation</a><a id="For-Single-Series-Evaluation-1"></a><a class="docs-heading-anchor-permalink" href="#For-Single-Series-Evaluation" title="Permalink"></a></h4><pre><code class="language-julia hljs"># Standard metrics in original units
mae(actual, predicted)
rmse(actual, predicted)

# Percentage-based (avoid if zeros present)
mape(actual, predicted)
wape(actual, predicted)  # Handles zeros better</code></pre><h4 id="For-Detecting-Forecast-Bias"><a class="docs-heading-anchor" href="#For-Detecting-Forecast-Bias">For Detecting Forecast Bias</a><a id="For-Detecting-Forecast-Bias-1"></a><a class="docs-heading-anchor-permalink" href="#For-Detecting-Forecast-Bias" title="Permalink"></a></h4><pre><code class="language-julia hljs"># Normalized measure of cumulative error
tracking_signal(actual, predicted)
# Interpretation: values outside [-4, 4] indicate systematic bias

# Simple bias (positive = under-forecasting)
forecast_bias(actual, predicted)</code></pre><h4 id="For-Comparing-to-Benchmark"><a class="docs-heading-anchor" href="#For-Comparing-to-Benchmark">For Comparing to Benchmark</a><a id="For-Comparing-to-Benchmark-1"></a><a class="docs-heading-anchor-permalink" href="#For-Comparing-to-Benchmark" title="Permalink"></a></h4><pre><code class="language-julia hljs"># Theil&#39;s U2: comparison to naive forecast
theil_u2(actual, predicted, m=1)
# &lt; 1: better than naive, &gt; 1: worse than naive

# Theil&#39;s U1: normalized error
theil_u1(actual, predicted)
# 0 = perfect, 1 = worst</code></pre><h4 id="For-Direction-Prediction-(Trading,-etc.)"><a class="docs-heading-anchor" href="#For-Direction-Prediction-(Trading,-etc.)">For Direction Prediction (Trading, etc.)</a><a id="For-Direction-Prediction-(Trading,-etc.)-1"></a><a class="docs-heading-anchor-permalink" href="#For-Direction-Prediction-(Trading,-etc.)" title="Permalink"></a></h4><pre><code class="language-julia hljs"># What fraction of up/down movements were predicted correctly?
directional_accuracy(actual, predicted)</code></pre><h4 id="For-Probabilistic-Forecasts-/-Prediction-Intervals"><a class="docs-heading-anchor" href="#For-Probabilistic-Forecasts-/-Prediction-Intervals">For Probabilistic Forecasts / Prediction Intervals</a><a id="For-Probabilistic-Forecasts-/-Prediction-Intervals-1"></a><a class="docs-heading-anchor-permalink" href="#For-Probabilistic-Forecasts-/-Prediction-Intervals" title="Permalink"></a></h4><pre><code class="language-julia hljs"># Does the interval contain the actual value at expected rate?
coverage_probability(actual, lower, upper)
# Should match your confidence level (e.g., 0.95 for 95% intervals)

# Interval score (rewards narrow intervals, penalizes misses)
winkler_score(actual, lower, upper, alpha=0.05)

# Quantile forecast evaluation
pinball_loss_series(actual, predicted_quantile, quantile=0.9)</code></pre><h4 id="For-Preserving-Temporal-Structure"><a class="docs-heading-anchor" href="#For-Preserving-Temporal-Structure">For Preserving Temporal Structure</a><a id="For-Preserving-Temporal-Structure-1"></a><a class="docs-heading-anchor-permalink" href="#For-Preserving-Temporal-Structure" title="Permalink"></a></h4><pre><code class="language-julia hljs"># Does the forecast maintain autocorrelation patterns?
autocorrelation_error(actual, predicted, max_lag=10)</code></pre><hr/><h2 id="Common-Mistakes-to-Avoid"><a class="docs-heading-anchor" href="#Common-Mistakes-to-Avoid">Common Mistakes to Avoid</a><a id="Common-Mistakes-to-Avoid-1"></a><a class="docs-heading-anchor-permalink" href="#Common-Mistakes-to-Avoid" title="Permalink"></a></h2><h3 id="Regression"><a class="docs-heading-anchor" href="#Regression">Regression</a><a id="Regression-1"></a><a class="docs-heading-anchor-permalink" href="#Regression" title="Permalink"></a></h3><ol><li><strong>Using MAPE with zeros</strong>: MAPE is undefined when actual values are zero. Use SMAPE or WMAPE instead.</li><li><strong>Ignoring scale</strong>: When comparing models across different datasets, use scale-independent metrics (R², MAPE, MASE).</li><li><strong>Only using R²</strong>: R² can be misleading for non-linear relationships. Always check residual plots.</li></ol><h3 id="Classification"><a class="docs-heading-anchor" href="#Classification">Classification</a><a id="Classification-1"></a><a class="docs-heading-anchor-permalink" href="#Classification" title="Permalink"></a></h3><ol><li><strong>Using accuracy on imbalanced data</strong>: A model predicting the majority class always achieves high accuracy. Use balanced_accuracy, MCC, or per-class metrics.</li><li><strong>Optimizing for wrong metric</strong>: If false negatives are costly (medical diagnosis), optimize for recall, not precision.</li></ol><h3 id="Binary-Classification"><a class="docs-heading-anchor" href="#Binary-Classification">Binary Classification</a><a id="Binary-Classification-1"></a><a class="docs-heading-anchor-permalink" href="#Binary-Classification" title="Permalink"></a></h3><ol><li><strong>Comparing AUC across very different datasets</strong>: AUC can be misleading if class distributions differ significantly.</li><li><strong>Ignoring calibration</strong>: High AUC doesn&#39;t mean probabilities are well-calibrated. Check Brier score.</li><li><strong>Using accuracy on imbalanced data</strong>: Use MCC instead.</li></ol><h3 id="Information-Retrieval"><a class="docs-heading-anchor" href="#Information-Retrieval">Information Retrieval</a><a id="Information-Retrieval-1"></a><a class="docs-heading-anchor-permalink" href="#Information-Retrieval" title="Permalink"></a></h3><ol><li><strong>Using NDCG with binary relevance</strong>: While valid, simpler metrics (MAP, MRR) may be more interpretable.</li><li><strong>Ignoring position</strong>: Metrics like precision don&#39;t account for ranking. Use NDCG or MRR.</li></ol><h3 id="Time-Series"><a class="docs-heading-anchor" href="#Time-Series">Time Series</a><a id="Time-Series-1"></a><a class="docs-heading-anchor-permalink" href="#Time-Series" title="Permalink"></a></h3><ol><li><strong>Not using scaled metrics</strong>: Raw MAE/RMSE can&#39;t be compared across series with different scales.</li><li><strong>Ignoring seasonality in MASE</strong>: Set <code>m</code> to match your data&#39;s seasonal period.</li><li><strong>Only checking point accuracy</strong>: Also evaluate bias (tracking<em>signal) and intervals (coverage</em>probability).</li></ol><hr/><h2 id="Metric-Selection-Summary-Table"><a class="docs-heading-anchor" href="#Metric-Selection-Summary-Table">Metric Selection Summary Table</a><a id="Metric-Selection-Summary-Table-1"></a><a class="docs-heading-anchor-permalink" href="#Metric-Selection-Summary-Table" title="Permalink"></a></h2><table><tr><th style="text-align: right">Scenario</th><th style="text-align: right">Recommended Metric</th><th style="text-align: right">Alternative</th></tr><tr><td style="text-align: right">General regression</td><td style="text-align: right">RMSE</td><td style="text-align: right">MAE</td></tr><tr><td style="text-align: right">Regression with outliers</td><td style="text-align: right">Huber loss</td><td style="text-align: right">MdAE</td></tr><tr><td style="text-align: right">Stakeholder reporting</td><td style="text-align: right">MAPE (if no zeros)</td><td style="text-align: right">SMAPE</td></tr><tr><td style="text-align: right">Imbalanced binary classification</td><td style="text-align: right">MCC</td><td style="text-align: right">Balanced accuracy</td></tr><tr><td style="text-align: right">Medical diagnosis</td><td style="text-align: right">Sensitivity + Specificity</td><td style="text-align: right">Youden&#39;s J</td></tr><tr><td style="text-align: right">Search ranking</td><td style="text-align: right">NDCG</td><td style="text-align: right">MRR</td></tr><tr><td style="text-align: right">Recommendation system</td><td style="text-align: right">Hit rate, Recall@k</td><td style="text-align: right">MAP@k</td></tr><tr><td style="text-align: right">Forecast comparison</td><td style="text-align: right">MASE</td><td style="text-align: right">RMSSE</td></tr><tr><td style="text-align: right">Forecast monitoring</td><td style="text-align: right">Tracking signal</td><td style="text-align: right">Forecast bias</td></tr><tr><td style="text-align: right">Prediction intervals</td><td style="text-align: right">Coverage + Winkler</td><td style="text-align: right">Pinball loss</td></tr></table></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../getting_started/">« Getting Started</a><a class="docs-footer-nextpage" href="../time_series/">Time Series Forecasting »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Friday 30 January 2026 10:09">Friday 30 January 2026</span>. Using Julia version 1.11.8.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
