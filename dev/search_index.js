var documenterSearchIndex = {"docs":
[{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"Complete reference for all functions in UnifiedMetrics.jl.","category":"section"},{"location":"api/#Regression-Metrics","page":"API Reference","title":"Regression Metrics","text":"","category":"section"},{"location":"api/#Error-Metrics","page":"API Reference","title":"Error Metrics","text":"","category":"section"},{"location":"api/#Bias-Metrics","page":"API Reference","title":"Bias Metrics","text":"","category":"section"},{"location":"api/#Percentage-Error-Metrics","page":"API Reference","title":"Percentage Error Metrics","text":"","category":"section"},{"location":"api/#Logarithmic-Error-Metrics","page":"API Reference","title":"Logarithmic Error Metrics","text":"","category":"section"},{"location":"api/#Relative-Error-Metrics","page":"API Reference","title":"Relative Error Metrics","text":"","category":"section"},{"location":"api/#Explained-Variance","page":"API Reference","title":"Explained Variance","text":"","category":"section"},{"location":"api/#Robust-Loss-Functions","page":"API Reference","title":"Robust Loss Functions","text":"","category":"section"},{"location":"api/#Quantile-Loss","page":"API Reference","title":"Quantile Loss","text":"","category":"section"},{"location":"api/#GLM-Deviance-Metrics","page":"API Reference","title":"GLM Deviance Metrics","text":"","category":"section"},{"location":"api/#Classification-Metrics","page":"API Reference","title":"Classification Metrics","text":"","category":"section"},{"location":"api/#Accuracy-Metrics","page":"API Reference","title":"Accuracy Metrics","text":"","category":"section"},{"location":"api/#Agreement-Metrics","page":"API Reference","title":"Agreement Metrics","text":"","category":"section"},{"location":"api/#Correlation-Metrics","page":"API Reference","title":"Correlation Metrics","text":"","category":"section"},{"location":"api/#Confusion-Matrix","page":"API Reference","title":"Confusion Matrix","text":"","category":"section"},{"location":"api/#Top-K-Metrics","page":"API Reference","title":"Top-K Metrics","text":"","category":"section"},{"location":"api/#Loss-Functions","page":"API Reference","title":"Loss Functions","text":"","category":"section"},{"location":"api/#Binary-Classification-Metrics","page":"API Reference","title":"Binary Classification Metrics","text":"","category":"section"},{"location":"api/#ROC-Based-Metrics","page":"API Reference","title":"ROC-Based Metrics","text":"","category":"section"},{"location":"api/#Probability-Metrics","page":"API Reference","title":"Probability Metrics","text":"","category":"section"},{"location":"api/#Precision-and-Recall","page":"API Reference","title":"Precision and Recall","text":"","category":"section"},{"location":"api/#F-Score","page":"API Reference","title":"F-Score","text":"","category":"section"},{"location":"api/#Error-Rates","page":"API Reference","title":"Error Rates","text":"","category":"section"},{"location":"api/#Combined-Metrics","page":"API Reference","title":"Combined Metrics","text":"","category":"section"},{"location":"api/#Likelihood-Ratios","page":"API Reference","title":"Likelihood Ratios","text":"","category":"section"},{"location":"api/#Business-Metrics","page":"API Reference","title":"Business Metrics","text":"","category":"section"},{"location":"api/#Information-Retrieval-Metrics","page":"API Reference","title":"Information Retrieval Metrics","text":"","category":"section"},{"location":"api/#Set-Based-Metrics","page":"API Reference","title":"Set-Based Metrics","text":"","category":"section"},{"location":"api/#Ranking-Metrics","page":"API Reference","title":"Ranking Metrics","text":"","category":"section"},{"location":"api/#Average-Precision","page":"API Reference","title":"Average Precision","text":"","category":"section"},{"location":"api/#Reciprocal-Rank","page":"API Reference","title":"Reciprocal Rank","text":"","category":"section"},{"location":"api/#Hit-Rate","page":"API Reference","title":"Hit Rate","text":"","category":"section"},{"location":"api/#Recommendation-Metrics","page":"API Reference","title":"Recommendation Metrics","text":"","category":"section"},{"location":"api/#Time-Series-Metrics","page":"API Reference","title":"Time Series Metrics","text":"","category":"section"},{"location":"api/#Scaled-Error-Metrics","page":"API Reference","title":"Scaled Error Metrics","text":"","category":"section"},{"location":"api/#Bias-Metrics-2","page":"API Reference","title":"Bias Metrics","text":"","category":"section"},{"location":"api/#Benchmark-Comparison","page":"API Reference","title":"Benchmark Comparison","text":"","category":"section"},{"location":"api/#Percentage-Metrics","page":"API Reference","title":"Percentage Metrics","text":"","category":"section"},{"location":"api/#Directional-Metrics","page":"API Reference","title":"Directional Metrics","text":"","category":"section"},{"location":"api/#Prediction-Interval-Metrics","page":"API Reference","title":"Prediction Interval Metrics","text":"","category":"section"},{"location":"api/#Autocorrelation-Metrics","page":"API Reference","title":"Autocorrelation Metrics","text":"","category":"section"},{"location":"api/#Index","page":"API Reference","title":"Index","text":"","category":"section"},{"location":"api/#UnifiedMetrics.ae","page":"API Reference","title":"UnifiedMetrics.ae","text":"ae(actual, predicted)\n\nCompute the elementwise absolute error between two numeric vectors.\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth numeric vector\npredicted::AbstractVector{<:Real}: Predicted numeric vector\n\nExamples\n\nactual = [1.1, 1.9, 3.0, 4.4, 5.0, 5.6]\npredicted = [0.9, 1.8, 2.5, 4.5, 5.0, 6.2]\nae(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.mae","page":"API Reference","title":"UnifiedMetrics.mae","text":"mae(actual, predicted)\n\nCompute the mean absolute error between two numeric vectors.\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth numeric vector\npredicted::AbstractVector{<:Real}: Predicted numeric vector\n\nExamples\n\nactual = [1.1, 1.9, 3.0, 4.4, 5.0, 5.6]\npredicted = [0.9, 1.8, 2.5, 4.5, 5.0, 6.2]\nmae(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.mdae","page":"API Reference","title":"UnifiedMetrics.mdae","text":"mdae(actual, predicted)\n\nCompute the median absolute error between two numeric vectors.\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth numeric vector\npredicted::AbstractVector{<:Real}: Predicted numeric vector\n\nExamples\n\nactual = [1.1, 1.9, 3.0, 4.4, 5.0, 5.6]\npredicted = [0.9, 1.8, 2.5, 4.5, 5.0, 6.2]\nmdae(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.se","page":"API Reference","title":"UnifiedMetrics.se","text":"se(actual, predicted)\n\nCompute the elementwise squared error between two numeric vectors.\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth numeric vector\npredicted::AbstractVector{<:Real}: Predicted numeric vector\n\nExamples\n\nactual = [1.1, 1.9, 3.0, 4.4, 5.0, 5.6]\npredicted = [0.9, 1.8, 2.5, 4.5, 5.0, 6.2]\nse(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.sse","page":"API Reference","title":"UnifiedMetrics.sse","text":"sse(actual, predicted)\n\nCompute the sum of squared errors between two numeric vectors.\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth numeric vector\npredicted::AbstractVector{<:Real}: Predicted numeric vector\n\nExamples\n\nactual = [1.1, 1.9, 3.0, 4.4, 5.0, 5.6]\npredicted = [0.9, 1.8, 2.5, 4.5, 5.0, 6.2]\nsse(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.mse","page":"API Reference","title":"UnifiedMetrics.mse","text":"mse(actual, predicted)\n\nCompute the mean squared error between two numeric vectors.\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth numeric vector\npredicted::AbstractVector{<:Real}: Predicted numeric vector\n\nExamples\n\nactual = [1.1, 1.9, 3.0, 4.4, 5.0, 5.6]\npredicted = [0.9, 1.8, 2.5, 4.5, 5.0, 6.2]\nmse(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.rmse","page":"API Reference","title":"UnifiedMetrics.rmse","text":"rmse(actual, predicted)\n\nCompute the root mean squared error between two numeric vectors.\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth numeric vector\npredicted::AbstractVector{<:Real}: Predicted numeric vector\n\nExamples\n\nactual = [1.1, 1.9, 3.0, 4.4, 5.0, 5.6]\npredicted = [0.9, 1.8, 2.5, 4.5, 5.0, 6.2]\nrmse(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.nrmse","page":"API Reference","title":"UnifiedMetrics.nrmse","text":"nrmse(actual, predicted; normalization=:range)\n\nCompute the Normalized Root Mean Squared Error.\n\nNormalizes RMSE to make it comparable across different scales.\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth numeric vector\npredicted::AbstractVector{<:Real}: Predicted numeric vector\nnormalization::Symbol: Normalization method (default: :range)\n:range - Normalize by range (max - min) of actual values\n:mean - Normalize by mean of actual values (coefficient of variation of RMSE)\n:std - Normalize by standard deviation of actual values\n:iqr - Normalize by interquartile range of actual values\n\nExamples\n\nactual = [1.1, 1.9, 3.0, 4.4, 5.0, 5.6]\npredicted = [0.9, 1.8, 2.5, 4.5, 5.0, 6.2]\nnrmse(actual, predicted)  # Normalized by range\nnrmse(actual, predicted, normalization=:mean)  # CV(RMSE)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.max_error","page":"API Reference","title":"UnifiedMetrics.max_error","text":"max_error(actual, predicted)\n\nCompute the maximum absolute error between two numeric vectors.\n\nUseful for understanding the worst-case prediction error.\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth numeric vector\npredicted::AbstractVector{<:Real}: Predicted numeric vector\n\nExamples\n\nactual = [1.1, 1.9, 3.0, 4.4, 5.0, 5.6]\npredicted = [0.9, 1.8, 2.5, 4.5, 5.0, 6.2]\nmax_error(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.max_ae","page":"API Reference","title":"UnifiedMetrics.max_ae","text":"max_ae(actual, predicted)\n\nAlias for max_error. Compute the maximum absolute error.\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth numeric vector\npredicted::AbstractVector{<:Real}: Predicted numeric vector\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.bias","page":"API Reference","title":"UnifiedMetrics.bias","text":"bias(actual, predicted)\n\nCompute the average amount by which actual is greater than predicted.\n\nIf a model is unbiased, bias(actual, predicted) should be close to zero.\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth numeric vector\npredicted::AbstractVector{<:Real}: Predicted numeric vector\n\nExamples\n\nactual = [1.1, 1.9, 3.0, 4.4, 5.0, 5.6]\npredicted = [0.9, 1.8, 2.5, 4.5, 5.0, 6.2]\nbias(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.percent_bias","page":"API Reference","title":"UnifiedMetrics.percent_bias","text":"percent_bias(actual, predicted)\n\nCompute the average amount that actual is greater than predicted as a percentage of the absolute value of actual.\n\nReturns -Inf, Inf, or NaN if any elements of actual are 0.\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth numeric vector\npredicted::AbstractVector{<:Real}: Predicted numeric vector\n\nExamples\n\nactual = [1.1, 1.9, 3.0, 4.4, 5.0, 5.6]\npredicted = [0.9, 1.8, 2.5, 4.5, 5.0, 6.2]\npercent_bias(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.mpe","page":"API Reference","title":"UnifiedMetrics.mpe","text":"mpe(actual, predicted)\n\nCompute the Mean Percentage Error (signed).\n\nUnlike MAPE, MPE can indicate systematic bias: positive values indicate under-prediction on average, negative values indicate over-prediction.\n\nReturns Inf, -Inf, or NaN if actual contains zeros.\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth numeric vector\npredicted::AbstractVector{<:Real}: Predicted numeric vector\n\nExamples\n\nactual = [1.1, 1.9, 3.0, 4.4, 5.0, 5.6]\npredicted = [0.9, 1.8, 2.5, 4.5, 5.0, 6.2]\nmpe(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.ape","page":"API Reference","title":"UnifiedMetrics.ape","text":"ape(actual, predicted)\n\nCompute the elementwise absolute percent error between two numeric vectors.\n\nReturns -Inf, Inf, or NaN if actual contains zeros.\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth numeric vector\npredicted::AbstractVector{<:Real}: Predicted numeric vector\n\nExamples\n\nactual = [1.1, 1.9, 3.0, 4.4, 5.0, 5.6]\npredicted = [0.9, 1.8, 2.5, 4.5, 5.0, 6.2]\nape(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.mape","page":"API Reference","title":"UnifiedMetrics.mape","text":"mape(actual, predicted)\n\nCompute the mean absolute percent error between two numeric vectors.\n\nReturns -Inf, Inf, or NaN if actual contains zeros. Due to instability at or near zero, smape or mase are often used as alternatives.\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth numeric vector\npredicted::AbstractVector{<:Real}: Predicted numeric vector\n\nExamples\n\nactual = [1.1, 1.9, 3.0, 4.4, 5.0, 5.6]\npredicted = [0.9, 1.8, 2.5, 4.5, 5.0, 6.2]\nmape(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.smape","page":"API Reference","title":"UnifiedMetrics.smape","text":"smape(actual, predicted)\n\nCompute the symmetric mean absolute percentage error between two numeric vectors.\n\nDefined as 2 * mean(abs(actual - predicted) / (abs(actual) + abs(predicted))). Returns NaN only if both actual and predicted are zero at the same position. Has an upper bound of 2.\n\nsmape is symmetric: smape(x, y) == smape(y, x).\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth numeric vector\npredicted::AbstractVector{<:Real}: Predicted numeric vector\n\nExamples\n\nactual = [1.1, 1.9, 3.0, 4.4, 5.0, 5.6]\npredicted = [0.9, 1.8, 2.5, 4.5, 5.0, 6.2]\nsmape(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.wmape","page":"API Reference","title":"UnifiedMetrics.wmape","text":"wmape(actual, predicted)\n\nCompute the Weighted Mean Absolute Percentage Error.\n\nWMAPE weights errors by the magnitude of actual values, making it more robust than MAPE when actual values vary significantly in magnitude.\n\nWMAPE = sum(|actual - predicted|) / sum(|actual|)\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth numeric vector\npredicted::AbstractVector{<:Real}: Predicted numeric vector\n\nExamples\n\nactual = [1.1, 1.9, 3.0, 4.4, 5.0, 5.6]\npredicted = [0.9, 1.8, 2.5, 4.5, 5.0, 6.2]\nwmape(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.sle","page":"API Reference","title":"UnifiedMetrics.sle","text":"sle(actual, predicted)\n\nCompute the elementwise squared log error between two numeric vectors.\n\nAdds one to both actual and predicted before taking the natural logarithm to avoid taking the log of zero. Not appropriate for negative values.\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth non-negative vector\npredicted::AbstractVector{<:Real}: Predicted non-negative vector\n\nExamples\n\nactual = [1.1, 1.9, 3.0, 4.4, 5.0, 5.6]\npredicted = [0.9, 1.8, 2.5, 4.5, 5.0, 6.2]\nsle(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.msle","page":"API Reference","title":"UnifiedMetrics.msle","text":"msle(actual, predicted)\n\nCompute the mean squared log error between two numeric vectors.\n\nAdds one to both actual and predicted before taking the natural logarithm to avoid taking the log of zero. Not appropriate for negative values.\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth non-negative vector\npredicted::AbstractVector{<:Real}: Predicted non-negative vector\n\nExamples\n\nactual = [1.1, 1.9, 3.0, 4.4, 5.0, 5.6]\npredicted = [0.9, 1.8, 2.5, 4.5, 5.0, 6.2]\nmsle(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.rmsle","page":"API Reference","title":"UnifiedMetrics.rmsle","text":"rmsle(actual, predicted)\n\nCompute the root mean squared log error between two numeric vectors.\n\nAdds one to both actual and predicted before taking the natural logarithm to avoid taking the log of zero. Not appropriate for negative values.\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth non-negative vector\npredicted::AbstractVector{<:Real}: Predicted non-negative vector\n\nExamples\n\nactual = [1.1, 1.9, 3.0, 4.4, 5.0, 5.6]\npredicted = [0.9, 1.8, 2.5, 4.5, 5.0, 6.2]\nrmsle(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.rse","page":"API Reference","title":"UnifiedMetrics.rse","text":"rse(actual, predicted)\n\nCompute the relative squared error between two numeric vectors.\n\nDivides sse(actual, predicted) by sse(actual, mean(actual)), providing the squared error relative to a naive model that predicts the mean for every data point.\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth numeric vector\npredicted::AbstractVector{<:Real}: Predicted numeric vector\n\nExamples\n\nactual = [1.1, 1.9, 3.0, 4.4, 5.0, 5.6]\npredicted = [0.9, 1.8, 2.5, 4.5, 5.0, 6.2]\nrse(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.rrse","page":"API Reference","title":"UnifiedMetrics.rrse","text":"rrse(actual, predicted)\n\nCompute the root relative squared error between two numeric vectors.\n\nTakes the square root of rse(actual, predicted).\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth numeric vector\npredicted::AbstractVector{<:Real}: Predicted numeric vector\n\nExamples\n\nactual = [1.1, 1.9, 3.0, 4.4, 5.0, 5.6]\npredicted = [0.9, 1.8, 2.5, 4.5, 5.0, 6.2]\nrrse(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.rae","page":"API Reference","title":"UnifiedMetrics.rae","text":"rae(actual, predicted)\n\nCompute the relative absolute error between two numeric vectors.\n\nDivides sum(ae(actual, predicted)) by sum(ae(actual, mean(actual))), providing the absolute error relative to a naive model that predicts the mean for every data point.\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth numeric vector\npredicted::AbstractVector{<:Real}: Predicted numeric vector\n\nExamples\n\nactual = [1.1, 1.9, 3.0, 4.4, 5.0, 5.6]\npredicted = [0.9, 1.8, 2.5, 4.5, 5.0, 6.2]\nrae(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.explained_variation","page":"API Reference","title":"UnifiedMetrics.explained_variation","text":"explained_variation(actual, predicted)\n\nCompute the explained variation (coefficient of determination, R²) between two numeric vectors.\n\nSubtracts rse(actual, predicted) from 1. Can return negative values if predictions are worse than predicting the mean.\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth numeric vector\npredicted::AbstractVector{<:Real}: Predicted numeric vector\n\nExamples\n\nactual = [1.1, 1.9, 3.0, 4.4, 5.0, 5.6]\npredicted = [0.9, 1.8, 2.5, 4.5, 5.0, 6.2]\nexplained_variation(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.adjusted_r2","page":"API Reference","title":"UnifiedMetrics.adjusted_r2","text":"adjusted_r2(actual, predicted, n_features)\n\nCompute the Adjusted R² (coefficient of determination adjusted for number of predictors).\n\nAdjusted R² penalizes the addition of irrelevant features to a model.\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth numeric vector\npredicted::AbstractVector{<:Real}: Predicted numeric vector\nn_features::Integer: Number of features/predictors in the model\n\nExamples\n\nactual = [1.1, 1.9, 3.0, 4.4, 5.0, 5.6]\npredicted = [0.9, 1.8, 2.5, 4.5, 5.0, 6.2]\nadjusted_r2(actual, predicted, 2)  # Model with 2 features\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.huber_loss","page":"API Reference","title":"UnifiedMetrics.huber_loss","text":"huber_loss(actual, predicted; delta=1.0)\n\nCompute the Huber loss, which is quadratic for small errors and linear for large errors.\n\nHuber loss is less sensitive to outliers than MSE. For errors smaller than delta, it behaves like MSE; for larger errors, it behaves like MAE.\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth numeric vector\npredicted::AbstractVector{<:Real}: Predicted numeric vector\ndelta::Real: Threshold where loss transitions from quadratic to linear (default: 1.0)\n\nExamples\n\nactual = [1.1, 1.9, 3.0, 4.4, 5.0, 5.6]\npredicted = [0.9, 1.8, 2.5, 4.5, 5.0, 6.2]\nhuber_loss(actual, predicted)\nhuber_loss(actual, predicted, delta=0.5)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.log_cosh_loss","page":"API Reference","title":"UnifiedMetrics.log_cosh_loss","text":"log_cosh_loss(actual, predicted)\n\nCompute the log-cosh loss, a smooth approximation of MAE.\n\nLog-cosh is approximately equal to (x^2)/2 for small x and abs(x) - log(2) for large x. It has the advantage of being twice differentiable everywhere.\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth numeric vector\npredicted::AbstractVector{<:Real}: Predicted numeric vector\n\nExamples\n\nactual = [1.1, 1.9, 3.0, 4.4, 5.0, 5.6]\npredicted = [0.9, 1.8, 2.5, 4.5, 5.0, 6.2]\nlog_cosh_loss(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.quantile_loss","page":"API Reference","title":"UnifiedMetrics.quantile_loss","text":"quantile_loss(actual, predicted; quantile=0.5)\n\nCompute the quantile (pinball) loss for quantile regression.\n\nThe quantile loss asymmetrically penalizes over-prediction and under-prediction. At quantile=0.5, this is equivalent to MAE. For quantile<0.5, under-prediction is penalized more; for quantile>0.5, over-prediction is penalized more.\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth numeric vector\npredicted::AbstractVector{<:Real}: Predicted numeric vector\nquantile::Real: Target quantile in (0, 1) (default: 0.5)\n\nExamples\n\nactual = [1.1, 1.9, 3.0, 4.4, 5.0, 5.6]\npredicted = [0.9, 1.8, 2.5, 4.5, 5.0, 6.2]\nquantile_loss(actual, predicted, quantile=0.5)  # Equivalent to MAE\nquantile_loss(actual, predicted, quantile=0.9)  # Penalize under-prediction more\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.pinball_loss","page":"API Reference","title":"UnifiedMetrics.pinball_loss","text":"pinball_loss(actual, predicted; quantile=0.5)\n\nAlias for quantile_loss. Compute the pinball loss for quantile regression.\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.tweedie_deviance","page":"API Reference","title":"UnifiedMetrics.tweedie_deviance","text":"tweedie_deviance(actual, predicted; power=1.5)\n\nCompute the Tweedie deviance for generalized linear models.\n\nThe power parameter controls the distribution assumption:\n\npower=0: Normal distribution (equivalent to MSE)\npower=1: Poisson distribution\npower=2: Gamma distribution\npower=3: Inverse Gaussian distribution\n1 < power < 2: Compound Poisson-Gamma (common for insurance claims)\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth non-negative vector\npredicted::AbstractVector{<:Real}: Predicted non-negative vector\npower::Real: Tweedie power parameter (default: 1.5)\n\nExamples\n\nactual = [1.1, 1.9, 3.0, 4.4, 5.0, 5.6]\npredicted = [0.9, 1.8, 2.5, 4.5, 5.0, 6.2]\ntweedie_deviance(actual, predicted, power=1.5)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.mean_poisson_deviance","page":"API Reference","title":"UnifiedMetrics.mean_poisson_deviance","text":"mean_poisson_deviance(actual, predicted)\n\nCompute the mean Poisson deviance.\n\nEquivalent to tweedie_deviance with power=1. Appropriate for count data.\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth non-negative vector (counts)\npredicted::AbstractVector{<:Real}: Predicted positive vector\n\nExamples\n\nactual = [1, 2, 3, 4, 5, 6]\npredicted = [1.1, 1.9, 3.1, 3.9, 5.1, 5.9]\nmean_poisson_deviance(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.mean_gamma_deviance","page":"API Reference","title":"UnifiedMetrics.mean_gamma_deviance","text":"mean_gamma_deviance(actual, predicted)\n\nCompute the mean Gamma deviance.\n\nEquivalent to tweedie_deviance with power=2. Appropriate for positive continuous targets with variance proportional to the square of the mean.\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth positive vector\npredicted::AbstractVector{<:Real}: Predicted positive vector\n\nExamples\n\nactual = [1.1, 1.9, 3.0, 4.4, 5.0, 5.6]\npredicted = [0.9, 1.8, 2.5, 4.5, 5.0, 6.2]\nmean_gamma_deviance(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.d2_tweedie_score","page":"API Reference","title":"UnifiedMetrics.d2_tweedie_score","text":"d2_tweedie_score(actual, predicted; power=1.5)\n\nCompute the D² (deviance explained) score using Tweedie deviance.\n\nSimilar to R² but uses Tweedie deviance instead of squared error. D² = 1 - deviance(actual, predicted) / deviance(actual, mean(actual))\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth non-negative vector\npredicted::AbstractVector{<:Real}: Predicted non-negative vector\npower::Real: Tweedie power parameter (default: 1.5)\n\nExamples\n\nactual = [1.1, 1.9, 3.0, 4.4, 5.0, 5.6]\npredicted = [0.9, 1.8, 2.5, 4.5, 5.0, 6.2]\nd2_tweedie_score(actual, predicted, power=1.5)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.accuracy","page":"API Reference","title":"UnifiedMetrics.accuracy","text":"accuracy(actual, predicted)\n\nCompute the classification accuracy (proportion of correctly classified observations).\n\nArguments\n\nactual::AbstractVector: Ground truth vector\npredicted::AbstractVector: Predicted vector\n\nExamples\n\nactual = ['a', 'a', 'c', 'b', 'c']\npredicted = ['a', 'b', 'c', 'b', 'a']\naccuracy(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.ce","page":"API Reference","title":"UnifiedMetrics.ce","text":"ce(actual, predicted)\n\nCompute the classification error (proportion of misclassified observations).\n\nArguments\n\nactual::AbstractVector: Ground truth vector\npredicted::AbstractVector: Predicted vector\n\nExamples\n\nactual = ['a', 'a', 'c', 'b', 'c']\npredicted = ['a', 'b', 'c', 'b', 'a']\nce(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.balanced_accuracy","page":"API Reference","title":"UnifiedMetrics.balanced_accuracy","text":"balanced_accuracy(actual, predicted)\n\nCompute balanced accuracy, which accounts for imbalanced datasets.\n\nBalanced accuracy is the macro-averaged recall: the average of recall scores for each class. It gives equal weight to each class regardless of its frequency.\n\nArguments\n\nactual::AbstractVector: Ground truth vector\npredicted::AbstractVector: Predicted vector\n\nExamples\n\nactual = [1, 1, 1, 1, 0, 0]  # Imbalanced: 4 positives, 2 negatives\npredicted = [1, 1, 1, 0, 0, 0]\nbalanced_accuracy(actual, predicted)  # (0.75 + 1.0) / 2 = 0.875\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.cohens_kappa","page":"API Reference","title":"UnifiedMetrics.cohens_kappa","text":"cohens_kappa(actual, predicted)\n\nCompute Cohen's Kappa coefficient for inter-rater agreement.\n\nKappa measures agreement between two raters, accounting for agreement by chance.\n\nκ = 1: Perfect agreement\nκ = 0: Agreement equivalent to chance\nκ < 0: Less agreement than chance\n\nArguments\n\nactual::AbstractVector: Ground truth vector\npredicted::AbstractVector: Predicted vector\n\nExamples\n\nactual = ['a', 'a', 'b', 'b', 'c', 'c']\npredicted = ['a', 'b', 'b', 'b', 'c', 'a']\ncohens_kappa(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.ScoreQuadraticWeightedKappa","page":"API Reference","title":"UnifiedMetrics.ScoreQuadraticWeightedKappa","text":"ScoreQuadraticWeightedKappa(rater_a, rater_b; min_rating=nothing, max_rating=nothing)\n\nCompute the quadratic weighted kappa between two vectors of integer ratings.\n\nArguments\n\nrater_a::AbstractVector{<:Integer}: First rater's ratings\nrater_b::AbstractVector{<:Integer}: Second rater's ratings\nmin_rating::Union{Integer,Nothing}: Minimum possible rating (default: minimum of both vectors)\nmax_rating::Union{Integer,Nothing}: Maximum possible rating (default: maximum of both vectors)\n\nExamples\n\nrater_a = [1, 4, 5, 5, 2, 1]\nrater_b = [2, 2, 4, 5, 3, 3]\nScoreQuadraticWeightedKappa(rater_a, rater_b, min_rating=1, max_rating=5)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.MeanQuadraticWeightedKappa","page":"API Reference","title":"UnifiedMetrics.MeanQuadraticWeightedKappa","text":"MeanQuadraticWeightedKappa(kappas; weights=nothing)\n\nCompute the mean quadratic weighted kappa, optionally weighted.\n\nUses Fisher's z-transformation for averaging.\n\nArguments\n\nkappas::AbstractVector{<:Real}: Vector of kappa values\nweights::Union{AbstractVector{<:Real},Nothing}: Optional weights (default: equal weights)\n\nExamples\n\nkappas = [0.3, 0.2, 0.2, 0.5, 0.1, 0.2]\nweights = [1.0, 2.5, 1.0, 1.0, 2.0, 3.0]\nMeanQuadraticWeightedKappa(kappas, weights=weights)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.matthews_corrcoef","page":"API Reference","title":"UnifiedMetrics.matthews_corrcoef","text":"matthews_corrcoef(actual, predicted)\n\nCompute the Matthews Correlation Coefficient (MCC) for binary classification.\n\nMCC is considered one of the best metrics for binary classification, especially for imbalanced datasets. It returns a value in [-1, 1]:\n\n+1: Perfect prediction\n0: Random prediction\n-1: Total disagreement\n\nArguments\n\nactual::AbstractVector{<:Real}: Binary ground truth (1 for positive, 0 for negative)\npredicted::AbstractVector{<:Real}: Binary predictions (1 for positive, 0 for negative)\n\nExamples\n\nactual = [1, 1, 1, 0, 0, 0]\npredicted = [1, 0, 1, 1, 0, 0]\nmatthews_corrcoef(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.mcc","page":"API Reference","title":"UnifiedMetrics.mcc","text":"mcc(actual, predicted)\n\nAlias for matthews_corrcoef. Compute the Matthews Correlation Coefficient.\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.confusion_matrix","page":"API Reference","title":"UnifiedMetrics.confusion_matrix","text":"confusion_matrix(actual, predicted)\n\nCompute the confusion matrix for classification.\n\nReturns a dictionary with keys:\n\n:matrix - The confusion matrix as a 2D array\n:labels - The class labels in order\n\nFor binary classification with classes 0 and 1:\n\nmatrix[1,1] = TN (true negatives)\nmatrix[1,2] = FP (false positives)\nmatrix[2,1] = FN (false negatives)\nmatrix[2,2] = TP (true positives)\n\nArguments\n\nactual::AbstractVector: Ground truth vector\npredicted::AbstractVector: Predicted vector\n\nExamples\n\nactual = [1, 1, 1, 0, 0, 0]\npredicted = [1, 0, 1, 1, 0, 0]\ncm = confusion_matrix(actual, predicted)\ncm[:matrix]  # 2x2 confusion matrix\ncm[:labels]  # [0, 1]\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.top_k_accuracy","page":"API Reference","title":"UnifiedMetrics.top_k_accuracy","text":"top_k_accuracy(actual, predicted_probs, k)\n\nCompute top-k accuracy for multi-class classification.\n\nReturns the fraction of samples where the true class is among the top k predictions.\n\nArguments\n\nactual::AbstractVector{<:Integer}: Ground truth class indices (1-indexed)\npredicted_probs::AbstractMatrix{<:Real}: Matrix of predicted probabilities (samples × classes)\nk::Integer: Number of top predictions to consider\n\nExamples\n\nactual = [1, 2, 3, 1]\npredicted_probs = [0.8 0.1 0.1;   # Sample 1: class 1 most likely\n                   0.2 0.5 0.3;   # Sample 2: class 2 most likely\n                   0.1 0.3 0.6;   # Sample 3: class 3 most likely\n                   0.3 0.4 0.3]   # Sample 4: class 2 most likely (actual is 1)\ntop_k_accuracy(actual, predicted_probs, 1)  # Standard accuracy\ntop_k_accuracy(actual, predicted_probs, 2)  # Top-2 accuracy\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.hamming_loss","page":"API Reference","title":"UnifiedMetrics.hamming_loss","text":"hamming_loss(actual, predicted)\n\nCompute the Hamming loss (fraction of misclassified labels).\n\nFor single-label classification, this equals the classification error (1 - accuracy). For multi-label classification, it measures the fraction of incorrect labels.\n\nArguments\n\nactual::AbstractVector: Ground truth vector\npredicted::AbstractVector: Predicted vector\n\nExamples\n\nactual = ['a', 'a', 'b', 'b', 'c', 'c']\npredicted = ['a', 'b', 'b', 'b', 'c', 'a']\nhamming_loss(actual, predicted)\n\n\n\n\n\nhamming_loss(actual, predicted)\n\nCompute Hamming loss for multi-label classification.\n\nArguments\n\nactual::AbstractMatrix{Bool}: Ground truth binary matrix (samples × labels)\npredicted::AbstractMatrix{Bool}: Predicted binary matrix (samples × labels)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.zero_one_loss","page":"API Reference","title":"UnifiedMetrics.zero_one_loss","text":"zero_one_loss(actual, predicted)\n\nCompute the zero-one loss (fraction of samples with any incorrect prediction).\n\nFor single-label classification, this equals the classification error.\n\nArguments\n\nactual::AbstractVector: Ground truth vector\npredicted::AbstractVector: Predicted vector\n\nExamples\n\nactual = ['a', 'a', 'b', 'b']\npredicted = ['a', 'b', 'b', 'b']\nzero_one_loss(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.hinge_loss","page":"API Reference","title":"UnifiedMetrics.hinge_loss","text":"hinge_loss(actual, predicted)\n\nCompute the hinge loss for binary classification (used in SVMs).\n\nActual values should be -1 or 1. Predicted values are the decision function values (not probabilities).\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth (-1 or 1)\npredicted::AbstractVector{<:Real}: Decision function values (raw scores)\n\nExamples\n\nactual = [1, 1, -1, -1]\npredicted = [0.8, 0.3, -0.5, 0.1]  # Decision function values\nhinge_loss(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.squared_hinge_loss","page":"API Reference","title":"UnifiedMetrics.squared_hinge_loss","text":"squared_hinge_loss(actual, predicted)\n\nCompute the squared hinge loss (used in some SVMs).\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth (-1 or 1)\npredicted::AbstractVector{<:Real}: Decision function values (raw scores)\n\nExamples\n\nactual = [1, 1, -1, -1]\npredicted = [0.8, 0.3, -0.5, 0.1]\nsquared_hinge_loss(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.auc","page":"API Reference","title":"UnifiedMetrics.auc","text":"auc(actual, predicted)\n\nCompute the area under the ROC curve (AUC).\n\nUses the Mann-Whitney U statistic for fast computation without building the ROC curve.\n\nArguments\n\nactual::AbstractVector{<:Real}: Binary ground truth (1 for positive, 0 for negative)\npredicted::AbstractVector{<:Real}: Predicted scores (higher = more likely positive)\n\nExamples\n\nactual = [1, 1, 1, 0, 0, 0]\npredicted = [0.9, 0.8, 0.4, 0.5, 0.3, 0.2]\nauc(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.gini_coefficient","page":"API Reference","title":"UnifiedMetrics.gini_coefficient","text":"gini_coefficient(actual, predicted)\n\nCompute the Gini coefficient from AUC.\n\nGini = 2 * AUC - 1\n\nThe Gini coefficient ranges from -1 to 1:\n\n1: Perfect prediction\n0: Random prediction\n-1: Perfectly wrong prediction\n\nArguments\n\nactual::AbstractVector{<:Real}: Binary ground truth (1 for positive, 0 for negative)\npredicted::AbstractVector{<:Real}: Predicted scores (higher = more likely positive)\n\nExamples\n\nactual = [1, 1, 1, 0, 0, 0]\npredicted = [0.9, 0.8, 0.4, 0.5, 0.3, 0.2]\ngini_coefficient(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.ks_statistic","page":"API Reference","title":"UnifiedMetrics.ks_statistic","text":"ks_statistic(actual, predicted)\n\nCompute the Kolmogorov-Smirnov statistic for binary classification.\n\nKS statistic is the maximum difference between the cumulative distribution functions of positive and negative classes. Higher values indicate better discrimination.\n\nArguments\n\nactual::AbstractVector{<:Real}: Binary ground truth (1 for positive, 0 for negative)\npredicted::AbstractVector{<:Real}: Predicted scores (higher = more likely positive)\n\nExamples\n\nactual = [1, 1, 1, 0, 0, 0]\npredicted = [0.9, 0.8, 0.4, 0.5, 0.3, 0.2]\nks_statistic(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.ll","page":"API Reference","title":"UnifiedMetrics.ll","text":"ll(actual, predicted)\n\nCompute the elementwise log loss (cross-entropy loss).\n\nArguments\n\nactual::AbstractVector{<:Real}: Binary ground truth (1 for positive, 0 for negative)\npredicted::AbstractVector{<:Real}: Predicted probabilities for positive class\n\nExamples\n\nactual = [1, 1, 1, 0, 0, 0]\npredicted = [0.9, 0.8, 0.4, 0.5, 0.3, 0.2]\nll(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.logloss","page":"API Reference","title":"UnifiedMetrics.logloss","text":"logloss(actual, predicted)\n\nCompute the mean log loss (cross-entropy loss).\n\nArguments\n\nactual::AbstractVector{<:Real}: Binary ground truth (1 for positive, 0 for negative)\npredicted::AbstractVector{<:Real}: Predicted probabilities for positive class\n\nExamples\n\nactual = [1, 1, 1, 0, 0, 0]\npredicted = [0.9, 0.8, 0.4, 0.5, 0.3, 0.2]\nlogloss(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.brier_score","page":"API Reference","title":"UnifiedMetrics.brier_score","text":"brier_score(actual, predicted)\n\nCompute the Brier score for probability predictions.\n\nThe Brier score is the mean squared error of predicted probabilities compared to binary outcomes. Lower is better (0 is perfect, 1 is worst).\n\nArguments\n\nactual::AbstractVector{<:Real}: Binary ground truth (1 for positive, 0 for negative)\npredicted::AbstractVector{<:Real}: Predicted probabilities for positive class [0, 1]\n\nExamples\n\nactual = [1, 1, 1, 0, 0, 0]\npredicted = [0.9, 0.8, 0.4, 0.5, 0.3, 0.2]\nbrier_score(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.precision","page":"API Reference","title":"UnifiedMetrics.precision","text":"precision(actual, predicted)\n\nCompute precision (positive predictive value).\n\nProportion of positive predictions that are actually positive.\n\nArguments\n\nactual::AbstractVector{<:Real}: Binary ground truth (1 for positive, 0 for negative)\npredicted::AbstractVector{<:Real}: Binary predictions (1 for positive, 0 for negative)\n\nExamples\n\nactual = [1, 1, 1, 0, 0, 0]\npredicted = [1, 1, 1, 1, 1, 1]\nprecision(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.recall","page":"API Reference","title":"UnifiedMetrics.recall","text":"recall(actual, predicted)\n\nCompute recall (sensitivity, true positive rate).\n\nProportion of actual positives that are correctly predicted.\n\nArguments\n\nactual::AbstractVector{<:Real}: Binary ground truth (1 for positive, 0 for negative)\npredicted::AbstractVector{<:Real}: Binary predictions (1 for positive, 0 for negative)\n\nExamples\n\nactual = [1, 1, 1, 0, 0, 0]\npredicted = [1, 0, 1, 1, 1, 1]\nrecall(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.sensitivity","page":"API Reference","title":"UnifiedMetrics.sensitivity","text":"sensitivity(actual, predicted)\n\nAlias for recall. Compute sensitivity (true positive rate).\n\nProportion of actual positives that are correctly predicted.\n\nArguments\n\nactual::AbstractVector{<:Real}: Binary ground truth (1 for positive, 0 for negative)\npredicted::AbstractVector{<:Real}: Binary predictions (1 for positive, 0 for negative)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.specificity","page":"API Reference","title":"UnifiedMetrics.specificity","text":"specificity(actual, predicted)\n\nCompute specificity (true negative rate, selectivity).\n\nProportion of actual negatives that are correctly predicted.\n\nArguments\n\nactual::AbstractVector{<:Real}: Binary ground truth (1 for positive, 0 for negative)\npredicted::AbstractVector{<:Real}: Binary predictions (1 for positive, 0 for negative)\n\nExamples\n\nactual = [1, 1, 1, 0, 0, 0]\npredicted = [1, 0, 1, 1, 0, 0]\nspecificity(actual, predicted)  # 2/3 = 0.667\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.npv","page":"API Reference","title":"UnifiedMetrics.npv","text":"npv(actual, predicted)\n\nCompute the Negative Predictive Value.\n\nProportion of negative predictions that are actually negative.\n\nArguments\n\nactual::AbstractVector{<:Real}: Binary ground truth (1 for positive, 0 for negative)\npredicted::AbstractVector{<:Real}: Binary predictions (1 for positive, 0 for negative)\n\nExamples\n\nactual = [1, 1, 1, 0, 0, 0]\npredicted = [1, 0, 1, 1, 0, 0]\nnpv(actual, predicted)  # 2/3 = 0.667 (of negative predictions, 2 of 3 are correct)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.fbeta_score","page":"API Reference","title":"UnifiedMetrics.fbeta_score","text":"fbeta_score(actual, predicted; beta=1.0)\n\nCompute the F-beta score, a weighted harmonic mean of precision and recall.\n\nWhen beta=1, this is the F1 score (equal weight to precision and recall). When beta<1, precision is weighted more heavily. When beta>1, recall is weighted more heavily.\n\nArguments\n\nactual::AbstractVector{<:Real}: Binary ground truth (1 for positive, 0 for negative)\npredicted::AbstractVector{<:Real}: Binary predictions (1 for positive, 0 for negative)\nbeta::Real: Weight parameter (default: 1.0)\n\nExamples\n\nactual = [1, 1, 1, 0, 0, 0]\npredicted = [1, 0, 1, 1, 1, 1]\nfbeta_score(actual, predicted)  # F1 score\nfbeta_score(actual, predicted, beta=0.5)  # F0.5 score (precision-weighted)\nfbeta_score(actual, predicted, beta=2.0)  # F2 score (recall-weighted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.fpr","page":"API Reference","title":"UnifiedMetrics.fpr","text":"fpr(actual, predicted)\n\nCompute the False Positive Rate (fall-out, 1 - specificity).\n\nProportion of actual negatives that are incorrectly predicted as positive.\n\nArguments\n\nactual::AbstractVector{<:Real}: Binary ground truth (1 for positive, 0 for negative)\npredicted::AbstractVector{<:Real}: Binary predictions (1 for positive, 0 for negative)\n\nExamples\n\nactual = [1, 1, 1, 0, 0, 0]\npredicted = [1, 0, 1, 1, 0, 0]\nfpr(actual, predicted)  # 1/3 = 0.333\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.fnr","page":"API Reference","title":"UnifiedMetrics.fnr","text":"fnr(actual, predicted)\n\nCompute the False Negative Rate (miss rate, 1 - recall).\n\nProportion of actual positives that are incorrectly predicted as negative.\n\nArguments\n\nactual::AbstractVector{<:Real}: Binary ground truth (1 for positive, 0 for negative)\npredicted::AbstractVector{<:Real}: Binary predictions (1 for positive, 0 for negative)\n\nExamples\n\nactual = [1, 1, 1, 0, 0, 0]\npredicted = [1, 0, 1, 1, 0, 0]\nfnr(actual, predicted)  # 1/3 = 0.333\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.youden_j","page":"API Reference","title":"UnifiedMetrics.youden_j","text":"youden_j(actual, predicted)\n\nCompute Youden's J statistic (informedness).\n\nJ = sensitivity + specificity - 1 = TPR - FPR\n\nRanges from -1 to 1, where 1 indicates perfect prediction.\n\nArguments\n\nactual::AbstractVector{<:Real}: Binary ground truth (1 for positive, 0 for negative)\npredicted::AbstractVector{<:Real}: Binary predictions (1 for positive, 0 for negative)\n\nExamples\n\nactual = [1, 1, 1, 0, 0, 0]\npredicted = [1, 0, 1, 1, 0, 0]\nyouden_j(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.markedness","page":"API Reference","title":"UnifiedMetrics.markedness","text":"markedness(actual, predicted)\n\nCompute markedness (deltaP, Δp).\n\nMarkedness = PPV + NPV - 1 = precision + NPV - 1\n\nThe counterpart to informedness (Youden's J) in the prediction direction.\n\nArguments\n\nactual::AbstractVector{<:Real}: Binary ground truth (1 for positive, 0 for negative)\npredicted::AbstractVector{<:Real}: Binary predictions (1 for positive, 0 for negative)\n\nExamples\n\nactual = [1, 1, 1, 0, 0, 0]\npredicted = [1, 0, 1, 1, 0, 0]\nmarkedness(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.fowlkes_mallows_index","page":"API Reference","title":"UnifiedMetrics.fowlkes_mallows_index","text":"fowlkes_mallows_index(actual, predicted)\n\nCompute the Fowlkes-Mallows index.\n\nFM = sqrt(PPV × TPR) = sqrt(precision × recall)\n\nGeometric mean of precision and recall, ranges from 0 to 1.\n\nArguments\n\nactual::AbstractVector{<:Real}: Binary ground truth (1 for positive, 0 for negative)\npredicted::AbstractVector{<:Real}: Binary predictions (1 for positive, 0 for negative)\n\nExamples\n\nactual = [1, 1, 1, 0, 0, 0]\npredicted = [1, 0, 1, 1, 0, 0]\nfowlkes_mallows_index(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.positive_likelihood_ratio","page":"API Reference","title":"UnifiedMetrics.positive_likelihood_ratio","text":"positive_likelihood_ratio(actual, predicted)\n\nCompute the Positive Likelihood Ratio (LR+).\n\nLR+ = TPR / FPR = sensitivity / (1 - specificity)\n\nIndicates how much more likely a positive prediction is for actual positives. Higher values are better.\n\nArguments\n\nactual::AbstractVector{<:Real}: Binary ground truth (1 for positive, 0 for negative)\npredicted::AbstractVector{<:Real}: Binary predictions (1 for positive, 0 for negative)\n\nExamples\n\nactual = [1, 1, 1, 0, 0, 0]\npredicted = [1, 0, 1, 1, 0, 0]\npositive_likelihood_ratio(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.negative_likelihood_ratio","page":"API Reference","title":"UnifiedMetrics.negative_likelihood_ratio","text":"negative_likelihood_ratio(actual, predicted)\n\nCompute the Negative Likelihood Ratio (LR-).\n\nLR- = FNR / TNR = (1 - sensitivity) / specificity\n\nIndicates how much more likely a negative prediction is for actual positives. Lower values are better.\n\nArguments\n\nactual::AbstractVector{<:Real}: Binary ground truth (1 for positive, 0 for negative)\npredicted::AbstractVector{<:Real}: Binary predictions (1 for positive, 0 for negative)\n\nExamples\n\nactual = [1, 1, 1, 0, 0, 0]\npredicted = [1, 0, 1, 1, 0, 0]\nnegative_likelihood_ratio(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.diagnostic_odds_ratio","page":"API Reference","title":"UnifiedMetrics.diagnostic_odds_ratio","text":"diagnostic_odds_ratio(actual, predicted)\n\nCompute the Diagnostic Odds Ratio (DOR).\n\nDOR = LR+ / LR- = (TP × TN) / (FP × FN)\n\nThe ratio of the odds of a positive prediction in actual positives to the odds in actual negatives. Higher values indicate better discrimination.\n\nArguments\n\nactual::AbstractVector{<:Real}: Binary ground truth (1 for positive, 0 for negative)\npredicted::AbstractVector{<:Real}: Binary predictions (1 for positive, 0 for negative)\n\nExamples\n\nactual = [1, 1, 1, 0, 0, 0]\npredicted = [1, 0, 1, 1, 0, 0]\ndiagnostic_odds_ratio(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.lift","page":"API Reference","title":"UnifiedMetrics.lift","text":"lift(actual, predicted; percentile=0.1)\n\nCompute the lift at a given percentile.\n\nLift measures how much better the model is at identifying positives compared to random selection. Lift = (% positives in top X%) / (% positives overall).\n\nArguments\n\nactual::AbstractVector{<:Real}: Binary ground truth (1 for positive, 0 for negative)\npredicted::AbstractVector{<:Real}: Predicted scores (higher = more likely positive)\npercentile::Real: Top fraction to consider (default: 0.1 = top 10%)\n\nExamples\n\nactual = [1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\npredicted = [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.0]\nlift(actual, predicted, percentile=0.3)  # Lift in top 30%\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.gain","page":"API Reference","title":"UnifiedMetrics.gain","text":"gain(actual, predicted; percentile=0.1)\n\nCompute the cumulative gain at a given percentile.\n\nGain measures what percentage of total positives are captured in the top X%.\n\nArguments\n\nactual::AbstractVector{<:Real}: Binary ground truth (1 for positive, 0 for negative)\npredicted::AbstractVector{<:Real}: Predicted scores (higher = more likely positive)\npercentile::Real: Top fraction to consider (default: 0.1 = top 10%)\n\nExamples\n\nactual = [1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\npredicted = [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.0]\ngain(actual, predicted, percentile=0.3)  # What % of positives in top 30%\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.f1","page":"API Reference","title":"UnifiedMetrics.f1","text":"f1(actual, predicted)\n\nCompute the F1 score in the context of information retrieval.\n\nComputes 2 * precision * recall / (precision + recall) where precision is the proportion of retrieved documents that are relevant, and recall is the proportion of relevant documents that are retrieved.\n\nReturns 0 if there are no true positives.\n\nArguments\n\nactual::AbstractVector: Ground truth relevant documents (order doesn't matter)\npredicted::AbstractVector: Retrieved documents (order doesn't matter)\n\nExamples\n\nactual = ['a', 'c', 'd']\npredicted = ['d', 'e']\nf1(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.precision_at_k","page":"API Reference","title":"UnifiedMetrics.precision_at_k","text":"precision_at_k(actual, predicted; k=10)\n\nCompute precision@k for information retrieval.\n\nThe fraction of top k predictions that are relevant.\n\nArguments\n\nactual::AbstractVector: Ground truth relevant items\npredicted::AbstractVector: Ranked predictions (highest rank first)\nk::Integer: Number of top predictions to consider (default: 10)\n\nExamples\n\nactual = [\"a\", \"b\", \"c\", \"d\"]\npredicted = [\"a\", \"x\", \"b\", \"y\", \"z\"]\nprecision_at_k(actual, predicted, k=3)  # 2/3 (2 of top 3 are relevant)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.recall_at_k","page":"API Reference","title":"UnifiedMetrics.recall_at_k","text":"recall_at_k(actual, predicted; k=10)\n\nCompute recall@k for information retrieval.\n\nThe fraction of relevant items that appear in the top k predictions.\n\nArguments\n\nactual::AbstractVector: Ground truth relevant items\npredicted::AbstractVector: Ranked predictions (highest rank first)\nk::Integer: Number of top predictions to consider (default: 10)\n\nExamples\n\nactual = [\"a\", \"b\", \"c\", \"d\"]\npredicted = [\"a\", \"x\", \"b\", \"y\", \"z\"]\nrecall_at_k(actual, predicted, k=3)  # 2/4 = 0.5 (found \"a\" and \"b\" in top 3)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.f1_at_k","page":"API Reference","title":"UnifiedMetrics.f1_at_k","text":"f1_at_k(actual, predicted; k=10)\n\nCompute F1@k for information retrieval.\n\nHarmonic mean of precision@k and recall@k.\n\nArguments\n\nactual::AbstractVector: Ground truth relevant items\npredicted::AbstractVector: Ranked predictions (highest rank first)\nk::Integer: Number of top predictions to consider (default: 10)\n\nExamples\n\nactual = [\"a\", \"b\", \"c\", \"d\"]\npredicted = [\"a\", \"x\", \"b\", \"y\", \"z\"]\nf1_at_k(actual, predicted, k=3)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.dcg","page":"API Reference","title":"UnifiedMetrics.dcg","text":"dcg(relevance; k=nothing)\n\nCompute the Discounted Cumulative Gain at position k.\n\nDCG measures the usefulness of a ranking based on relevance scores, with a logarithmic discount to penalize relevant items appearing lower in the ranking.\n\nDCG = Σ (2^rel_i - 1) / log2(i + 1)\n\nArguments\n\nrelevance::AbstractVector{<:Real}: Relevance scores in ranked order (highest rank first)\nk::Union{Integer,Nothing}: Number of positions to consider (default: all)\n\nExamples\n\nrelevance = [3, 2, 3, 0, 1, 2]  # Relevance scores for ranked items\ndcg(relevance)  # DCG for all positions\ndcg(relevance, k=3)  # DCG@3\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.idcg","page":"API Reference","title":"UnifiedMetrics.idcg","text":"idcg(relevance; k=nothing)\n\nCompute the Ideal Discounted Cumulative Gain at position k.\n\nIDCG is the DCG of the best possible ranking (relevance scores sorted descending).\n\nArguments\n\nrelevance::AbstractVector{<:Real}: Relevance scores (order doesn't matter)\nk::Union{Integer,Nothing}: Number of positions to consider (default: all)\n\nExamples\n\nrelevance = [3, 2, 3, 0, 1, 2]\nidcg(relevance)  # Ideal DCG for all positions\nidcg(relevance, k=3)  # Ideal DCG@3\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.ndcg","page":"API Reference","title":"UnifiedMetrics.ndcg","text":"ndcg(relevance; k=nothing)\n\nCompute the Normalized Discounted Cumulative Gain at position k.\n\nNDCG = DCG / IDCG\n\nNormalizes DCG to [0, 1] by dividing by the ideal DCG.\n\nArguments\n\nrelevance::AbstractVector{<:Real}: Relevance scores in ranked order (highest rank first)\nk::Union{Integer,Nothing}: Number of positions to consider (default: all)\n\nExamples\n\nrelevance = [3, 2, 3, 0, 1, 2]  # Actual ranking\nndcg(relevance)  # NDCG for all positions\nndcg(relevance, k=3)  # NDCG@3\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.mean_ndcg","page":"API Reference","title":"UnifiedMetrics.mean_ndcg","text":"mean_ndcg(relevances; k=nothing)\n\nCompute the mean NDCG over multiple queries.\n\nArguments\n\nrelevances::AbstractVector{<:AbstractVector{<:Real}}: List of relevance score vectors\nk::Union{Integer,Nothing}: Number of positions to consider (default: all)\n\nExamples\n\nrelevances = [[3, 2, 1, 0], [2, 1, 2, 1], [1, 1, 0, 0]]\nmean_ndcg(relevances)  # Mean NDCG across queries\nmean_ndcg(relevances, k=2)  # Mean NDCG@2\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.apk","page":"API Reference","title":"UnifiedMetrics.apk","text":"apk(k, actual, predicted)\n\nCompute the average precision at k.\n\nLoops over the first k values of predicted. For each value that is in actual and hasn't been predicted before, increments the score by (number of hits so far) / position. Returns the final score divided by min(length(actual), k).\n\nReturns NaN if actual is empty.\n\nArguments\n\nk::Integer: Number of predictions to consider\nactual::AbstractVector: Ground truth relevant documents (order doesn't matter)\npredicted::AbstractVector: Retrieved documents in ranked order (most relevant first)\n\nExamples\n\nactual = ['a', 'b', 'd']\npredicted = ['b', 'c', 'a', 'e', 'f']\napk(3, actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.mapk","page":"API Reference","title":"UnifiedMetrics.mapk","text":"mapk(k, actual, predicted)\n\nCompute the mean average precision at k.\n\nEvaluates apk for each pair of elements from actual and predicted lists, then returns the mean.\n\nArguments\n\nk::Integer: Number of predictions to consider for each query\nactual::AbstractVector{<:AbstractVector}: List of ground truth vectors\npredicted::AbstractVector{<:AbstractVector}: List of prediction vectors\n\nExamples\n\nactual = [['a', 'b'], ['a'], ['x', 'y', 'b']]\npredicted = [['a', 'c', 'd'], ['x', 'b', 'a', 'b'], ['y']]\nmapk(2, actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.reciprocal_rank","page":"API Reference","title":"UnifiedMetrics.reciprocal_rank","text":"reciprocal_rank(actual, predicted)\n\nCompute the Reciprocal Rank for a single query.\n\nArguments\n\nactual::AbstractVector: Ground truth relevant items\npredicted::AbstractVector: Ranked predictions (highest rank first)\n\nExamples\n\nactual = [\"a\", \"b\"]\npredicted = [\"c\", \"a\", \"b\", \"d\"]\nreciprocal_rank(actual, predicted)  # 1/2 = 0.5 (first relevant at position 2)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.mrr","page":"API Reference","title":"UnifiedMetrics.mrr","text":"mrr(actual, predicted)\n\nCompute the Mean Reciprocal Rank.\n\nMRR is the average of reciprocal ranks of the first relevant item for each query.\n\nArguments\n\nactual::AbstractVector{<:AbstractVector}: List of ground truth relevant items for each query\npredicted::AbstractVector{<:AbstractVector}: List of ranked predictions for each query\n\nExamples\n\nactual = [[\"a\", \"b\"], [\"c\"], [\"d\", \"e\"]]\npredicted = [[\"b\", \"a\", \"c\"], [\"a\", \"c\", \"d\"], [\"e\", \"d\", \"f\"]]\nmrr(actual, predicted)  # (1/2 + 1/2 + 1/1) / 3 = 0.667\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.hit_rate","page":"API Reference","title":"UnifiedMetrics.hit_rate","text":"hit_rate(actual, predicted; k=10)\n\nCompute the hit rate (recall@k) for recommendation systems.\n\nHit rate is the fraction of queries where at least one relevant item appears in the top k predictions.\n\nArguments\n\nactual::AbstractVector{<:AbstractVector}: List of ground truth relevant items for each query\npredicted::AbstractVector{<:AbstractVector}: List of ranked predictions for each query\nk::Integer: Number of top predictions to consider (default: 10)\n\nExamples\n\nactual = [[\"a\", \"b\"], [\"c\"], [\"d\", \"e\"]]\npredicted = [[\"a\", \"x\", \"y\"], [\"x\", \"y\", \"z\"], [\"e\", \"f\", \"g\"]]\nhit_rate(actual, predicted, k=3)  # 2/3 queries have a hit in top 3\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.coverage","page":"API Reference","title":"UnifiedMetrics.coverage","text":"coverage(predicted, catalog)\n\nCompute the catalog coverage of recommendations.\n\nCoverage measures what fraction of items in the catalog have been recommended at least once across all predictions.\n\nArguments\n\npredicted::AbstractVector{<:AbstractVector}: List of predictions for all queries\ncatalog::AbstractVector: Full catalog of items\n\nExamples\n\ncatalog = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"]\npredicted = [[\"a\", \"b\"], [\"a\", \"c\"], [\"b\", \"d\"]]\ncoverage(predicted, catalog)  # 4/6 = 0.667 (recommended: a, b, c, d)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.novelty","page":"API Reference","title":"UnifiedMetrics.novelty","text":"novelty(predicted, item_popularity)\n\nCompute the novelty of recommendations.\n\nNovelty measures how unexpected/surprising the recommendations are, based on the popularity of recommended items. Higher novelty means recommending less popular (long-tail) items.\n\nArguments\n\npredicted::AbstractVector{<:AbstractVector}: List of predictions for all queries\nitem_popularity::Dict: Dictionary mapping items to their popularity (0-1)\n\nExamples\n\npopularity = Dict(\"a\" => 0.9, \"b\" => 0.5, \"c\" => 0.1, \"d\" => 0.05)\npredicted = [[\"a\", \"b\"], [\"c\", \"d\"]]\nnovelty(predicted, popularity)  # Average -log2(popularity)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.mase","page":"API Reference","title":"UnifiedMetrics.mase","text":"mase(actual, predicted; m=1)\n\nCompute the Mean Absolute Scaled Error for time series data.\n\nMASE compares the prediction error to the error of a naive forecast. The naive forecast predicts the value from m periods ago (seasonal naive for m > 1).\n\nMASE < 1 indicates the model outperforms the naive forecast. MASE = 1 indicates performance equal to naive forecast. MASE > 1 indicates the model underperforms the naive forecast.\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth time series (ordered by time)\npredicted::AbstractVector{<:Real}: Predicted time series\nm::Integer: Seasonal period / frequency (default: 1)\nm=1: Non-seasonal naive forecast (random walk)\nm=4: Quarterly seasonality (compare with same quarter last year)\nm=7: Weekly seasonality for daily data\nm=12: Monthly seasonality (compare with same month last year)\nm=52: Weekly seasonality for weekly data\n\nExamples\n\nactual = [1.1, 1.9, 3.0, 4.4, 5.0, 5.6]\npredicted = [0.9, 1.8, 2.5, 4.5, 5.0, 6.2]\nmase(actual, predicted)  # Non-seasonal (m=1)\nmase(actual, predicted, m=2)  # With seasonal period 2\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.msse","page":"API Reference","title":"UnifiedMetrics.msse","text":"msse(actual, predicted; m=1)\n\nCompute the Mean Squared Scaled Error for time series data.\n\nSimilar to MASE but uses squared errors, making it more sensitive to large errors.\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth time series (ordered by time)\npredicted::AbstractVector{<:Real}: Predicted time series\nm::Integer: Seasonal period / frequency (default: 1)\n\nExamples\n\nactual = [1.1, 1.9, 3.0, 4.4, 5.0, 5.6]\npredicted = [0.9, 1.8, 2.5, 4.5, 5.0, 6.2]\nmsse(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.rmsse","page":"API Reference","title":"UnifiedMetrics.rmsse","text":"rmsse(actual, predicted; m=1)\n\nCompute the Root Mean Squared Scaled Error for time series data.\n\nSquare root of MSSE, on the same scale as the original data.\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth time series (ordered by time)\npredicted::AbstractVector{<:Real}: Predicted time series\nm::Integer: Seasonal period / frequency (default: 1)\n\nExamples\n\nactual = [1.1, 1.9, 3.0, 4.4, 5.0, 5.6]\npredicted = [0.9, 1.8, 2.5, 4.5, 5.0, 6.2]\nrmsse(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.tracking_signal","page":"API Reference","title":"UnifiedMetrics.tracking_signal","text":"tracking_signal(actual, predicted)\n\nCompute the tracking signal for forecast monitoring.\n\nTracking signal = Cumulative Forecast Error / MAD\n\nUsed to detect forecast bias. Values outside [-4, 4] typically indicate bias.\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth time series\npredicted::AbstractVector{<:Real}: Predicted time series\n\nExamples\n\nactual = [100, 110, 105, 115, 120]\npredicted = [98, 108, 110, 112, 125]\ntracking_signal(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.forecast_bias","page":"API Reference","title":"UnifiedMetrics.forecast_bias","text":"forecast_bias(actual, predicted)\n\nCompute the forecast bias (cumulative forecast error normalized by number of periods).\n\nPositive bias indicates systematic under-forecasting. Negative bias indicates systematic over-forecasting.\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth time series\npredicted::AbstractVector{<:Real}: Predicted time series\n\nExamples\n\nactual = [100, 110, 105, 115, 120]\npredicted = [98, 108, 110, 112, 125]\nforecast_bias(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.theil_u1","page":"API Reference","title":"UnifiedMetrics.theil_u1","text":"theil_u1(actual, predicted)\n\nCompute Theil's U1 statistic (inequality coefficient).\n\nU1 ranges from 0 to 1:\n\n0: Perfect forecast\n1: Worst possible forecast\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth time series\npredicted::AbstractVector{<:Real}: Predicted time series\n\nExamples\n\nactual = [1.1, 1.9, 3.0, 4.4, 5.0, 5.6]\npredicted = [0.9, 1.8, 2.5, 4.5, 5.0, 6.2]\ntheil_u1(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.theil_u2","page":"API Reference","title":"UnifiedMetrics.theil_u2","text":"theil_u2(actual, predicted; m=1)\n\nCompute Theil's U2 statistic (compares to naive forecast).\n\nU2 < 1: Model outperforms naive forecast U2 = 1: Model equals naive forecast U2 > 1: Model underperforms naive forecast\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth time series\npredicted::AbstractVector{<:Real}: Predicted time series\nm::Integer: Seasonal period for naive forecast (default: 1)\n\nExamples\n\nactual = [1.1, 1.9, 3.0, 4.4, 5.0, 5.6]\npredicted = [0.9, 1.8, 2.5, 4.5, 5.0, 6.2]\ntheil_u2(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.wape","page":"API Reference","title":"UnifiedMetrics.wape","text":"wape(actual, predicted)\n\nCompute the Weighted Absolute Percentage Error for time series.\n\nWAPE = Σ|actual - predicted| / Σ|actual|\n\nUnlike MAPE, WAPE is well-defined when actual values are zero and gives more weight to larger values.\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth time series\npredicted::AbstractVector{<:Real}: Predicted time series\n\nExamples\n\nactual = [100, 200, 150, 300, 250]\npredicted = [110, 190, 160, 290, 260]\nwape(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.directional_accuracy","page":"API Reference","title":"UnifiedMetrics.directional_accuracy","text":"directional_accuracy(actual, predicted)\n\nCompute the directional accuracy (hit rate for direction of change).\n\nMeasures how often the forecast correctly predicts whether the value goes up or down.\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth time series\npredicted::AbstractVector{<:Real}: Predicted time series\n\nExamples\n\nactual = [100, 110, 105, 115, 120]  # Changes: +10, -5, +10, +5\npredicted = [100, 108, 106, 112, 118]  # Changes: +8, -2, +6, +6\ndirectional_accuracy(actual, predicted)  # All directions match = 1.0\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.coverage_probability","page":"API Reference","title":"UnifiedMetrics.coverage_probability","text":"coverage_probability(actual, lower, upper)\n\nCompute the coverage probability of prediction intervals.\n\nMeasures what fraction of actual values fall within the prediction intervals.\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth time series\nlower::AbstractVector{<:Real}: Lower bounds of prediction intervals\nupper::AbstractVector{<:Real}: Upper bounds of prediction intervals\n\nExamples\n\nactual = [100, 110, 105, 115, 120]\nlower = [95, 105, 100, 108, 112]  # 95% lower bounds\nupper = [105, 115, 112, 122, 128]  # 95% upper bounds\ncoverage_probability(actual, lower, upper)  # Should be ≈ 0.95 if well-calibrated\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.pinball_loss_series","page":"API Reference","title":"UnifiedMetrics.pinball_loss_series","text":"pinball_loss_series(actual, predicted; quantile=0.5)\n\nCompute the pinball (quantile) loss for time series probabilistic forecasts.\n\nSame as quantile_loss but named to be consistent with time series literature.\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth time series\npredicted::AbstractVector{<:Real}: Quantile forecast\nquantile::Real: Target quantile in (0, 1) (default: 0.5 = median)\n\nExamples\n\nactual = [100, 110, 105, 115, 120]\npredicted_median = [98, 108, 110, 112, 118]  # Median forecasts\npinball_loss_series(actual, predicted_median, quantile=0.5)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.winkler_score","page":"API Reference","title":"UnifiedMetrics.winkler_score","text":"winkler_score(actual, lower, upper; alpha=0.05)\n\nCompute the Winkler score for prediction interval evaluation.\n\nThe Winkler score rewards narrow intervals and penalizes intervals that don't contain the actual value.\n\nLower scores are better.\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth time series\nlower::AbstractVector{<:Real}: Lower bounds of prediction intervals\nupper::AbstractVector{<:Real}: Upper bounds of prediction intervals\nalpha::Real: Significance level (default: 0.05 for 95% intervals)\n\nExamples\n\nactual = [100, 110, 105, 115, 120]\nlower = [95, 105, 100, 108, 112]\nupper = [105, 115, 112, 122, 128]\nwinkler_score(actual, lower, upper, alpha=0.05)\n\n\n\n\n\n","category":"function"},{"location":"api/#UnifiedMetrics.autocorrelation_error","page":"API Reference","title":"UnifiedMetrics.autocorrelation_error","text":"autocorrelation_error(actual, predicted; max_lag=10)\n\nCompute the error in autocorrelation structure.\n\nMeasures how well the forecast preserves the autocorrelation structure of the actual series. Lower values indicate better preservation of temporal patterns.\n\nArguments\n\nactual::AbstractVector{<:Real}: Ground truth time series\npredicted::AbstractVector{<:Real}: Predicted time series\nmax_lag::Integer: Maximum lag to consider (default: 10)\n\nExamples\n\nactual = [1.1, 1.9, 3.0, 4.4, 5.0, 5.6, 6.2, 7.1, 8.0, 9.2]\npredicted = [0.9, 1.8, 2.5, 4.5, 5.0, 6.2, 6.0, 7.0, 8.1, 9.0]\nautocorrelation_error(actual, predicted)\n\n\n\n\n\n","category":"function"},{"location":"time_series/#Time-Series-Forecasting-Metrics","page":"Time Series Forecasting","title":"Time Series Forecasting Metrics","text":"A comprehensive guide to evaluating time series forecasting models.","category":"section"},{"location":"time_series/#Why-Time-Series-Metrics-Are-Different","page":"Time Series Forecasting","title":"Why Time Series Metrics Are Different","text":"Time series evaluation has unique challenges that standard regression metrics don't address:\n\nScale Dependence: MAE of 10 means nothing without context - is that good or bad?\nBenchmark Comparison: How does your model compare to simple baselines (naive, seasonal naive)?\nTemporal Structure: Errors may be autocorrelated, biased, or directionally wrong\nProbabilistic Forecasts: Modern forecasting produces prediction intervals, not just point forecasts\nMultiple Horizons: Accuracy often degrades as forecast horizon increases\n\nUnifiedMetrics.jl provides 13 specialized metrics to address these challenges.","category":"section"},{"location":"time_series/#Metrics-at-a-Glance","page":"Time Series Forecasting","title":"Metrics at a Glance","text":"Metric Category Range Key Insight\nmase Scaled Error [0, ∞) Is model better than naive?\nmsse Scaled Error [0, ∞) Squared version of MASE\nrmsse Scaled Error [0, ∞) Same scale as data\ntracking_signal Bias (-∞, ∞) Is forecast systematically off?\nforecast_bias Bias (-∞, ∞) Average over/under prediction\ntheil_u1 Benchmark [0, 1] Normalized inequality\ntheil_u2 Benchmark [0, ∞) Comparison to naive\nwape Percentage [0, ∞) Weighted percentage error\ndirectional_accuracy Direction [0, 1] Up/down prediction accuracy\ncoverage_probability Intervals [0, 1] Interval calibration\nwinkler_score Intervals [0, ∞) Interval sharpness + calibration\npinball_loss_series Quantile [0, ∞) Quantile forecast accuracy\nautocorrelation_error Structure [0, ∞) Temporal pattern preservation\n\n","category":"section"},{"location":"time_series/#Choosing-the-Right-Time-Series-Metric","page":"Time Series Forecasting","title":"Choosing the Right Time Series Metric","text":"","category":"section"},{"location":"time_series/#Decision-Flowchart","page":"Time Series Forecasting","title":"Decision Flowchart","text":"What do you need to evaluate?\n│\n├─► Point Forecast Accuracy\n│   │\n│   ├─► Need to compare across different series/scales?\n│   │   │\n│   │   ├─► YES ──► mase() or rmsse()\n│   │   │\n│   │   └─► NO ──► mae() or rmse() from regression metrics\n│   │\n│   └─► Need percentage-based reporting?\n│       │\n│       ├─► Data has zeros? ──► wape()\n│       │\n│       └─► No zeros ──► mape() from regression metrics\n│\n├─► Forecast Bias Detection\n│   │\n│   ├─► Real-time monitoring ──► tracking_signal()\n│   │\n│   └─► One-time evaluation ──► forecast_bias()\n│\n├─► Benchmark Comparison\n│   │\n│   └─► Is model better than naive forecast? ──► theil_u2() or mase()\n│\n├─► Direction Prediction (Trading)\n│   │\n│   └─► directional_accuracy()\n│\n└─► Probabilistic Forecasts\n    │\n    ├─► Prediction intervals ──► coverage_probability() + winkler_score()\n    │\n    └─► Quantile forecasts ──► pinball_loss_series()","category":"section"},{"location":"time_series/#Metric-Selection-by-Use-Case","page":"Time Series Forecasting","title":"Metric Selection by Use Case","text":"Use Case Primary Metric Secondary Metrics\nM-competition style evaluation mase rmsse, mape\nSupply chain forecasting wape mase, forecast_bias\nDemand forecasting mase tracking_signal, coverage_probability\nFinancial/trading directional_accuracy theil_u2\nWeather forecasting rmsse coverage_probability, winkler_score\nReal-time monitoring tracking_signal forecast_bias\nModel selection mase theil_u2, winkler_score\n\n","category":"section"},{"location":"time_series/#Scaled-Error-Metrics","page":"Time Series Forecasting","title":"Scaled Error Metrics","text":"The most important innovation in time series evaluation. These metrics compare your forecast error to the error of a naive benchmark, making them scale-independent and interpretable.","category":"section"},{"location":"time_series/#MASE-(Mean-Absolute-Scaled-Error)","page":"Time Series Forecasting","title":"MASE (Mean Absolute Scaled Error)","text":"mase(actual, predicted; m=1)\n\nCompute the Mean Absolute Scaled Error. See API Reference for full documentation.","category":"section"},{"location":"time_series/#Why-MASE-is-the-Gold-Standard","page":"Time Series Forecasting","title":"Why MASE is the Gold Standard","text":"Scale-independent: Compare forecasts across products, regions, or time periods with different scales\nInterpretable threshold: MASE < 1 means better than naive, MASE > 1 means worse\nHandles zeros: Unlike MAPE, works with intermittent demand\nSymmetric: Treats over- and under-forecasting equally\nRecommended: Official metric of M3 and M4 forecasting competitions","category":"section"},{"location":"time_series/#Understanding-the-Seasonal-Period-m","page":"Time Series Forecasting","title":"Understanding the Seasonal Period m","text":"The m parameter defines what \"naive forecast\" means:\n\nYour Data Seasonality m Value Naive Forecast\nDaily sales Weekly pattern 7 Same day last week\nDaily sales No clear pattern 1 Yesterday's value\nWeekly data Yearly pattern 52 Same week last year\nMonthly data Yearly pattern 12 Same month last year\nQuarterly data Yearly pattern 4 Same quarter last year\nHourly data Daily pattern 24 Same hour yesterday\n\nExample:\n\n# Monthly retail sales with yearly seasonality\nactual = [100, 95, 110, 120, 140, 160, 155, 150, 130, 115, 105, 180,  # Year 1\n          105, 98, 115, 125, 145, 165, 160, 155, 135, 120, 110, 190]  # Year 2\n\npredicted = [102, 97, 112, 118, 142, 158, 157, 152, 132, 117, 107, 178,\n             107, 100, 117, 123, 147, 163, 162, 157, 137, 122, 112, 188]\n\n# Compare to seasonal naive (same month last year)\nmase(actual, predicted, m=12)  # Yearly seasonality\n\n# Compare to simple naive (previous month)\nmase(actual, predicted, m=1)   # Usually higher - seasonal naive is a tougher benchmark","category":"section"},{"location":"time_series/#MASE-Interpretation-Guide","page":"Time Series Forecasting","title":"MASE Interpretation Guide","text":"MASE Value Interpretation Action\n< 0.5 Excellent Model is production-ready\n0.5 - 0.8 Good Model adds significant value\n0.8 - 1.0 Acceptable Model slightly beats naive\n1.0 Break-even Model equals naive benchmark\n1.0 - 1.5 Poor Model worse than naive\n> 1.5 Very Poor Investigate model issues","category":"section"},{"location":"time_series/#MSSE-and-RMSSE","page":"Time Series Forecasting","title":"MSSE and RMSSE","text":"msse(actual, predicted; m=1)\nrmsse(actual, predicted; m=1)\n\nSquared scaled error metrics. See API Reference for full documentation.","category":"section"},{"location":"time_series/#When-to-Use-RMSSE-vs-MASE","page":"Time Series Forecasting","title":"When to Use RMSSE vs MASE","text":"RMSSE: Penalizes large errors more heavily (like RMSE vs MAE)\nMASE: More robust to outliers\nM5 competition used RMSSE as the primary metric\n\nactual = [100, 110, 105, 200, 120]  # Note: 200 is an outlier\npredicted = [102, 108, 107, 150, 118]\n\nmase(actual, predicted)   # Less affected by the large error at position 4\nrmsse(actual, predicted)  # More affected by the large error\n\n","category":"section"},{"location":"time_series/#Bias-Detection-Metrics","page":"Time Series Forecasting","title":"Bias Detection Metrics","text":"Systematic bias is a common problem in forecasting. A model might have good overall accuracy but consistently over- or under-predict.","category":"section"},{"location":"time_series/#Tracking-Signal","page":"Time Series Forecasting","title":"Tracking Signal","text":"tracking_signal(actual, predicted)\n\nMonitor forecast bias over time. See API Reference for full documentation.","category":"section"},{"location":"time_series/#Real-Time-Bias-Monitoring","page":"Time Series Forecasting","title":"Real-Time Bias Monitoring","text":"The tracking signal is designed for continuous monitoring of forecast performance:\n\n# Monitor forecast bias over time\nfunction monitor_forecast(actual_stream, predicted_stream)\n    for t in eachindex(actual_stream)\n        actual_so_far = actual_stream[1:t]\n        predicted_so_far = predicted_stream[1:t]\n\n        ts = tracking_signal(actual_so_far, predicted_so_far)\n\n        if abs(ts) > 4\n            println(\"⚠️  Period $t: Tracking signal = $(round(ts, digits=2))\")\n            if ts > 0\n                println(\"   Model is under-forecasting. Consider adjusting upward.\")\n            else\n                println(\"   Model is over-forecasting. Consider adjusting downward.\")\n            end\n        end\n    end\nend","category":"section"},{"location":"time_series/#Control-Chart-Interpretation","page":"Time Series Forecasting","title":"Control Chart Interpretation","text":"Tracking Signal Status Action\n-4 to +4 In control Continue monitoring\n±4 to ±6 Warning Investigate recent forecasts\nBeyond ±6 Out of control Recalibrate model immediately","category":"section"},{"location":"time_series/#Forecast-Bias","page":"Time Series Forecasting","title":"Forecast Bias","text":"forecast_bias(actual, predicted)\n\nCompute the average forecast error. See API Reference for full documentation.","category":"section"},{"location":"time_series/#Bias-vs-Tracking-Signal","page":"Time Series Forecasting","title":"Bias vs Tracking Signal","text":"Metric Use Case Output\nforecast_bias One-time evaluation Average error (in original units)\ntracking_signal Continuous monitoring Normalized ratio (unitless)\n\nactual = [100, 110, 105, 115, 120]\npredicted = [95, 105, 100, 110, 115]  # Consistently under-predicting by ~5\n\nforecast_bias(actual, predicted)    # Returns 5.0 (average under-prediction)\ntracking_signal(actual, predicted)  # Returns ~5.0 (normalized, indicates bias)\n\n","category":"section"},{"location":"time_series/#Benchmark-Comparison-Metrics","page":"Time Series Forecasting","title":"Benchmark Comparison Metrics","text":"","category":"section"},{"location":"time_series/#Theil's-U-Statistics","page":"Time Series Forecasting","title":"Theil's U Statistics","text":"theil_u1(actual, predicted)\ntheil_u2(actual, predicted; m=1)\n\nBenchmark comparison metrics. See API Reference for full documentation.","category":"section"},{"location":"time_series/#Understanding-Theil's-U1-vs-U2","page":"Time Series Forecasting","title":"Understanding Theil's U1 vs U2","text":"Statistic Range Interpretation\nU1 [0, 1] 0 = perfect, 1 = worst possible\nU2 [0, ∞) < 1 = better than naive, > 1 = worse than naive\n\nU2 is more commonly used because it directly answers: \"Is my model better than just using the last value?\"\n\nactual = [100, 110, 105, 115, 120, 125]\npredicted = [98, 108, 107, 113, 118, 123]\n\n# Is this forecast better than naive?\nu2 = theil_u2(actual, predicted)\nprintln(\"Theil U2: $u2\")\nprintln(u2 < 1 ? \"Model beats naive forecast\" : \"Naive forecast is better\")\n\n","category":"section"},{"location":"time_series/#Percentage-Based-Metrics","page":"Time Series Forecasting","title":"Percentage-Based Metrics","text":"","category":"section"},{"location":"time_series/#WAPE-(Weighted-Absolute-Percentage-Error)","page":"Time Series Forecasting","title":"WAPE (Weighted Absolute Percentage Error)","text":"wape(actual, predicted)\n\nWeighted percentage error metric. See API Reference for full documentation.","category":"section"},{"location":"time_series/#WAPE-vs-MAPE","page":"Time Series Forecasting","title":"WAPE vs MAPE","text":"Metric Formula Handles Zeros? Weighting\nMAPE mean(|error| / |actual|) No (undefined) Equal weight\nWAPE sum(|error|) / sum(|actual|) Yes Weighted by actual\n\nWAPE is preferred for:\n\nIntermittent demand (many zeros)\nAggregated reporting (total error as % of total actual)\nSupply chain metrics\n\n# Intermittent demand with zeros\nactual = [0, 10, 0, 0, 20, 5, 0, 15]\npredicted = [1, 8, 2, 0, 18, 6, 1, 14]\n\n# MAPE would be undefined due to zeros\n# mape(actual, predicted)  # Don't use!\n\n# WAPE works fine\nwape(actual, predicted)  # Returns meaningful percentage\n\n","category":"section"},{"location":"time_series/#Directional-Accuracy","page":"Time Series Forecasting","title":"Directional Accuracy","text":"directional_accuracy(actual, predicted)\n\nMeasures how often the model predicts the correct direction of change. See API Reference for full documentation.","category":"section"},{"location":"time_series/#When-Direction-Matters-More-Than-Magnitude","page":"Time Series Forecasting","title":"When Direction Matters More Than Magnitude","text":"In many applications, predicting the direction of change is more valuable than predicting the exact value:\n\nTrading: Buy/sell signals depend on up/down prediction\nInventory: Increase/decrease stock based on demand direction\nCapacity planning: Scale up/down based on trend direction\n\n# Stock price forecasting\nactual_prices = [100.0, 102.0, 101.0, 103.0, 102.5, 104.0, 103.0, 105.0]\npredicted_prices = [99.0, 101.5, 102.0, 102.5, 103.0, 103.5, 104.0, 104.5]\n\n# MAE might look good...\nmae(actual_prices, predicted_prices)  # ~1.0\n\n# But what about direction?\nda = directional_accuracy(actual_prices, predicted_prices)\nprintln(\"Directional Accuracy: $(round(da * 100, digits=1))%\")\nprintln(da > 0.5 ? \"Model has predictive value for direction\" : \"Model fails to predict direction\")","category":"section"},{"location":"time_series/#Directional-Accuracy-Benchmarks","page":"Time Series Forecasting","title":"Directional Accuracy Benchmarks","text":"DA Value Interpretation\n> 60% Good directional forecasting\n50-60% Marginal predictive value\n~50% No better than coin flip\n< 50% Worse than random (consider inverting)\n\n","category":"section"},{"location":"time_series/#Prediction-Interval-Metrics","page":"Time Series Forecasting","title":"Prediction Interval Metrics","text":"Modern forecasting produces probabilistic forecasts with prediction intervals, not just point predictions. These metrics evaluate interval quality.","category":"section"},{"location":"time_series/#Coverage-Probability","page":"Time Series Forecasting","title":"Coverage Probability","text":"coverage_probability(actual, lower, upper)\n\nCompute the proportion of actual values within prediction intervals. See API Reference for full documentation.","category":"section"},{"location":"time_series/#Calibration-Assessment","page":"Time Series Forecasting","title":"Calibration Assessment","text":"A well-calibrated 95% prediction interval should contain the actual value ~95% of the time:\n\nactual = [100, 110, 105, 115, 120, 125, 130, 128, 135, 140]\n\n# Your model's 95% prediction intervals\nlower_95 = [92, 102, 97, 107, 112, 117, 122, 120, 127, 132]\nupper_95 = [108, 118, 113, 123, 128, 133, 138, 136, 143, 148]\n\ncoverage = coverage_probability(actual, lower_95, upper_95)\nprintln(\"95% Interval Coverage: $(round(coverage * 100, digits=1))%\")\n\nif coverage < 0.90\n    println(\"⚠️  Under-coverage: Intervals too narrow\")\nelseif coverage > 0.99\n    println(\"⚠️  Over-coverage: Intervals too wide (but valid)\")\nelse\n    println(\"✓ Well-calibrated\")\nend","category":"section"},{"location":"time_series/#Winkler-Score","page":"Time Series Forecasting","title":"Winkler Score","text":"winkler_score(actual, lower, upper; alpha=0.05)\n\nEvaluate prediction intervals for sharpness and calibration. See API Reference for full documentation.","category":"section"},{"location":"time_series/#Why-Winkler-Score?","page":"Time Series Forecasting","title":"Why Winkler Score?","text":"Coverage alone doesn't tell the whole story. Two models can have the same coverage but different interval widths:\n\nModel A: 95% coverage with wide intervals (less useful)\nModel B: 95% coverage with narrow intervals (more useful)\n\nWinkler score rewards sharp (narrow) intervals while penalizing miscoverage:\n\nactual = [100, 110, 105]\n\n# Model A: Wide intervals (always covers, but not useful)\nlower_a = [80, 90, 85]\nupper_a = [120, 130, 125]\n\n# Model B: Narrow intervals (same coverage, more useful)\nlower_b = [95, 105, 100]\nupper_b = [105, 115, 110]\n\n# Both have 100% coverage\ncoverage_probability(actual, lower_a, upper_a)  # 1.0\ncoverage_probability(actual, lower_b, upper_b)  # 1.0\n\n# But Winkler score prefers narrower intervals\nwinkler_score(actual, lower_a, upper_a, alpha=0.05)  # Higher (worse)\nwinkler_score(actual, lower_b, upper_b, alpha=0.05)  # Lower (better)","category":"section"},{"location":"time_series/#Pinball-Loss-(Quantile-Loss)","page":"Time Series Forecasting","title":"Pinball Loss (Quantile Loss)","text":"pinball_loss_series(actual, predicted; quantile=0.5)\n\nEvaluate quantile forecasts. See API Reference for full documentation.","category":"section"},{"location":"time_series/#Evaluating-Quantile-Forecasts","page":"Time Series Forecasting","title":"Evaluating Quantile Forecasts","text":"For probabilistic forecasts that output multiple quantiles:\n\nactual = [100, 110, 105, 115, 120]\n\n# Forecasts at different quantiles\nforecast_p10 = [85, 95, 90, 100, 105]    # 10th percentile\nforecast_p50 = [98, 108, 103, 113, 118]  # Median\nforecast_p90 = [112, 122, 117, 127, 132] # 90th percentile\n\n# Evaluate each quantile\nfor (q, forecast) in [(0.1, forecast_p10), (0.5, forecast_p50), (0.9, forecast_p90)]\n    loss = pinball_loss_series(actual, forecast, quantile=q)\n    println(\"P$(Int(q*100)) Pinball Loss: $(round(loss, digits=3))\")\nend\n\n","category":"section"},{"location":"time_series/#Autocorrelation-Preservation","page":"Time Series Forecasting","title":"Autocorrelation Preservation","text":"autocorrelation_error(actual, predicted; max_lag=10)\n\nMeasure how well the forecast preserves the temporal structure. See API Reference for full documentation.","category":"section"},{"location":"time_series/#When-Temporal-Structure-Matters","page":"Time Series Forecasting","title":"When Temporal Structure Matters","text":"Some applications require forecasts that preserve the statistical properties of the original series:\n\nSimulation and scenario generation\nSynthetic data for testing\nRisk modeling (preserving volatility clustering)\n\n# Original series has strong autocorrelation\nactual = cumsum(randn(100))  # Random walk\n\n# Good forecast preserves autocorrelation structure\ngood_forecast = actual .+ randn(100) * 0.5  # Small noise\n\n# Bad forecast destroys autocorrelation\nbad_forecast = shuffle(actual)  # Shuffled - no temporal structure\n\nautocorrelation_error(actual, good_forecast, max_lag=10)  # Low\nautocorrelation_error(actual, bad_forecast, max_lag=10)   # High\n\n","category":"section"},{"location":"time_series/#Complete-Evaluation-Framework","page":"Time Series Forecasting","title":"Complete Evaluation Framework","text":"","category":"section"},{"location":"time_series/#Recommended-Evaluation-Protocol","page":"Time Series Forecasting","title":"Recommended Evaluation Protocol","text":"For comprehensive time series model evaluation, use this framework:\n\nusing UnifiedMetrics\n\nfunction evaluate_forecast(actual, predicted, lower, upper; m=1, alpha=0.05)\n    println(\"=\" ^ 60)\n    println(\"TIME SERIES FORECAST EVALUATION REPORT\")\n    println(\"=\" ^ 60)\n\n    # 1. Point Forecast Accuracy\n    println(\"\\n📊 POINT FORECAST ACCURACY\")\n    println(\"-\" ^ 40)\n    println(\"MAE:  $(round(mae(actual, predicted), digits=3))\")\n    println(\"RMSE: $(round(rmse(actual, predicted), digits=3))\")\n    println(\"MAPE: $(round(mape(actual, predicted) * 100, digits=2))%\")\n    println(\"WAPE: $(round(wape(actual, predicted) * 100, digits=2))%\")\n\n    # 2. Scale-Independent Metrics\n    println(\"\\n📏 SCALE-INDEPENDENT METRICS\")\n    println(\"-\" ^ 40)\n    m_val = mase(actual, predicted, m=m)\n    println(\"MASE (m=$m):  $(round(m_val, digits=3))\")\n    println(\"RMSSE (m=$m): $(round(rmsse(actual, predicted, m=m), digits=3))\")\n    println(\"Theil U2:     $(round(theil_u2(actual, predicted, m=m), digits=3))\")\n\n    if m_val < 1\n        println(\"✓ Model outperforms naive forecast\")\n    else\n        println(\"⚠ Model underperforms naive forecast\")\n    end\n\n    # 3. Bias Analysis\n    println(\"\\n🎯 BIAS ANALYSIS\")\n    println(\"-\" ^ 40)\n    fb = forecast_bias(actual, predicted)\n    ts = tracking_signal(actual, predicted)\n    println(\"Forecast Bias:    $(round(fb, digits=3))\")\n    println(\"Tracking Signal:  $(round(ts, digits=3))\")\n\n    if abs(ts) > 4\n        println(\"⚠ Systematic bias detected!\")\n    else\n        println(\"✓ No significant bias\")\n    end\n\n    # 4. Directional Accuracy\n    println(\"\\n↗️ DIRECTIONAL ACCURACY\")\n    println(\"-\" ^ 40)\n    da = directional_accuracy(actual, predicted)\n    println(\"Direction Accuracy: $(round(da * 100, digits=1))%\")\n\n    # 5. Prediction Intervals (if provided)\n    if !isnothing(lower) && !isnothing(upper)\n        println(\"\\n📈 PREDICTION INTERVAL QUALITY\")\n        println(\"-\" ^ 40)\n        cov = coverage_probability(actual, lower, upper)\n        wink = winkler_score(actual, lower, upper, alpha=alpha)\n        expected_cov = 1 - alpha\n\n        println(\"Expected Coverage: $(round(expected_cov * 100, digits=1))%\")\n        println(\"Actual Coverage:   $(round(cov * 100, digits=1))%\")\n        println(\"Winkler Score:     $(round(wink, digits=3))\")\n\n        if abs(cov - expected_cov) < 0.05\n            println(\"✓ Intervals well-calibrated\")\n        elseif cov < expected_cov\n            println(\"⚠ Under-coverage: intervals too narrow\")\n        else\n            println(\"⚠ Over-coverage: intervals too wide\")\n        end\n    end\n\n    println(\"\\n\" * \"=\" ^ 60)\nend\n\n# Example usage\nactual = [100.0, 110.0, 105.0, 115.0, 120.0, 125.0, 130.0, 128.0]\npredicted = [98.0, 108.0, 107.0, 113.0, 118.0, 123.0, 128.0, 126.0]\nlower = [90.0, 100.0, 99.0, 105.0, 110.0, 115.0, 120.0, 118.0]\nupper = [106.0, 116.0, 115.0, 121.0, 126.0, 131.0, 136.0, 134.0]\n\nevaluate_forecast(actual, predicted, lower, upper, m=1, alpha=0.05)","category":"section"},{"location":"time_series/#Multi-Series-Comparison","page":"Time Series Forecasting","title":"Multi-Series Comparison","text":"When comparing forecasts across multiple time series:\n\nfunction compare_models_across_series(series_data, models)\n    results = Dict{String, Vector{Float64}}()\n\n    for model_name in keys(models)\n        results[model_name] = Float64[]\n    end\n\n    for (actual, model_forecasts) in series_data\n        for (model_name, predicted) in model_forecasts\n            push!(results[model_name], mase(actual, predicted))\n        end\n    end\n\n    println(\"Model Comparison (MASE)\")\n    println(\"-\" ^ 40)\n    for (model_name, mase_values) in results\n        avg_mase = mean(mase_values)\n        println(\"$model_name: $(round(avg_mase, digits=3)) (avg across $(length(mase_values)) series)\")\n    end\nend\n\n","category":"section"},{"location":"time_series/#Common-Pitfalls-and-Solutions","page":"Time Series Forecasting","title":"Common Pitfalls and Solutions","text":"","category":"section"},{"location":"time_series/#Pitfall-1:-Using-MAPE-with-Zeros","page":"Time Series Forecasting","title":"Pitfall 1: Using MAPE with Zeros","text":"Problem: MAPE is undefined when actual values are zero (common in intermittent demand).\n\nSolution: Use WAPE or MASE instead.\n\nactual = [0, 10, 0, 5, 0, 20]  # Intermittent demand\npredicted = [1, 9, 1, 4, 1, 19]\n\n# Don't do this:\n# mape(actual, predicted)  # Returns Inf or NaN\n\n# Do this instead:\nwape(actual, predicted)\nmase(actual, predicted)","category":"section"},{"location":"time_series/#Pitfall-2:-Ignoring-Seasonality-in-MASE","page":"Time Series Forecasting","title":"Pitfall 2: Ignoring Seasonality in MASE","text":"Problem: Using m=1 when data has seasonality makes the benchmark too easy to beat.\n\nSolution: Set m to match your data's seasonal period.\n\n# Monthly data with yearly seasonality\nactual = repeat([100, 80, 90, 110, 130, 150, 160, 155, 140, 120, 100, 180], 2)\npredicted = actual .+ randn(24) * 5\n\n# This makes naive look bad (comparing to previous month)\nmase(actual, predicted, m=1)  # Artificially low\n\n# This is the correct comparison (same month last year)\nmase(actual, predicted, m=12)  # More realistic assessment","category":"section"},{"location":"time_series/#Pitfall-3:-Only-Evaluating-Point-Forecasts","page":"Time Series Forecasting","title":"Pitfall 3: Only Evaluating Point Forecasts","text":"Problem: Ignoring prediction intervals misses important information about forecast uncertainty.\n\nSolution: Always evaluate both point accuracy and interval quality.\n\n# A model with great point accuracy but terrible intervals\nactual = [100, 110, 105, 115, 120]\npredicted = [100, 110, 105, 115, 120]  # Perfect point forecast!\nlower = [99, 109, 104, 114, 119]       # Intervals way too narrow\nupper = [101, 111, 106, 116, 121]\n\nmae(actual, predicted)  # 0.0 - Perfect!\ncoverage_probability(actual, lower, upper)  # May be < 0.95 - Problem!","category":"section"},{"location":"time_series/#Pitfall-4:-Not-Monitoring-for-Bias","page":"Time Series Forecasting","title":"Pitfall 4: Not Monitoring for Bias","text":"Problem: A model may have good overall accuracy but develop systematic bias over time.\n\nSolution: Use tracking signal for ongoing monitoring.\n\n# Model starts good but develops bias\nactual = [100, 102, 104, 106, 108, 110, 112, 114, 116, 118]\npredicted = [100, 101, 102, 103, 104, 105, 106, 107, 108, 109]  # Increasing under-forecast\n\n# Overall MAE looks okay\nmae(actual, predicted)  # ~4.5\n\n# But tracking signal reveals the problem\ntracking_signal(actual, predicted)  # High positive value - systematic under-forecasting\n\n","category":"section"},{"location":"time_series/#References-and-Further-Reading","page":"Time Series Forecasting","title":"References and Further Reading","text":"","category":"section"},{"location":"time_series/#Academic-References","page":"Time Series Forecasting","title":"Academic References","text":"Hyndman, R.J., & Koehler, A.B. (2006). \"Another look at measures of forecast accuracy.\" International Journal of Forecasting, 22(4), 679-688. (Introduced MASE)\nMakridakis, S., Spiliotis, E., & Assimakopoulos, V. (2020). \"The M4 Competition: 100,000 time series and 61 forecasting methods.\" International Journal of Forecasting, 36(1), 54-74.\nGneiting, T., & Raftery, A.E. (2007). \"Strictly proper scoring rules, prediction, and estimation.\" Journal of the American Statistical Association, 102(477), 359-378. (Theory behind proper scoring rules)","category":"section"},{"location":"time_series/#Metric-Selection-Guidelines","page":"Time Series Forecasting","title":"Metric Selection Guidelines","text":"M-competitions: Use MASE, sMAPE (symmetric MAPE), and RMSSE\nSupply chain: Use WAPE, MASE, and tracking signal\nFinance: Use directional accuracy, Theil's U2\nProbabilistic forecasting: Use coverage probability, Winkler score, CRPS","category":"section"},{"location":"getting_started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"This guide will help you get started with UnifiedMetrics.jl quickly.","category":"section"},{"location":"getting_started/#Installation","page":"Getting Started","title":"Installation","text":"Install UnifiedMetrics.jl using Julia's package manager:\n\nusing Pkg\nPkg.add(\"UnifiedMetrics\")\n\nOr in the REPL package mode (press ]):\n\npkg> add UnifiedMetrics","category":"section"},{"location":"getting_started/#Basic-Usage","page":"Getting Started","title":"Basic Usage","text":"All metrics in UnifiedMetrics.jl follow a consistent API pattern:\n\nmetric(actual, predicted)\n\nWhere:\n\nactual is the ground truth (what actually happened)\npredicted is your model's prediction","category":"section"},{"location":"getting_started/#Regression-Example","page":"Getting Started","title":"Regression Example","text":"using UnifiedMetrics\n\n# Your actual values and predictions\nactual = [1.0, 2.0, 3.0, 4.0, 5.0]\npredicted = [1.1, 2.0, 2.8, 4.2, 4.9]\n\n# Common metrics\nmae(actual, predicted)      # Mean Absolute Error: 0.12\nrmse(actual, predicted)     # Root Mean Squared Error: 0.14\nmape(actual, predicted)     # Mean Absolute Percentage Error\nexplained_variation(actual, predicted)  # R²: 0.99","category":"section"},{"location":"getting_started/#Classification-Example","page":"Getting Started","title":"Classification Example","text":"using UnifiedMetrics\n\n# Multi-class classification\nactual = [\"cat\", \"dog\", \"cat\", \"bird\", \"dog\"]\npredicted = [\"cat\", \"cat\", \"cat\", \"bird\", \"dog\"]\n\naccuracy(actual, predicted)  # 0.8\nce(actual, predicted)        # Classification Error: 0.2\nbalanced_accuracy(actual, predicted)\ncohens_kappa(actual, predicted)","category":"section"},{"location":"getting_started/#Binary-Classification-Example","page":"Getting Started","title":"Binary Classification Example","text":"using UnifiedMetrics\n\n# Binary labels\nactual = [1, 1, 1, 0, 0, 0]\npredicted_labels = [1, 0, 1, 1, 0, 0]\npredicted_probs = [0.9, 0.4, 0.8, 0.6, 0.3, 0.2]\n\n# Threshold-based metrics (use labels)\nprecision(actual, predicted_labels)\nrecall(actual, predicted_labels)\nfbeta_score(actual, predicted_labels)\nmcc(actual, predicted_labels)\n\n# Probability-based metrics (use probabilities)\nauc(actual, predicted_probs)\nbrier_score(actual, predicted_probs)\nlogloss(actual, predicted_probs)","category":"section"},{"location":"getting_started/#Information-Retrieval-Example","page":"Getting Started","title":"Information Retrieval Example","text":"using UnifiedMetrics\n\n# Ranking evaluation with relevance scores\nrelevance = [3, 2, 3, 0, 1, 2]  # Relevance of items in your ranking\nndcg(relevance)        # Normalized DCG\nndcg(relevance, k=3)   # NDCG at position 3\n\n# Set-based retrieval\nactual_relevant = [\"doc1\", \"doc3\", \"doc5\"]\nretrieved = [\"doc1\", \"doc2\", \"doc3\", \"doc4\"]\n\nprecision_at_k(actual_relevant, retrieved, k=3)\nrecall_at_k(actual_relevant, retrieved, k=3)","category":"section"},{"location":"getting_started/#Time-Series-Example","page":"Getting Started","title":"Time Series Example","text":"using UnifiedMetrics\n\n# Time series forecasting\nactual = [100.0, 110.0, 105.0, 115.0, 120.0, 125.0]\npredicted = [98.0, 108.0, 110.0, 112.0, 118.0, 127.0]\n\n# Scale-independent metrics\nmase(actual, predicted)          # Non-seasonal\nmase(actual, predicted, m=4)     # Quarterly seasonality\n\n# Bias detection\ntracking_signal(actual, predicted)\nforecast_bias(actual, predicted)\n\n# Prediction intervals\nlower = [95.0, 105.0, 100.0, 108.0, 112.0, 120.0]\nupper = [105.0, 115.0, 115.0, 122.0, 128.0, 135.0]\ncoverage_probability(actual, lower, upper)","category":"section"},{"location":"getting_started/#Understanding-Metric-Outputs","page":"Getting Started","title":"Understanding Metric Outputs","text":"","category":"section"},{"location":"getting_started/#Metrics-Where-Lower-is-Better","page":"Getting Started","title":"Metrics Where Lower is Better","text":"Most error metrics: MAE, RMSE, MSE, MAPE, Brier score, log loss, Hamming loss, etc.","category":"section"},{"location":"getting_started/#Metrics-Where-Higher-is-Better","page":"Getting Started","title":"Metrics Where Higher is Better","text":"Accuracy, balanced accuracy\nR² (explained variation)\nAUC, Gini coefficient\nPrecision, recall, F-score\nNDCG, MRR, hit rate","category":"section"},{"location":"getting_started/#Metrics-with-Specific-Interpretations","page":"Getting Started","title":"Metrics with Specific Interpretations","text":"Metric Range Perfect Score Random/Baseline\nR² (-∞, 1] 1 0\nAUC [0, 1] 1 0.5\nMCC [-1, 1] 1 0\nCohen's Kappa [-1, 1] 1 0\nMASE [0, ∞) 0 1","category":"section"},{"location":"getting_started/#Tips-for-Effective-Evaluation","page":"Getting Started","title":"Tips for Effective Evaluation","text":"Use multiple metrics: No single metric tells the whole story\nMatch metric to objective: Choose metrics that reflect your business goals\nConsider data characteristics: Imbalanced data needs appropriate metrics\nReport confidence intervals: Single numbers can be misleading","category":"section"},{"location":"getting_started/#Next-Steps","page":"Getting Started","title":"Next Steps","text":"Read Choosing the Right Metric for guidance on metric selection\nExplore domain-specific pages for detailed metric documentation\nCheck the API Reference for complete function signatures","category":"section"},{"location":"binary_classification/#Binary-Classification-Metrics","page":"Binary Classification","title":"Binary Classification Metrics","text":"Metrics for evaluating models that predict between two classes (positive/negative, yes/no, 1/0).","category":"section"},{"location":"binary_classification/#Overview","page":"Binary Classification","title":"Overview","text":"Binary classification metrics fall into two categories:\n\nThreshold-dependent: Require binary predictions (0/1)\nPrecision, Recall, F-score, Specificity\nThreshold-independent: Use probability scores\nAUC, Brier score, Log loss","category":"section"},{"location":"binary_classification/#Quick-Reference","page":"Binary Classification","title":"Quick Reference","text":"Metric Input Type Range Best For\nauc Probabilities [0, 1] Model comparison\nprecision Labels [0, 1] Minimizing FP\nrecall Labels [0, 1] Minimizing FN\nfbeta_score Labels [0, 1] Balanced evaluation\nmcc Labels [-1, 1] Imbalanced data\nbrier_score Probabilities [0, 1] Calibration","category":"section"},{"location":"binary_classification/#ROC-Based-Metrics","page":"Binary Classification","title":"ROC-Based Metrics","text":"","category":"section"},{"location":"binary_classification/#Area-Under-ROC-Curve","page":"Binary Classification","title":"Area Under ROC Curve","text":"auc(actual, predicted_probs)\n\nInterpretation:\n\nAUC = 1.0: Perfect ranking\nAUC = 0.5: Random guessing\nAUC < 0.5: Worse than random (flip predictions!)\n\nGuidelines:\n\n0.9-1.0: Excellent\n0.8-0.9: Good\n0.7-0.8: Fair\n0.6-0.7: Poor\n0.5-0.6: Fail","category":"section"},{"location":"binary_classification/#Gini-Coefficient","page":"Binary Classification","title":"Gini Coefficient","text":"gini_coefficient(actual, predicted_probs)\n\nRelationship: Gini = 2 × AUC - 1","category":"section"},{"location":"binary_classification/#KS-Statistic","page":"Binary Classification","title":"KS Statistic","text":"ks_statistic(actual, predicted_probs)\n\nWhen to use: Credit scoring, marketing response modeling.","category":"section"},{"location":"binary_classification/#Probability-Calibration-Metrics","page":"Binary Classification","title":"Probability Calibration Metrics","text":"","category":"section"},{"location":"binary_classification/#Log-Loss","page":"Binary Classification","title":"Log Loss","text":"ll(actual, predicted_probs)      # Elementwise\nlogloss(actual, predicted_probs) # Mean\n\nWhen to use:\n\nTraining neural networks (cross-entropy loss)\nWhen probability values matter, not just ranking","category":"section"},{"location":"binary_classification/#Brier-Score","page":"Binary Classification","title":"Brier Score","text":"brier_score(actual, predicted_probs)\n\nInterpretation:\n\n0: Perfect calibration\n0.25: Random guessing for balanced data\n1: Complete miscalibration\n\nWhen to use: Weather forecasting, medical prognosis - anywhere probability calibration matters.","category":"section"},{"location":"binary_classification/#Precision-and-Recall","page":"Binary Classification","title":"Precision and Recall","text":"","category":"section"},{"location":"binary_classification/#Precision-(Positive-Predictive-Value)","page":"Binary Classification","title":"Precision (Positive Predictive Value)","text":"precision(actual, predicted_labels)\n\nInterpretation: Of all samples predicted positive, what fraction are actually positive?\n\nOptimize for precision when: False positives are costly\n\nSpam detection (don't mark good email as spam)\nLegal discovery (don't flag innocent documents)","category":"section"},{"location":"binary_classification/#Recall-(Sensitivity,-True-Positive-Rate)","page":"Binary Classification","title":"Recall (Sensitivity, True Positive Rate)","text":"recall(actual, predicted_labels)\nsensitivity(actual, predicted_labels)  # Alias\n\nInterpretation: Of all actual positives, what fraction did we detect?\n\nOptimize for recall when: False negatives are costly\n\nDisease screening (don't miss sick patients)\nFraud detection (don't miss fraudulent transactions)\nSecurity threats (don't miss actual threats)","category":"section"},{"location":"binary_classification/#F-Score","page":"Binary Classification","title":"F-Score","text":"fbeta_score(actual, predicted_labels; beta=1.0)\n\nChoosing beta:\n\nβ = 1: Equal weight to precision and recall (F1)\nβ = 0.5: Precision weighted 2× more than recall\nβ = 2: Recall weighted 2× more than precision\n\nFormula: F_β = (1 + β²) × (precision × recall) / (β² × precision + recall)","category":"section"},{"location":"binary_classification/#Specificity-and-NPV","page":"Binary Classification","title":"Specificity and NPV","text":"specificity(actual, predicted_labels)\nnpv(actual, predicted_labels)\n\nRelationships:\n\nSensitivity (recall) + FNR = 1\nSpecificity + FPR = 1\nPrecision + FDR = 1\nNPV + FOR = 1","category":"section"},{"location":"binary_classification/#Error-Rates","page":"Binary Classification","title":"Error Rates","text":"fpr(actual, predicted_labels)  # False Positive Rate\nfnr(actual, predicted_labels)  # False Negative Rate","category":"section"},{"location":"binary_classification/#Combined-Metrics","page":"Binary Classification","title":"Combined Metrics","text":"","category":"section"},{"location":"binary_classification/#Youden's-J-(Informedness)","page":"Binary Classification","title":"Youden's J (Informedness)","text":"youden_j(actual, predicted_labels)\n\nUse case: Finding optimal threshold that maximizes sensitivity + specificity.","category":"section"},{"location":"binary_classification/#Markedness","page":"Binary Classification","title":"Markedness","text":"markedness(actual, predicted_labels)\n\nInterpretation: How marked (informative) are positive and negative predictions?","category":"section"},{"location":"binary_classification/#Fowlkes-Mallows-Index","page":"Binary Classification","title":"Fowlkes-Mallows Index","text":"fowlkes_mallows_index(actual, predicted_labels)","category":"section"},{"location":"binary_classification/#Likelihood-Ratios-(Medical/Diagnostic)","page":"Binary Classification","title":"Likelihood Ratios (Medical/Diagnostic)","text":"positive_likelihood_ratio(actual, predicted_labels)\nnegative_likelihood_ratio(actual, predicted_labels)\ndiagnostic_odds_ratio(actual, predicted_labels)\n\nInterpretation of LR+:\n\nLR+ > 10: Strong evidence for positive\nLR+ = 5-10: Moderate evidence\nLR+ = 2-5: Weak evidence\nLR+ = 1: Useless test\n\nInterpretation of LR-:\n\nLR- < 0.1: Strong evidence for negative\nLR- = 0.1-0.2: Moderate evidence\nLR- = 0.2-0.5: Weak evidence\nLR- = 1: Useless test","category":"section"},{"location":"binary_classification/#Business-Metrics","page":"Binary Classification","title":"Business Metrics","text":"","category":"section"},{"location":"binary_classification/#Lift","page":"Binary Classification","title":"Lift","text":"lift(actual, predicted_probs; percentile=0.1)\n\nInterpretation: How many times better than random in the top X%?\n\nLift = 3 in top 10%: 3× more positives than random","category":"section"},{"location":"binary_classification/#Gain","page":"Binary Classification","title":"Gain","text":"gain(actual, predicted_probs; percentile=0.1)\n\nInterpretation: What percentage of all positives are captured in top X%?","category":"section"},{"location":"binary_classification/#Usage-Examples","page":"Binary Classification","title":"Usage Examples","text":"","category":"section"},{"location":"binary_classification/#Complete-Binary-Classification-Evaluation","page":"Binary Classification","title":"Complete Binary Classification Evaluation","text":"using UnifiedMetrics\n\nactual = [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\npredicted_probs = [0.9, 0.8, 0.7, 0.3, 0.6, 0.4, 0.35, 0.2, 0.1, 0.05]\npredicted_labels = predicted_probs .>= 0.5\n\nprintln(\"=== Threshold-Independent ===\")\nprintln(\"AUC: \", round(auc(actual, predicted_probs), digits=3))\nprintln(\"Gini: \", round(gini_coefficient(actual, predicted_probs), digits=3))\nprintln(\"KS Statistic: \", round(ks_statistic(actual, predicted_probs), digits=3))\nprintln(\"Brier Score: \", round(brier_score(actual, predicted_probs), digits=3))\nprintln(\"Log Loss: \", round(logloss(actual, predicted_probs), digits=3))\n\nprintln(\"\\n=== Threshold-Dependent (threshold=0.5) ===\")\nprintln(\"Precision: \", round(precision(actual, predicted_labels), digits=3))\nprintln(\"Recall: \", round(recall(actual, predicted_labels), digits=3))\nprintln(\"F1 Score: \", round(fbeta_score(actual, predicted_labels), digits=3))\nprintln(\"Specificity: \", round(specificity(actual, predicted_labels), digits=3))\nprintln(\"MCC: \", round(mcc(actual, predicted_labels), digits=3))","category":"section"},{"location":"binary_classification/#Comparing-Different-Thresholds","page":"Binary Classification","title":"Comparing Different Thresholds","text":"using UnifiedMetrics\n\nactual = [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\nprobs = [0.9, 0.8, 0.7, 0.3, 0.6, 0.4, 0.35, 0.2, 0.1, 0.05]\n\nfor threshold in [0.3, 0.5, 0.7]\n    labels = probs .>= threshold\n    println(\"Threshold: $threshold\")\n    println(\"  Precision: $(round(precision(actual, labels), digits=2))\")\n    println(\"  Recall: $(round(recall(actual, labels), digits=2))\")\n    println(\"  F1: $(round(fbeta_score(actual, labels), digits=2))\")\n    println(\"  Youden's J: $(round(youden_j(actual, labels), digits=2))\")\nend","category":"section"},{"location":"binary_classification/#Medical-Diagnostic-Evaluation","page":"Binary Classification","title":"Medical Diagnostic Evaluation","text":"using UnifiedMetrics\n\n# Disease screening results\nactual = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]  # 1 = has disease\npredicted = [1, 1, 1, 0, 0, 0, 0, 0, 1, 0]  # Test results\n\nprintln(\"=== Diagnostic Performance ===\")\nprintln(\"Sensitivity: \", round(sensitivity(actual, predicted), digits=3))\nprintln(\"Specificity: \", round(specificity(actual, predicted), digits=3))\nprintln(\"PPV (Precision): \", round(precision(actual, predicted), digits=3))\nprintln(\"NPV: \", round(npv(actual, predicted), digits=3))\nprintln(\"LR+: \", round(positive_likelihood_ratio(actual, predicted), digits=2))\nprintln(\"LR-: \", round(negative_likelihood_ratio(actual, predicted), digits=2))\nprintln(\"DOR: \", round(diagnostic_odds_ratio(actual, predicted), digits=1))","category":"section"},{"location":"binary_classification/#Marketing/Business-Application","page":"Binary Classification","title":"Marketing/Business Application","text":"using UnifiedMetrics\n\n# Customer response prediction\nactual_responded = [1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\npredicted_scores = [0.9, 0.8, 0.3, 0.7, 0.6, 0.5, 0.4, 0.2, 0.1, 0.05]\n\nprintln(\"=== Campaign Targeting ===\")\nfor pct in [0.1, 0.2, 0.3, 0.5]\n    println(\"Top $(Int(pct*100))%:\")\n    println(\"  Lift: $(round(lift(actual_responded, predicted_scores, percentile=pct), digits=2))x\")\n    println(\"  Gain: $(round(gain(actual_responded, predicted_scores, percentile=pct)*100, digits=1))%\")\nend","category":"section"},{"location":"binary_classification/#Handling-Imbalanced-Data","page":"Binary Classification","title":"Handling Imbalanced Data","text":"using UnifiedMetrics\n\n# Highly imbalanced: 95% negative, 5% positive\nactual = vcat(fill(0, 95), fill(1, 5))\npredicted = vcat(fill(0, 100))  # Naive: always predict negative\n\nprintln(\"=== Naive Model on Imbalanced Data ===\")\nprintln(\"Accuracy: \", accuracy(actual, predicted))  # 0.95 - misleading!\nprintln(\"Recall: \", recall(actual, predicted))       # 0.0 - reveals the problem\nprintln(\"MCC: \", mcc(actual, predicted))             # 0.0 - correctly shows failure\n\n# A better model\npredicted_better = vcat(fill(0, 90), fill(1, 5), fill(0, 3), fill(1, 2))\nprintln(\"\\n=== Better Model ===\")\nprintln(\"Accuracy: \", accuracy(actual, predicted_better))\nprintln(\"Recall: \", recall(actual, predicted_better))\nprintln(\"Precision: \", precision(actual, predicted_better))\nprintln(\"MCC: \", round(mcc(actual, predicted_better), digits=3))\n\nSee the API Reference for complete function documentation.","category":"section"},{"location":"information_retrieval/#Information-Retrieval-Metrics","page":"Information Retrieval","title":"Information Retrieval Metrics","text":"Metrics for evaluating search engines, recommendation systems, and ranking models.","category":"section"},{"location":"information_retrieval/#Overview","page":"Information Retrieval","title":"Overview","text":"Information retrieval metrics evaluate how well a system ranks or retrieves relevant items. They're essential for:\n\nSearch engines\nRecommendation systems\nQuestion answering\nDocument retrieval","category":"section"},{"location":"information_retrieval/#Quick-Reference","page":"Information Retrieval","title":"Quick Reference","text":"Metric Input Range Best For\nndcg Graded relevance [0, 1] Search ranking\nmrr Binary relevance [0, 1] Finding first result\nmapk Binary relevance [0, 1] Overall ranking quality\nhit_rate Binary relevance [0, 1] Recommendations\nprecision_at_k Binary relevance [0, 1] Top-k quality","category":"section"},{"location":"information_retrieval/#Relevance-Types","page":"Information Retrieval","title":"Relevance Types","text":"","category":"section"},{"location":"information_retrieval/#Binary-Relevance","page":"Information Retrieval","title":"Binary Relevance","text":"Items are either relevant (1) or not (0).\n\nactual_relevant = [\"doc1\", \"doc3\", \"doc5\"]  # Relevant documents\nretrieved = [\"doc1\", \"doc2\", \"doc3\"]         # What the system returned","category":"section"},{"location":"information_retrieval/#Graded-Relevance","page":"Information Retrieval","title":"Graded Relevance","text":"Items have relevance scores (e.g., 0-5).\n\nrelevance = [3, 2, 1, 0, 2]  # Scores for items in ranked order","category":"section"},{"location":"information_retrieval/#Discounted-Cumulative-Gain-(DCG)-Family","page":"Information Retrieval","title":"Discounted Cumulative Gain (DCG) Family","text":"","category":"section"},{"location":"information_retrieval/#DCG","page":"Information Retrieval","title":"DCG","text":"dcg(relevance)\n\nHow it works: Sums relevance scores with logarithmic discount by position.\n\nItems at top positions contribute more\nFormula: Σ (2^rel - 1) / log₂(i + 1)","category":"section"},{"location":"information_retrieval/#IDCG-(Ideal-DCG)","page":"Information Retrieval","title":"IDCG (Ideal DCG)","text":"idcg(relevance)\n\nHow it works: DCG of the best possible ranking (sorted by relevance descending).","category":"section"},{"location":"information_retrieval/#NDCG-(Normalized-DCG)","page":"Information Retrieval","title":"NDCG (Normalized DCG)","text":"ndcg(relevance; k=nothing)\n\nInterpretation:\n\nNDCG = 1: Perfect ranking\nNDCG = 0: No relevant items retrieved\n\nWhen to use:\n\nSearch engine evaluation\nWhen relevance is graded (not binary)\nWhen position matters","category":"section"},{"location":"information_retrieval/#Mean-NDCG","page":"Information Retrieval","title":"Mean NDCG","text":"mean_ndcg(relevances; k=nothing)\n\nWhen to use: Evaluating across multiple queries.","category":"section"},{"location":"information_retrieval/#Reciprocal-Rank-Metrics","page":"Information Retrieval","title":"Reciprocal Rank Metrics","text":"","category":"section"},{"location":"information_retrieval/#Reciprocal-Rank","page":"Information Retrieval","title":"Reciprocal Rank","text":"reciprocal_rank(actual, predicted)\n\nHow it works: 1/position of first relevant item.","category":"section"},{"location":"information_retrieval/#Mean-Reciprocal-Rank-(MRR)","page":"Information Retrieval","title":"Mean Reciprocal Rank (MRR)","text":"mrr(actual_list, predicted_list)\n\nWhen to use:\n\nQuestion answering systems\nWhen only the first relevant result matters\nVoice assistants, \"I'm Feeling Lucky\" searches\n\nInterpretation:\n\nMRR = 1: First result always relevant\nMRR = 0.5: First relevant result is typically at position 2","category":"section"},{"location":"information_retrieval/#Average-Precision","page":"Information Retrieval","title":"Average Precision","text":"","category":"section"},{"location":"information_retrieval/#AP@K","page":"Information Retrieval","title":"AP@K","text":"apk(k, actual, predicted)\n\nHow it works: Average precision at each position where a relevant item is found.","category":"section"},{"location":"information_retrieval/#MAP@K-(Mean-Average-Precision)","page":"Information Retrieval","title":"MAP@K (Mean Average Precision)","text":"mapk(k, actual_list, predicted_list)\n\nWhen to use:\n\nStandard metric for document retrieval\nWhen both precision and recall matter\nBenchmark datasets (TREC, MS MARCO)","category":"section"},{"location":"information_retrieval/#Set-Based-Retrieval-Metrics","page":"Information Retrieval","title":"Set-Based Retrieval Metrics","text":"","category":"section"},{"location":"information_retrieval/#F1-Score-(IR-Context)","page":"Information Retrieval","title":"F1 Score (IR Context)","text":"f1(actual, predicted)","category":"section"},{"location":"information_retrieval/#Precision@K","page":"Information Retrieval","title":"Precision@K","text":"precision_at_k(actual, predicted; k)\n\nInterpretation: Of the top K results, what fraction are relevant?","category":"section"},{"location":"information_retrieval/#Recall@K","page":"Information Retrieval","title":"Recall@K","text":"recall_at_k(actual, predicted; k)\n\nInterpretation: Of all relevant items, what fraction appear in top K?","category":"section"},{"location":"information_retrieval/#F1@K","page":"Information Retrieval","title":"F1@K","text":"f1_at_k(actual, predicted; k)","category":"section"},{"location":"information_retrieval/#Hit-Rate","page":"Information Retrieval","title":"Hit Rate","text":"hit_rate(actual_list, predicted_list; k)\n\nWhen to use:\n\nRecommendation systems\nWhen showing at least one good item is success\nE-commerce, media streaming","category":"section"},{"location":"information_retrieval/#Recommendation-System-Metrics","page":"Information Retrieval","title":"Recommendation System Metrics","text":"","category":"section"},{"location":"information_retrieval/#Coverage","page":"Information Retrieval","title":"Coverage","text":"coverage(recommendations, catalog)\n\nInterpretation: What fraction of the catalog gets recommended?\n\nHigh coverage: Diverse recommendations\nLow coverage: Recommendations focus on popular items","category":"section"},{"location":"information_retrieval/#Novelty","page":"Information Retrieval","title":"Novelty","text":"novelty(recommendations, popularity)\n\nInterpretation: Are we recommending non-obvious items?\n\nHigh novelty: Recommending less popular (\"long-tail\") items\nLow novelty: Recommending already-popular items","category":"section"},{"location":"information_retrieval/#Usage-Examples","page":"Information Retrieval","title":"Usage Examples","text":"","category":"section"},{"location":"information_retrieval/#Search-Engine-Evaluation","page":"Information Retrieval","title":"Search Engine Evaluation","text":"using UnifiedMetrics\n\n# Graded relevance scores for top 6 results\n# 3 = highly relevant, 2 = relevant, 1 = marginally relevant, 0 = not relevant\nrelevance = [3, 2, 1, 0, 2, 1]\n\nprintln(\"DCG: \", round(dcg(relevance), digits=3))\nprintln(\"NDCG: \", round(ndcg(relevance), digits=3))\nprintln(\"NDCG@3: \", round(ndcg(relevance, k=3), digits=3))\n\n# Multiple queries\nrelevances = [\n    [3, 2, 1, 0],    # Query 1\n    [0, 1, 2, 3],    # Query 2 (poor ranking)\n    [3, 3, 2, 1],    # Query 3 (good ranking)\n]\nprintln(\"Mean NDCG: \", round(mean_ndcg(relevances), digits=3))\nprintln(\"Mean NDCG@2: \", round(mean_ndcg(relevances, k=2), digits=3))","category":"section"},{"location":"information_retrieval/#Document-Retrieval-Evaluation","page":"Information Retrieval","title":"Document Retrieval Evaluation","text":"using UnifiedMetrics\n\n# Multiple queries\nactual_relevant = [\n    [\"doc1\", \"doc5\", \"doc7\"],           # Relevant docs for query 1\n    [\"doc2\", \"doc3\"],                    # Relevant docs for query 2\n    [\"doc4\", \"doc6\", \"doc8\", \"doc9\"],   # Relevant docs for query 3\n]\n\nretrieved = [\n    [\"doc1\", \"doc2\", \"doc5\", \"doc3\"],   # Retrieved for query 1\n    [\"doc1\", \"doc2\", \"doc4\"],           # Retrieved for query 2\n    [\"doc4\", \"doc5\", \"doc6\", \"doc7\"],   # Retrieved for query 3\n]\n\nprintln(\"MAP@3: \", round(mapk(3, actual_relevant, retrieved), digits=3))\nprintln(\"MRR: \", round(mrr(actual_relevant, retrieved), digits=3))","category":"section"},{"location":"information_retrieval/#Recommendation-System-Evaluation","page":"Information Retrieval","title":"Recommendation System Evaluation","text":"using UnifiedMetrics\n\n# User-item recommendations\nactual_liked = [\n    [\"item_a\", \"item_c\"],              # User 1's liked items\n    [\"item_b\", \"item_d\", \"item_e\"],    # User 2's liked items\n    [\"item_a\", \"item_f\"],              # User 3's liked items\n]\n\nrecommended = [\n    [\"item_a\", \"item_b\", \"item_g\", \"item_c\", \"item_h\"],\n    [\"item_x\", \"item_d\", \"item_y\", \"item_b\", \"item_z\"],\n    [\"item_f\", \"item_a\", \"item_m\", \"item_n\", \"item_o\"],\n]\n\nprintln(\"=== Recommendation Quality ===\")\nprintln(\"Hit Rate@3: \", round(hit_rate(actual_liked, recommended, k=3), digits=3))\nprintln(\"Hit Rate@5: \", round(hit_rate(actual_liked, recommended, k=5), digits=3))\nprintln(\"MRR: \", round(mrr(actual_liked, recommended), digits=3))\nprintln(\"MAP@5: \", round(mapk(5, actual_liked, recommended), digits=3))\n\n# Per-user metrics\nfor (i, (act, rec)) in enumerate(zip(actual_liked, recommended))\n    println(\"User $i - P@3: $(round(precision_at_k(act, rec, k=3), digits=2)), \",\n            \"R@3: $(round(recall_at_k(act, rec, k=3), digits=2))\")\nend","category":"section"},{"location":"information_retrieval/#Evaluating-Recommendation-Diversity","page":"Information Retrieval","title":"Evaluating Recommendation Diversity","text":"using UnifiedMetrics\n\n# Full catalog of items\ncatalog = [\"item_\" * string(i) for i in 1:100]\n\n# Recommendations for 50 users\nrecommendations = [[\"item_1\", \"item_2\", \"item_3\", \"item_5\", \"item_10\"],\n                   [\"item_1\", \"item_3\", \"item_7\", \"item_12\", \"item_15\"],\n                   # ... more users\n                  ]\n\n# What fraction of catalog was recommended?\ncov = coverage(recommendations, catalog)\nprintln(\"Catalog Coverage: $(round(cov*100, digits=1))%\")\n\n# Novelty (recommending less popular items)\npopularity = Dict(\"item_$i\" => 1.0/i for i in 1:100)  # Power law popularity\nnov = novelty(recommendations, popularity)\nprintln(\"Novelty: \", round(nov, digits=2))","category":"section"},{"location":"information_retrieval/#Comparing-Ranking-Models","page":"Information Retrieval","title":"Comparing Ranking Models","text":"using UnifiedMetrics\n\n# Ground truth relevance for 3 queries\nactual = [\n    [\"a\", \"b\", \"c\"],\n    [\"d\", \"e\"],\n    [\"f\", \"g\", \"h\", \"i\"],\n]\n\n# Model A's rankings\nmodel_a = [\n    [\"a\", \"x\", \"b\", \"y\", \"c\"],\n    [\"d\", \"z\", \"e\", \"w\"],\n    [\"f\", \"g\", \"x\", \"h\", \"i\"],\n]\n\n# Model B's rankings\nmodel_b = [\n    [\"x\", \"a\", \"b\", \"c\", \"y\"],\n    [\"e\", \"d\", \"z\", \"w\"],\n    [\"x\", \"y\", \"f\", \"g\", \"h\"],\n]\n\nprintln(\"=== Model Comparison ===\")\nfor (name, model) in [(\"Model A\", model_a), (\"Model B\", model_b)]\n    println(\"$name:\")\n    println(\"  MAP@3: $(round(mapk(3, actual, model), digits=3))\")\n    println(\"  MRR: $(round(mrr(actual, model), digits=3))\")\n    println(\"  Hit Rate@3: $(round(hit_rate(actual, model, k=3), digits=3))\")\nend\n\nSee the API Reference for complete function documentation.","category":"section"},{"location":"choosing_metrics/#Choosing-the-Right-Metric","page":"Choosing the Right Metric","title":"Choosing the Right Metric","text":"This guide helps you select the appropriate metric for your machine learning task. The right metric depends on your problem type, data characteristics, and business requirements.","category":"section"},{"location":"choosing_metrics/#Quick-Decision-Guide","page":"Choosing the Right Metric","title":"Quick Decision Guide","text":"","category":"section"},{"location":"choosing_metrics/#What-type-of-problem-are-you-solving?","page":"Choosing the Right Metric","title":"What type of problem are you solving?","text":"Problem Type Go to Section\nPredicting continuous values (prices, temperatures, etc.) Regression Metrics\nPredicting categories (multi-class) Classification Metrics\nPredicting yes/no outcomes Binary Classification Metrics\nRanking items (search, recommendations) Information Retrieval Metrics\nPredicting future values in a sequence Time Series Metrics\n\n","category":"section"},{"location":"choosing_metrics/#regression-guide","page":"Choosing the Right Metric","title":"Regression Metrics","text":"","category":"section"},{"location":"choosing_metrics/#Decision-Flowchart","page":"Choosing the Right Metric","title":"Decision Flowchart","text":"START: Regression Problem\n    |\n    v\nDo you need interpretable units?\n    |\n    +-- YES --> Do you want to penalize large errors more?\n    |               |\n    |               +-- YES --> Use RMSE\n    |               |\n    |               +-- NO --> Use MAE\n    |\n    +-- NO --> Do you need scale-independent comparison?\n                    |\n                    +-- YES --> Are there zeros in actual values?\n                    |               |\n                    |               +-- YES --> Use SMAPE or WMAPE\n                    |               |\n                    |               +-- NO --> Use MAPE\n                    |\n                    +-- NO --> Use R² (explained_variation)","category":"section"},{"location":"choosing_metrics/#When-to-Use-Each-Metric","page":"Choosing the Right Metric","title":"When to Use Each Metric","text":"Metric Use When Avoid When\nMAE You want average error in original units; outliers should not dominate You need to heavily penalize large errors\nRMSE Large errors are particularly bad; you want same units as target Outliers are present and acceptable\nMAPE You need percentage errors for stakeholder communication Actual values contain zeros or near-zeros\nSMAPE You need percentage errors and have zeros You need asymmetric error treatment\nR² You want to know proportion of variance explained Comparing models on different datasets\nMASE Comparing forecasts across different scales Non-time-series data","category":"section"},{"location":"choosing_metrics/#Detailed-Recommendations","page":"Choosing the Right Metric","title":"Detailed Recommendations","text":"","category":"section"},{"location":"choosing_metrics/#For-General-Regression-Tasks","page":"Choosing the Right Metric","title":"For General Regression Tasks","text":"Primary metric: rmse or mae\n\nUse rmse when large errors are costly (e.g., predicting house prices where a $100K error is much worse than ten $10K errors)\nUse mae when all errors matter equally (e.g., predicting delivery times)\n\n# Standard evaluation\nrmse(actual, predicted)  # Penalizes large errors\nmae(actual, predicted)   # Treats all errors equally","category":"section"},{"location":"choosing_metrics/#For-Percentage-Based-Reporting","page":"Choosing the Right Metric","title":"For Percentage-Based Reporting","text":"Primary metric: mape, smape, or wmape\n\n# When actual values are always positive and non-zero\nmape(actual, predicted)\n\n# When actual values may be zero\nsmape(actual, predicted)  # Symmetric, bounded [0, 2]\nwmape(actual, predicted)  # Weighted by actuals\n\n# To detect systematic bias\nmpe(actual, predicted)  # Positive = under-prediction","category":"section"},{"location":"choosing_metrics/#For-Model-Comparison","page":"Choosing the Right Metric","title":"For Model Comparison","text":"Primary metric: explained_variation (R²) or adjusted_r2\n\n# Basic R²\nexplained_variation(actual, predicted)  # 1 = perfect, 0 = mean baseline\n\n# When comparing models with different numbers of features\nadjusted_r2(actual, predicted, n_features)","category":"section"},{"location":"choosing_metrics/#For-Robust-Models-(Outlier-Resistant)","page":"Choosing the Right Metric","title":"For Robust Models (Outlier-Resistant)","text":"Primary metric: huber_loss or mdae\n\n# Huber loss: quadratic for small errors, linear for large\nhuber_loss(actual, predicted, delta=1.0)\n\n# Median Absolute Error: robust to outliers\nmdae(actual, predicted)","category":"section"},{"location":"choosing_metrics/#For-Skewed-Target-Variables","page":"Choosing the Right Metric","title":"For Skewed Target Variables","text":"Primary metric: rmsle or msle\n\n# For targets spanning multiple orders of magnitude (prices, populations)\nrmsle(actual, predicted)  # Penalizes under-prediction more","category":"section"},{"location":"choosing_metrics/#For-Count-Data-or-GLMs","page":"Choosing the Right Metric","title":"For Count Data or GLMs","text":"Primary metric: mean_poisson_deviance or mean_gamma_deviance\n\n# For count data (website visits, number of purchases)\nmean_poisson_deviance(actual, predicted)\n\n# For positive continuous data with variance ~ mean²\nmean_gamma_deviance(actual, predicted)\n\n","category":"section"},{"location":"choosing_metrics/#classification-guide","page":"Choosing the Right Metric","title":"Classification Metrics","text":"","category":"section"},{"location":"choosing_metrics/#Decision-Flowchart-2","page":"Choosing the Right Metric","title":"Decision Flowchart","text":"START: Multi-class Classification\n    |\n    v\nIs your dataset balanced?\n    |\n    +-- YES --> Use accuracy() or ce()\n    |\n    +-- NO --> Use balanced_accuracy() or cohens_kappa()\n                    |\n                    v\n               Do you need a single summary metric?\n                    |\n                    +-- YES --> For binary: mcc()\n                    |           For ordinal: ScoreQuadraticWeightedKappa()\n                    |\n                    +-- NO --> Use confusion_matrix() for detailed analysis","category":"section"},{"location":"choosing_metrics/#When-to-Use-Each-Metric-2","page":"Choosing the Right Metric","title":"When to Use Each Metric","text":"Metric Use When Avoid When\naccuracy Classes are balanced; simple reporting needed Imbalanced datasets\nbalanced_accuracy Classes are imbalanced You need per-class details\ncohens_kappa You want to account for chance agreement N/A\nmcc Binary classification; best single metric Multi-class (use macro-averaged)\nconfusion_matrix You need detailed error analysis Simple summary is sufficient","category":"section"},{"location":"choosing_metrics/#Detailed-Recommendations-2","page":"Choosing the Right Metric","title":"Detailed Recommendations","text":"","category":"section"},{"location":"choosing_metrics/#For-Balanced-Datasets","page":"Choosing the Right Metric","title":"For Balanced Datasets","text":"accuracy(actual, predicted)  # Simple and interpretable","category":"section"},{"location":"choosing_metrics/#For-Imbalanced-Datasets","page":"Choosing the Right Metric","title":"For Imbalanced Datasets","text":"# Macro-averaged recall across classes\nbalanced_accuracy(actual, predicted)\n\n# Accounts for chance agreement\ncohens_kappa(actual, predicted)","category":"section"},{"location":"choosing_metrics/#For-Ordinal-Classification","page":"Choosing the Right Metric","title":"For Ordinal Classification","text":"When classes have a natural order (e.g., ratings 1-5):\n\n# Penalizes predictions farther from true class\nScoreQuadraticWeightedKappa(actual, predicted, min_rating=1, max_rating=5)","category":"section"},{"location":"choosing_metrics/#For-Multi-Label-Classification","page":"Choosing the Right Metric","title":"For Multi-Label Classification","text":"# Fraction of incorrect labels\nhamming_loss(actual_matrix, predicted_matrix)\n\n","category":"section"},{"location":"choosing_metrics/#binary-guide","page":"Choosing the Right Metric","title":"Binary Classification Metrics","text":"","category":"section"},{"location":"choosing_metrics/#Decision-Flowchart-3","page":"Choosing the Right Metric","title":"Decision Flowchart","text":"START: Binary Classification\n    |\n    v\nWhat type of predictions do you have?\n    |\n    +-- Probabilities (0-1) --> Do you need threshold-independent evaluation?\n    |                               |\n    |                               +-- YES --> Use auc() or gini_coefficient()\n    |                               |\n    |                               +-- NO --> What matters more?\n    |                                               |\n    |                                               +-- Calibration --> brier_score() or logloss()\n    |                                               |\n    |                                               +-- Ranking --> ks_statistic()\n    |\n    +-- Binary Labels (0/1) --> What is your priority?\n                                    |\n                                    +-- Balance precision/recall --> fbeta_score()\n                                    |\n                                    +-- Minimize false positives --> precision()\n                                    |\n                                    +-- Minimize false negatives --> recall()\n                                    |\n                                    +-- Single best metric --> mcc()","category":"section"},{"location":"choosing_metrics/#When-to-Use-Each-Metric-3","page":"Choosing the Right Metric","title":"When to Use Each Metric","text":"Metric Use When Avoid When\nauc Comparing models; threshold hasn't been chosen You need a specific operating point\nprecision False positives are costly (spam detection) Missing positives is worse\nrecall False negatives are costly (disease detection) False alarms are problematic\nfbeta_score You need to balance precision and recall Clear priority for one over other\nmcc Imbalanced data; need single summary metric You need threshold-independent metric\nbrier_score Probability calibration matters Ranking is more important","category":"section"},{"location":"choosing_metrics/#Detailed-Recommendations-3","page":"Choosing the Right Metric","title":"Detailed Recommendations","text":"","category":"section"},{"location":"choosing_metrics/#For-Model-Selection-(Before-Choosing-Threshold)","page":"Choosing the Right Metric","title":"For Model Selection (Before Choosing Threshold)","text":"# Area Under ROC Curve - threshold independent\nauc(actual, predicted_scores)\n\n# Gini coefficient (= 2*AUC - 1)\ngini_coefficient(actual, predicted_scores)\n\n# Maximum separation between classes\nks_statistic(actual, predicted_scores)","category":"section"},{"location":"choosing_metrics/#For-Probability-Calibration","page":"Choosing the Right Metric","title":"For Probability Calibration","text":"When you need well-calibrated probabilities:\n\n# Mean squared error of probabilities\nbrier_score(actual, predicted_probs)  # Lower is better\n\n# Cross-entropy loss\nlogloss(actual, predicted_probs)  # Lower is better","category":"section"},{"location":"choosing_metrics/#For-Threshold-Based-Evaluation","page":"Choosing the Right Metric","title":"For Threshold-Based Evaluation","text":"After choosing a classification threshold:\n\n# Convert probabilities to labels\npredicted_labels = predicted_probs .>= threshold\n\n# When false positives are costly (spam filter, fraud detection)\nprecision(actual, predicted_labels)\n\n# When false negatives are costly (disease screening, security threats)\nrecall(actual, predicted_labels)\nsensitivity(actual, predicted_labels)  # Same as recall\n\n# Balanced metric\nfbeta_score(actual, predicted_labels)         # F1: equal weight\nfbeta_score(actual, predicted_labels, beta=0.5)  # Favor precision\nfbeta_score(actual, predicted_labels, beta=2.0)  # Favor recall","category":"section"},{"location":"choosing_metrics/#For-Medical/Diagnostic-Applications","page":"Choosing the Right Metric","title":"For Medical/Diagnostic Applications","text":"# Sensitivity (true positive rate)\nsensitivity(actual, predicted_labels)\n\n# Specificity (true negative rate)\nspecificity(actual, predicted_labels)\n\n# Youden's J (optimal threshold criterion)\nyouden_j(actual, predicted_labels)\n\n# Likelihood ratios for clinical decision making\npositive_likelihood_ratio(actual, predicted_labels)\nnegative_likelihood_ratio(actual, predicted_labels)\ndiagnostic_odds_ratio(actual, predicted_labels)","category":"section"},{"location":"choosing_metrics/#For-Imbalanced-Data","page":"Choosing the Right Metric","title":"For Imbalanced Data","text":"The single best metric for binary classification with imbalanced data:\n\n# Matthews Correlation Coefficient: accounts for all quadrants of confusion matrix\nmcc(actual, predicted_labels)  # Range: [-1, 1], 0 = random","category":"section"},{"location":"choosing_metrics/#For-Business-Applications","page":"Choosing the Right Metric","title":"For Business Applications","text":"# Lift: how much better than random in top X%\nlift(actual, predicted_scores, percentile=0.1)\n\n# Gain: what % of positives captured in top X%\ngain(actual, predicted_scores, percentile=0.1)\n\n","category":"section"},{"location":"choosing_metrics/#ir-guide","page":"Choosing the Right Metric","title":"Information Retrieval Metrics","text":"","category":"section"},{"location":"choosing_metrics/#Decision-Flowchart-4","page":"Choosing the Right Metric","title":"Decision Flowchart","text":"START: Ranking/Retrieval Problem\n    |\n    v\nDo you have graded relevance scores?\n    |\n    +-- YES --> Use ndcg() or dcg()\n    |\n    +-- NO (binary relevance) --> What matters more?\n                                      |\n                                      +-- Finding first relevant item --> mrr()\n                                      |\n                                      +-- Finding all relevant items --> recall_at_k()\n                                      |\n                                      +-- Precision of top results --> precision_at_k()\n                                      |\n                                      +-- Balance of both --> f1_at_k() or mapk()","category":"section"},{"location":"choosing_metrics/#When-to-Use-Each-Metric-4","page":"Choosing the Right Metric","title":"When to Use Each Metric","text":"Metric Use When Avoid When\nndcg Relevance is graded (0-5 stars) Binary relevance only\nmrr Only first relevant result matters All relevant items matter\nmap@k Ranking quality across positions matters Only top-1 or top-k matters\nrecall@k Coverage of relevant items is priority Precision matters more\nprecision@k Quality of top results is priority Missing relevant items is costly\nhit_rate At least one relevant in top-k is success Need finer granularity","category":"section"},{"location":"choosing_metrics/#Detailed-Recommendations-4","page":"Choosing the Right Metric","title":"Detailed Recommendations","text":"","category":"section"},{"location":"choosing_metrics/#For-Search-Engines","page":"Choosing the Right Metric","title":"For Search Engines","text":"# Graded relevance (best for search)\nndcg(relevance_scores, k=10)\n\n# Mean NDCG across queries\nmean_ndcg(relevances_list, k=10)\n\n# Mean Reciprocal Rank (how quickly users find what they want)\nmrr(actual_list, predicted_list)","category":"section"},{"location":"choosing_metrics/#For-Recommendation-Systems","page":"Choosing the Right Metric","title":"For Recommendation Systems","text":"# Did we show at least one good item?\nhit_rate(actual_list, predicted_list, k=10)\n\n# How many relevant items did we show?\nrecall_at_k(actual, predicted, k=10)\n\n# What fraction of shown items are relevant?\nprecision_at_k(actual, predicted, k=10)\n\n# Balanced metric\nf1_at_k(actual, predicted, k=10)\n\n# Catalog coverage (diversity)\ncoverage(predicted_list, full_catalog)\n\n# Novelty (recommending non-obvious items)\nnovelty(predicted_list, item_popularity)","category":"section"},{"location":"choosing_metrics/#For-E-commerce-/-Product-Search","page":"Choosing the Right Metric","title":"For E-commerce / Product Search","text":"# Average precision at k\napk(10, relevant_products, retrieved_products)\n\n# Mean AP across queries\nmapk(10, relevant_lists, retrieved_lists)\n\n","category":"section"},{"location":"choosing_metrics/#ts-guide","page":"Choosing the Right Metric","title":"Time Series Metrics","text":"","category":"section"},{"location":"choosing_metrics/#Decision-Flowchart-5","page":"Choosing the Right Metric","title":"Decision Flowchart","text":"START: Time Series Forecasting\n    |\n    v\nWhat aspect of forecast quality matters?\n    |\n    +-- Point forecast accuracy --> Is scale-independent comparison needed?\n    |                                   |\n    |                                   +-- YES --> mase() or theil_u2()\n    |                                   |\n    |                                   +-- NO --> rmse() or mae()\n    |\n    +-- Directional accuracy --> directional_accuracy()\n    |\n    +-- Forecast bias --> tracking_signal() or forecast_bias()\n    |\n    +-- Prediction intervals --> coverage_probability() or winkler_score()","category":"section"},{"location":"choosing_metrics/#When-to-Use-Each-Metric-5","page":"Choosing the Right Metric","title":"When to Use Each Metric","text":"Metric Use When Avoid When\nmase Comparing across series with different scales Single series evaluation\nrmsse Scale-independent; sensitive to large errors Outliers acceptable\ntracking_signal Monitoring for systematic bias One-time evaluation\ndirectional_accuracy Direction matters more than magnitude Magnitude accuracy critical\nwinkler_score Evaluating prediction intervals Point forecasts only\ntheil_u2 Comparing to naive benchmark Absolute accuracy needed","category":"section"},{"location":"choosing_metrics/#Detailed-Recommendations-5","page":"Choosing the Right Metric","title":"Detailed Recommendations","text":"","category":"section"},{"location":"choosing_metrics/#For-Comparing-Forecasts-Across-Different-Series","page":"Choosing the Right Metric","title":"For Comparing Forecasts Across Different Series","text":"The M-competition recommended metrics:\n\n# Mean Absolute Scaled Error (most recommended)\nmase(actual, predicted, m=1)     # Non-seasonal\nmase(actual, predicted, m=12)    # Monthly data with yearly seasonality\nmase(actual, predicted, m=7)     # Daily data with weekly seasonality\n\n# Root Mean Squared Scaled Error\nrmsse(actual, predicted, m=1)\n\nInterpretation:\n\nMASE < 1: Better than naive forecast\nMASE = 1: Same as naive forecast\nMASE > 1: Worse than naive forecast","category":"section"},{"location":"choosing_metrics/#For-Single-Series-Evaluation","page":"Choosing the Right Metric","title":"For Single Series Evaluation","text":"# Standard metrics in original units\nmae(actual, predicted)\nrmse(actual, predicted)\n\n# Percentage-based (avoid if zeros present)\nmape(actual, predicted)\nwape(actual, predicted)  # Handles zeros better","category":"section"},{"location":"choosing_metrics/#For-Detecting-Forecast-Bias","page":"Choosing the Right Metric","title":"For Detecting Forecast Bias","text":"# Normalized measure of cumulative error\ntracking_signal(actual, predicted)\n# Interpretation: values outside [-4, 4] indicate systematic bias\n\n# Simple bias (positive = under-forecasting)\nforecast_bias(actual, predicted)","category":"section"},{"location":"choosing_metrics/#For-Comparing-to-Benchmark","page":"Choosing the Right Metric","title":"For Comparing to Benchmark","text":"# Theil's U2: comparison to naive forecast\ntheil_u2(actual, predicted, m=1)\n# < 1: better than naive, > 1: worse than naive\n\n# Theil's U1: normalized error\ntheil_u1(actual, predicted)\n# 0 = perfect, 1 = worst","category":"section"},{"location":"choosing_metrics/#For-Direction-Prediction-(Trading,-etc.)","page":"Choosing the Right Metric","title":"For Direction Prediction (Trading, etc.)","text":"# What fraction of up/down movements were predicted correctly?\ndirectional_accuracy(actual, predicted)","category":"section"},{"location":"choosing_metrics/#For-Probabilistic-Forecasts-/-Prediction-Intervals","page":"Choosing the Right Metric","title":"For Probabilistic Forecasts / Prediction Intervals","text":"# Does the interval contain the actual value at expected rate?\ncoverage_probability(actual, lower, upper)\n# Should match your confidence level (e.g., 0.95 for 95% intervals)\n\n# Interval score (rewards narrow intervals, penalizes misses)\nwinkler_score(actual, lower, upper, alpha=0.05)\n\n# Quantile forecast evaluation\npinball_loss_series(actual, predicted_quantile, quantile=0.9)","category":"section"},{"location":"choosing_metrics/#For-Preserving-Temporal-Structure","page":"Choosing the Right Metric","title":"For Preserving Temporal Structure","text":"# Does the forecast maintain autocorrelation patterns?\nautocorrelation_error(actual, predicted, max_lag=10)\n\n","category":"section"},{"location":"choosing_metrics/#Common-Mistakes-to-Avoid","page":"Choosing the Right Metric","title":"Common Mistakes to Avoid","text":"","category":"section"},{"location":"choosing_metrics/#Regression","page":"Choosing the Right Metric","title":"Regression","text":"Using MAPE with zeros: MAPE is undefined when actual values are zero. Use SMAPE or WMAPE instead.\nIgnoring scale: When comparing models across different datasets, use scale-independent metrics (R², MAPE, MASE).\nOnly using R²: R² can be misleading for non-linear relationships. Always check residual plots.","category":"section"},{"location":"choosing_metrics/#Classification","page":"Choosing the Right Metric","title":"Classification","text":"Using accuracy on imbalanced data: A model predicting the majority class always achieves high accuracy. Use balanced_accuracy, MCC, or per-class metrics.\nOptimizing for wrong metric: If false negatives are costly (medical diagnosis), optimize for recall, not precision.","category":"section"},{"location":"choosing_metrics/#Binary-Classification","page":"Choosing the Right Metric","title":"Binary Classification","text":"Comparing AUC across very different datasets: AUC can be misleading if class distributions differ significantly.\nIgnoring calibration: High AUC doesn't mean probabilities are well-calibrated. Check Brier score.\nUsing accuracy on imbalanced data: Use MCC instead.","category":"section"},{"location":"choosing_metrics/#Information-Retrieval","page":"Choosing the Right Metric","title":"Information Retrieval","text":"Using NDCG with binary relevance: While valid, simpler metrics (MAP, MRR) may be more interpretable.\nIgnoring position: Metrics like precision don't account for ranking. Use NDCG or MRR.","category":"section"},{"location":"choosing_metrics/#Time-Series","page":"Choosing the Right Metric","title":"Time Series","text":"Not using scaled metrics: Raw MAE/RMSE can't be compared across series with different scales.\nIgnoring seasonality in MASE: Set m to match your data's seasonal period.\nOnly checking point accuracy: Also evaluate bias (trackingsignal) and intervals (coverageprobability).\n\n","category":"section"},{"location":"choosing_metrics/#Metric-Selection-Summary-Table","page":"Choosing the Right Metric","title":"Metric Selection Summary Table","text":"Scenario Recommended Metric Alternative\nGeneral regression RMSE MAE\nRegression with outliers Huber loss MdAE\nStakeholder reporting MAPE (if no zeros) SMAPE\nImbalanced binary classification MCC Balanced accuracy\nMedical diagnosis Sensitivity + Specificity Youden's J\nSearch ranking NDCG MRR\nRecommendation system Hit rate, Recall@k MAP@k\nForecast comparison MASE RMSSE\nForecast monitoring Tracking signal Forecast bias\nPrediction intervals Coverage + Winkler Pinball loss","category":"section"},{"location":"classification/#Classification-Metrics","page":"Classification","title":"Classification Metrics","text":"Metrics for evaluating multi-class classification models.","category":"section"},{"location":"classification/#Overview","page":"Classification","title":"Overview","text":"Classification metrics evaluate how well a model assigns items to discrete categories. These metrics work with any number of classes and support both integer and string labels.","category":"section"},{"location":"classification/#Quick-Reference","page":"Classification","title":"Quick Reference","text":"Metric Range Best For\naccuracy [0, 1] Balanced datasets\nbalanced_accuracy [0, 1] Imbalanced datasets\ncohens_kappa [-1, 1] Accounting for chance\nmcc [-1, 1] Binary, imbalanced data\nhamming_loss [0, 1] Multi-label problems","category":"section"},{"location":"classification/#Basic-Accuracy-Metrics","page":"Classification","title":"Basic Accuracy Metrics","text":"accuracy(actual, predicted)  # Classification accuracy\nce(actual, predicted)        # Classification error (1 - accuracy)\n\nWhen to use:\n\naccuracy: Quick evaluation on balanced datasets\nce (classification error): When you want error rate instead of accuracy\n\nImportant: Accuracy can be misleading on imbalanced datasets. If 95% of samples are class A, a model that always predicts A achieves 95% accuracy.","category":"section"},{"location":"classification/#Balanced-Accuracy","page":"Classification","title":"Balanced Accuracy","text":"balanced_accuracy(actual, predicted)\n\nWhen to use:\n\nImbalanced datasets\nWhen each class is equally important regardless of frequency\n\nHow it works: Computes recall for each class and averages them.","category":"section"},{"location":"classification/#Agreement-Metrics","page":"Classification","title":"Agreement Metrics","text":"cohens_kappa(actual, predicted)\n\nInterpretation:\n\nκ = 1: Perfect agreement\nκ = 0: Agreement equals chance\nκ < 0: Less agreement than chance\n\nGuidelines (Landis & Koch, 1977):\n\n0.81-1.00: Almost perfect\n0.61-0.80: Substantial\n0.41-0.60: Moderate\n0.21-0.40: Fair\n0.00-0.20: Slight","category":"section"},{"location":"classification/#Matthews-Correlation-Coefficient","page":"Classification","title":"Matthews Correlation Coefficient","text":"matthews_corrcoef(actual, predicted)\nmcc(actual, predicted)  # Alias\n\nWhen to use: The recommended single metric for binary classification, especially with imbalanced data.\n\nInterpretation:\n\nMCC = 1: Perfect predictions\nMCC = 0: Random predictions\nMCC = -1: Complete disagreement","category":"section"},{"location":"classification/#Confusion-Matrix","page":"Classification","title":"Confusion Matrix","text":"confusion_matrix(actual, predicted)\n\nHow to use:\n\nactual = [1, 1, 1, 0, 0, 0]\npredicted = [1, 0, 1, 1, 0, 0]\n\ncm = confusion_matrix(actual, predicted)\ncm[:matrix]   # The confusion matrix\ncm[:labels]   # [0, 1]\n\n# For binary classification [0, 1]:\n# matrix[1,1] = TN, matrix[1,2] = FP\n# matrix[2,1] = FN, matrix[2,2] = TP","category":"section"},{"location":"classification/#Top-K-Accuracy","page":"Classification","title":"Top-K Accuracy","text":"top_k_accuracy(actual, predicted_probs, k)\n\nWhen to use:\n\nMulti-class problems with many classes\nWhen partial credit for \"close\" predictions matters\nImage classification, recommendation systems\n\nExample:\n\nactual = [1, 2, 3]  # True classes (1-indexed)\nprobs = [0.7 0.2 0.1;   # Sample 1: class 1 most likely (correct)\n         0.3 0.4 0.3;   # Sample 2: class 2 most likely (correct)\n         0.4 0.4 0.2]   # Sample 3: class 3 not in top-2 (incorrect for top-2)\n\ntop_k_accuracy(actual, probs, 1)  # Standard accuracy\ntop_k_accuracy(actual, probs, 2)  # Correct if true class in top 2","category":"section"},{"location":"classification/#Quadratic-Weighted-Kappa","page":"Classification","title":"Quadratic Weighted Kappa","text":"ScoreQuadraticWeightedKappa(rater_a, rater_b; min_rating, max_rating)\nMeanQuadraticWeightedKappa(kappas; weights=nothing)\n\nWhen to use:\n\nOrdinal classification (ratings, severity levels)\nWhen distance between classes matters\nMedical/educational assessments\n\nExample:\n\n# Rating predictions (1-5 scale)\nactual_ratings = [1, 2, 3, 4, 5, 3, 2]\npredicted_ratings = [1, 2, 2, 4, 4, 3, 3]\n\n# Predicting 2 when actual is 3 is penalized less than predicting 1\nScoreQuadraticWeightedKappa(actual_ratings, predicted_ratings,\n                            min_rating=1, max_rating=5)","category":"section"},{"location":"classification/#Loss-Functions","page":"Classification","title":"Loss Functions","text":"","category":"section"},{"location":"classification/#Hamming-Loss","page":"Classification","title":"Hamming Loss","text":"hamming_loss(actual, predicted)\n\nWhen to use:\n\nMulti-label classification\nWhen you want to penalize each label error equally","category":"section"},{"location":"classification/#Zero-One-Loss","page":"Classification","title":"Zero-One Loss","text":"zero_one_loss(actual, predicted)","category":"section"},{"location":"classification/#Hinge-Loss","page":"Classification","title":"Hinge Loss","text":"hinge_loss(actual, predicted)\nsquared_hinge_loss(actual, predicted)\n\nWhen to use:\n\nSupport Vector Machine evaluation\nLabels should be -1 and 1 (not 0 and 1)\npredicted should be decision function values, not probabilities","category":"section"},{"location":"classification/#Usage-Examples","page":"Classification","title":"Usage Examples","text":"","category":"section"},{"location":"classification/#Evaluating-a-Multi-Class-Model","page":"Classification","title":"Evaluating a Multi-Class Model","text":"using UnifiedMetrics\n\nactual = [\"cat\", \"dog\", \"bird\", \"cat\", \"dog\", \"bird\"]\npredicted = [\"cat\", \"cat\", \"bird\", \"dog\", \"dog\", \"cat\"]\n\nprintln(\"Accuracy: \", accuracy(actual, predicted))\nprintln(\"Balanced Accuracy: \", balanced_accuracy(actual, predicted))\nprintln(\"Cohen's Kappa: \", cohens_kappa(actual, predicted))\n\ncm = confusion_matrix(actual, predicted)\nprintln(\"Confusion Matrix:\")\nprintln(cm[:matrix])\nprintln(\"Labels: \", cm[:labels])","category":"section"},{"location":"classification/#Handling-Imbalanced-Data","page":"Classification","title":"Handling Imbalanced Data","text":"# Highly imbalanced: 90% class A, 10% class B\nactual = vcat(fill(\"A\", 90), fill(\"B\", 10))\npredicted = fill(\"A\", 100)  # Naive model always predicts A\n\naccuracy(actual, predicted)           # 0.9 - looks good!\nbalanced_accuracy(actual, predicted)  # 0.5 - reveals the problem","category":"section"},{"location":"classification/#Ordinal-Classification","page":"Classification","title":"Ordinal Classification","text":"# Patient pain levels: 1 (none) to 5 (severe)\nactual = [1, 2, 3, 4, 5, 3, 2, 4]\npredicted = [1, 2, 2, 4, 4, 3, 3, 5]\n\n# Standard accuracy doesn't account for \"closeness\"\naccuracy(actual, predicted)  # 0.625\n\n# QWK penalizes predictions far from actual more\nScoreQuadraticWeightedKappa(actual, predicted, min_rating=1, max_rating=5)","category":"section"},{"location":"classification/#Multi-Label-Classification","page":"Classification","title":"Multi-Label Classification","text":"# Each sample can have multiple labels (e.g., image tags)\nactual = Bool[1 0 1; 0 1 1; 1 1 0]      # 3 samples, 3 labels\npredicted = Bool[1 1 1; 0 1 0; 1 0 0]   # Predictions\n\n# Fraction of incorrectly predicted labels\nhamming_loss(actual, predicted)  # 0.444 (4 of 9 labels wrong)\n\nSee the API Reference for complete function documentation.","category":"section"},{"location":"regression/#Regression-Metrics","page":"Regression","title":"Regression Metrics","text":"Metrics for evaluating models that predict continuous values.","category":"section"},{"location":"regression/#Overview","page":"Regression","title":"Overview","text":"Regression metrics measure the difference between predicted and actual continuous values. UnifiedMetrics.jl provides 32 regression metrics grouped into several categories:\n\nBasic error metrics: MAE, RMSE, MSE\nPercentage-based metrics: MAPE, SMAPE, WMAPE\nRelative metrics: R², RAE, RSE\nRobust metrics: Huber loss, MdAE\nGLM-specific metrics: Tweedie deviance, Poisson/Gamma deviance","category":"section"},{"location":"regression/#Quick-Reference","page":"Regression","title":"Quick Reference","text":"Metric Formula Best For\nmae mean(|y - ŷ|) General purpose, interpretable\nrmse √mean((y - ŷ)²) Penalizing large errors\nmape mean(|y - ŷ| / |y|) Percentage reporting\nsmape Symmetric MAPE When actuals have zeros\nexplained_variation 1 - RSS/TSS Model explanation power","category":"section"},{"location":"regression/#Basic-Error-Metrics","page":"Regression","title":"Basic Error Metrics","text":"","category":"section"},{"location":"regression/#Absolute-Error-Family","page":"Regression","title":"Absolute Error Family","text":"ae(actual, predicted)   # Elementwise absolute error\nmae(actual, predicted)  # Mean absolute error\nmdae(actual, predicted) # Median absolute error\n\nWhen to use:\n\nmae: Standard choice when all errors are equally important\nmdae: When you want robustness to outliers (median instead of mean)","category":"section"},{"location":"regression/#Squared-Error-Family","page":"Regression","title":"Squared Error Family","text":"se(actual, predicted)   # Elementwise squared error\nsse(actual, predicted)  # Sum of squared errors\nmse(actual, predicted)  # Mean squared error\nrmse(actual, predicted) # Root mean squared error\n\nWhen to use:\n\nrmse: When large errors are particularly costly\nmse: As a loss function for optimization (differentiable)","category":"section"},{"location":"regression/#Normalized-RMSE","page":"Regression","title":"Normalized RMSE","text":"nrmse(actual, predicted; normalization=:range)\n\nWhen to use: Comparing RMSE across datasets with different scales.","category":"section"},{"location":"regression/#Percentage-Based-Metrics","page":"Regression","title":"Percentage-Based Metrics","text":"","category":"section"},{"location":"regression/#Basic-Percentage-Errors","page":"Regression","title":"Basic Percentage Errors","text":"ape(actual, predicted)  # Elementwise absolute percentage error\nmape(actual, predicted) # Mean absolute percentage error\n\nWarning: MAPE is undefined when actual values are zero.","category":"section"},{"location":"regression/#Symmetric-and-Weighted-Alternatives","page":"Regression","title":"Symmetric and Weighted Alternatives","text":"smape(actual, predicted) # Symmetric MAPE\nwmape(actual, predicted) # Weighted MAPE\nmpe(actual, predicted)   # Mean percentage error (signed)\n\nWhen to use:\n\nsmape: When actuals may be zero; bounded between 0 and 2\nwmape: When larger values should have more influence\nmpe: When you want to detect systematic over/under-prediction","category":"section"},{"location":"regression/#Bias-Metrics","page":"Regression","title":"Bias Metrics","text":"bias(actual, predicted)         # Average bias\npercent_bias(actual, predicted) # Percentage bias\n\nInterpretation:\n\nPositive bias: Model under-predicts on average\nNegative bias: Model over-predicts on average","category":"section"},{"location":"regression/#Logarithmic-Error-Metrics","page":"Regression","title":"Logarithmic Error Metrics","text":"sle(actual, predicted)   # Elementwise squared log error\nmsle(actual, predicted)  # Mean squared log error\nrmsle(actual, predicted) # Root mean squared log error\n\nWhen to use:\n\nWhen targets span multiple orders of magnitude (prices, populations)\nWhen under-prediction is worse than over-prediction\nNote: Only for positive values","category":"section"},{"location":"regression/#Relative-Error-Metrics","page":"Regression","title":"Relative Error Metrics","text":"rse(actual, predicted)  # Relative squared error\nrrse(actual, predicted) # Root relative squared error\nrae(actual, predicted)  # Relative absolute error\n\nInterpretation: Error relative to a naive model that predicts the mean.","category":"section"},{"location":"regression/#Model-Explanation-Metrics","page":"Regression","title":"Model Explanation Metrics","text":"explained_variation(actual, predicted)      # R² (coefficient of determination)\nadjusted_r2(actual, predicted, n_features)  # Adjusted R²\n\nInterpretation for R²:\n\nR² = 1: Perfect predictions\nR² = 0: Model is no better than predicting the mean\nR² < 0: Model is worse than predicting the mean","category":"section"},{"location":"regression/#Extreme-Error-Metrics","page":"Regression","title":"Extreme Error Metrics","text":"max_error(actual, predicted) # Maximum absolute error\nmax_ae(actual, predicted)    # Alias for max_error\n\nWhen to use: When worst-case error matters (safety-critical applications).","category":"section"},{"location":"regression/#Robust-Loss-Functions","page":"Regression","title":"Robust Loss Functions","text":"huber_loss(actual, predicted; delta=1.0)\nlog_cosh_loss(actual, predicted)\n\nWhen to use:\n\nWhen data has outliers\nAs training loss functions for neural networks\nhuber_loss: Quadratic for small errors, linear for large\nlog_cosh_loss: Similar to Huber but twice differentiable","category":"section"},{"location":"regression/#Quantile-Loss","page":"Regression","title":"Quantile Loss","text":"quantile_loss(actual, predicted; quantile=0.5)\npinball_loss(actual, predicted; quantile=0.5)  # Alias\n\nWhen to use:\n\nQuantile regression\nWhen asymmetric errors matter\nquantile=0.9: Penalize under-prediction more\nquantile=0.1: Penalize over-prediction more","category":"section"},{"location":"regression/#GLM-Deviance-Metrics","page":"Regression","title":"GLM Deviance Metrics","text":"tweedie_deviance(actual, predicted; power=1.5)\nmean_poisson_deviance(actual, predicted)\nmean_gamma_deviance(actual, predicted)\nd2_tweedie_score(actual, predicted; power=1.5)\n\nWhen to use:\n\nEvaluating Generalized Linear Models\nmean_poisson_deviance: Count data (visitors, purchases)\nmean_gamma_deviance: Positive continuous with variance ∝ mean² (insurance claims)","category":"section"},{"location":"regression/#Usage-Examples","page":"Regression","title":"Usage Examples","text":"","category":"section"},{"location":"regression/#Comparing-Multiple-Models","page":"Regression","title":"Comparing Multiple Models","text":"using UnifiedMetrics\n\nactual = [10.0, 20.0, 30.0, 40.0, 50.0]\nmodel1_pred = [12.0, 18.0, 31.0, 38.0, 52.0]\nmodel2_pred = [11.0, 21.0, 29.0, 41.0, 48.0]\n\n# Compare using multiple metrics\nfor (name, pred) in [(\"Model 1\", model1_pred), (\"Model 2\", model2_pred)]\n    println(\"$name:\")\n    println(\"  MAE:  $(round(mae(actual, pred), digits=2))\")\n    println(\"  RMSE: $(round(rmse(actual, pred), digits=2))\")\n    println(\"  R²:   $(round(explained_variation(actual, pred), digits=3))\")\nend","category":"section"},{"location":"regression/#Handling-Data-with-Zeros","page":"Regression","title":"Handling Data with Zeros","text":"actual = [0.0, 10.0, 20.0, 0.0, 30.0]\npredicted = [1.0, 9.0, 21.0, 2.0, 28.0]\n\n# MAPE will be Inf due to zeros\n# mape(actual, predicted)  # Don't use!\n\n# Use SMAPE or WMAPE instead\nsmape(actual, predicted)  # Bounded, handles zeros\nwmape(actual, predicted)  # Weighted by actuals","category":"section"},{"location":"regression/#Robust-Evaluation-with-Outliers","page":"Regression","title":"Robust Evaluation with Outliers","text":"actual = [1.0, 2.0, 3.0, 4.0, 100.0]  # 100 is an outlier\npredicted = [1.1, 2.1, 2.9, 4.1, 5.0]\n\n# Standard metrics are heavily influenced by outlier\nrmse(actual, predicted)  # ~42.5\nmae(actual, predicted)   # ~19.0\n\n# Robust alternatives\nmdae(actual, predicted)  # ~0.1 (median)\nhuber_loss(actual, predicted, delta=1.0)  # Less sensitive\n\nSee the API Reference for complete function documentation.","category":"section"},{"location":"#UnifiedMetrics.jl","page":"Home","title":"UnifiedMetrics.jl","text":"A comprehensive Julia package for evaluating machine learning models. Provides 97+ metrics across regression, classification, binary classification, information retrieval, and time series forecasting.","category":"section"},{"location":"#Features","page":"Home","title":"Features","text":"Time Series Forecasting: 13 specialized metrics including MASE, RMSSE, tracking signal, Winkler score - with comprehensive guidance on scale-independent evaluation, bias detection, and probabilistic forecasting\nRegression: 32 metrics including MAE, RMSE, MAPE, R², Huber loss, and more\nClassification: 14 metrics including accuracy, balanced accuracy, Cohen's Kappa, MCC\nBinary Classification: 23 metrics including AUC, precision, recall, F-score, Brier score\nInformation Retrieval: 15 metrics including NDCG, MRR, MAP@K, hit rate","category":"section"},{"location":"#Installation","page":"Home","title":"Installation","text":"using Pkg\nPkg.add(\"UnifiedMetrics\")\n\nOr in the Julia REPL package mode (press ]):\n\nadd UnifiedMetrics","category":"section"},{"location":"#Quick-Example","page":"Home","title":"Quick Example","text":"","category":"section"},{"location":"#Time-Series-Forecasting","page":"Home","title":"Time Series Forecasting","text":"using UnifiedMetrics\n\nactual = [100.0, 110.0, 105.0, 115.0, 120.0, 125.0, 130.0, 128.0]\npredicted = [98.0, 108.0, 107.0, 113.0, 118.0, 123.0, 128.0, 126.0]\n\n# Scale-independent metrics (compare to naive forecast)\nmase(actual, predicted)           # < 1 means better than naive\nmase(actual, predicted, m=12)     # For seasonal data (monthly with yearly pattern)\n\n# Bias detection\ntracking_signal(actual, predicted)  # |TS| > 4 indicates systematic bias\n\n# Prediction intervals\nlower = [90.0, 100.0, 99.0, 105.0, 110.0, 115.0, 120.0, 118.0]\nupper = [106.0, 116.0, 115.0, 121.0, 126.0, 131.0, 136.0, 134.0]\ncoverage_probability(actual, lower, upper)  # Should match confidence level\nwinkler_score(actual, lower, upper, alpha=0.05)  # Lower is better","category":"section"},{"location":"#Regression","page":"Home","title":"Regression","text":"using UnifiedMetrics\n\nactual = [1.0, 2.0, 3.0, 4.0, 5.0]\npredicted = [1.1, 2.1, 2.9, 4.2, 4.8]\n\nmae(actual, predicted)      # Mean Absolute Error\nrmse(actual, predicted)     # Root Mean Squared Error\nmape(actual, predicted)     # Mean Absolute Percentage Error","category":"section"},{"location":"#Classification","page":"Home","title":"Classification","text":"using UnifiedMetrics\n\nactual = [1, 1, 0, 0, 1, 0]\npredicted = [1, 0, 0, 1, 1, 0]\n\naccuracy(actual, predicted)   # Classification Accuracy\nprecision(actual, predicted)  # Precision\nrecall(actual, predicted)     # Recall\nfbeta_score(actual, predicted) # F1 Score","category":"section"},{"location":"#Documentation-Overview","page":"Home","title":"Documentation Overview","text":"Getting Started: Installation and basic usage\nChoosing the Right Metric: Comprehensive guide on which metric to use for your problem\nTime Series Forecasting: In-depth guide to evaluating forecasting models - scale-independent metrics, bias detection, prediction intervals, and more\nOther Metrics: Detailed documentation for regression, classification, binary classification, and information retrieval\nAPI Reference: Complete function reference","category":"section"},{"location":"#Why-UnifiedMetrics.jl?","page":"Home","title":"Why UnifiedMetrics.jl?","text":"Comprehensive: One package for all evaluation needs\nConsistent API: All metrics follow the same metric(actual, predicted) pattern\nWell-documented: Every function includes docstrings with examples\nPure Julia: No external dependencies beyond StatsBase\nProduction-ready: Handles edge cases gracefully\nForecasting-focused: Special emphasis on time series evaluation with M-competition recommended metrics","category":"section"},{"location":"#Contents","page":"Home","title":"Contents","text":"Pages = [\n    \"getting_started.md\",\n    \"choosing_metrics.md\",\n    \"time_series.md\",\n    \"regression.md\",\n    \"classification.md\",\n    \"binary_classification.md\",\n    \"information_retrieval.md\",\n    \"api.md\",\n]\nDepth = 2","category":"section"}]
}
