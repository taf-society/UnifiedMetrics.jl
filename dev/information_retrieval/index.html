<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Information Retrieval · UnifiedMetrics.jl</title><meta name="title" content="Information Retrieval · UnifiedMetrics.jl"/><meta property="og:title" content="Information Retrieval · UnifiedMetrics.jl"/><meta property="twitter:title" content="Information Retrieval · UnifiedMetrics.jl"/><meta name="description" content="Documentation for UnifiedMetrics.jl."/><meta property="og:description" content="Documentation for UnifiedMetrics.jl."/><meta property="twitter:description" content="Documentation for UnifiedMetrics.jl."/><meta property="og:url" content="https://taf-society.github.io/UnifiedMetrics.jl/information_retrieval/"/><meta property="twitter:url" content="https://taf-society.github.io/UnifiedMetrics.jl/information_retrieval/"/><link rel="canonical" href="https://taf-society.github.io/UnifiedMetrics.jl/information_retrieval/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">UnifiedMetrics.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../getting_started/">Getting Started</a></li><li><a class="tocitem" href="../choosing_metrics/">Choosing the Right Metric</a></li><li><a class="tocitem" href="../time_series/">Time Series Forecasting</a></li><li><span class="tocitem">Other Metrics</span><ul><li><a class="tocitem" href="../regression/">Regression</a></li><li><a class="tocitem" href="../classification/">Classification</a></li><li><a class="tocitem" href="../binary_classification/">Binary Classification</a></li><li class="is-active"><a class="tocitem" href>Information Retrieval</a><ul class="internal"><li><a class="tocitem" href="#Overview"><span>Overview</span></a></li><li><a class="tocitem" href="#Quick-Reference"><span>Quick Reference</span></a></li><li><a class="tocitem" href="#Relevance-Types"><span>Relevance Types</span></a></li><li><a class="tocitem" href="#Discounted-Cumulative-Gain-(DCG)-Family"><span>Discounted Cumulative Gain (DCG) Family</span></a></li><li><a class="tocitem" href="#Reciprocal-Rank-Metrics"><span>Reciprocal Rank Metrics</span></a></li><li><a class="tocitem" href="#Average-Precision"><span>Average Precision</span></a></li><li><a class="tocitem" href="#Set-Based-Retrieval-Metrics"><span>Set-Based Retrieval Metrics</span></a></li><li><a class="tocitem" href="#Hit-Rate"><span>Hit Rate</span></a></li><li><a class="tocitem" href="#Recommendation-System-Metrics"><span>Recommendation System Metrics</span></a></li><li><a class="tocitem" href="#Usage-Examples"><span>Usage Examples</span></a></li></ul></li></ul></li><li><a class="tocitem" href="../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Other Metrics</a></li><li class="is-active"><a href>Information Retrieval</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Information Retrieval</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/taf-society/UnifiedMetrics.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/taf-society/UnifiedMetrics.jl/blob/main/docs/src/information_retrieval.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Information-Retrieval-Metrics"><a class="docs-heading-anchor" href="#Information-Retrieval-Metrics">Information Retrieval Metrics</a><a id="Information-Retrieval-Metrics-1"></a><a class="docs-heading-anchor-permalink" href="#Information-Retrieval-Metrics" title="Permalink"></a></h1><p>Metrics for evaluating search engines, recommendation systems, and ranking models.</p><h2 id="Overview"><a class="docs-heading-anchor" href="#Overview">Overview</a><a id="Overview-1"></a><a class="docs-heading-anchor-permalink" href="#Overview" title="Permalink"></a></h2><p>Information retrieval metrics evaluate how well a system ranks or retrieves relevant items. They&#39;re essential for:</p><ul><li>Search engines</li><li>Recommendation systems</li><li>Question answering</li><li>Document retrieval</li></ul><h2 id="Quick-Reference"><a class="docs-heading-anchor" href="#Quick-Reference">Quick Reference</a><a id="Quick-Reference-1"></a><a class="docs-heading-anchor-permalink" href="#Quick-Reference" title="Permalink"></a></h2><table><tr><th style="text-align: right">Metric</th><th style="text-align: right">Input</th><th style="text-align: right">Range</th><th style="text-align: right">Best For</th></tr><tr><td style="text-align: right"><code>ndcg</code></td><td style="text-align: right">Graded relevance</td><td style="text-align: right">[0, 1]</td><td style="text-align: right">Search ranking</td></tr><tr><td style="text-align: right"><code>mrr</code></td><td style="text-align: right">Binary relevance</td><td style="text-align: right">[0, 1]</td><td style="text-align: right">Finding first result</td></tr><tr><td style="text-align: right"><code>mapk</code></td><td style="text-align: right">Binary relevance</td><td style="text-align: right">[0, 1]</td><td style="text-align: right">Overall ranking quality</td></tr><tr><td style="text-align: right"><code>hit_rate</code></td><td style="text-align: right">Binary relevance</td><td style="text-align: right">[0, 1]</td><td style="text-align: right">Recommendations</td></tr><tr><td style="text-align: right"><code>precision_at_k</code></td><td style="text-align: right">Binary relevance</td><td style="text-align: right">[0, 1]</td><td style="text-align: right">Top-k quality</td></tr></table><h2 id="Relevance-Types"><a class="docs-heading-anchor" href="#Relevance-Types">Relevance Types</a><a id="Relevance-Types-1"></a><a class="docs-heading-anchor-permalink" href="#Relevance-Types" title="Permalink"></a></h2><h3 id="Binary-Relevance"><a class="docs-heading-anchor" href="#Binary-Relevance">Binary Relevance</a><a id="Binary-Relevance-1"></a><a class="docs-heading-anchor-permalink" href="#Binary-Relevance" title="Permalink"></a></h3><p>Items are either relevant (1) or not (0).</p><pre><code class="language-julia hljs">actual_relevant = [&quot;doc1&quot;, &quot;doc3&quot;, &quot;doc5&quot;]  # Relevant documents
retrieved = [&quot;doc1&quot;, &quot;doc2&quot;, &quot;doc3&quot;]         # What the system returned</code></pre><h3 id="Graded-Relevance"><a class="docs-heading-anchor" href="#Graded-Relevance">Graded Relevance</a><a id="Graded-Relevance-1"></a><a class="docs-heading-anchor-permalink" href="#Graded-Relevance" title="Permalink"></a></h3><p>Items have relevance scores (e.g., 0-5).</p><pre><code class="language-julia hljs">relevance = [3, 2, 1, 0, 2]  # Scores for items in ranked order</code></pre><h2 id="Discounted-Cumulative-Gain-(DCG)-Family"><a class="docs-heading-anchor" href="#Discounted-Cumulative-Gain-(DCG)-Family">Discounted Cumulative Gain (DCG) Family</a><a id="Discounted-Cumulative-Gain-(DCG)-Family-1"></a><a class="docs-heading-anchor-permalink" href="#Discounted-Cumulative-Gain-(DCG)-Family" title="Permalink"></a></h2><h3 id="DCG"><a class="docs-heading-anchor" href="#DCG">DCG</a><a id="DCG-1"></a><a class="docs-heading-anchor-permalink" href="#DCG" title="Permalink"></a></h3><pre><code class="language-julia hljs">dcg(relevance)</code></pre><p><strong>How it works</strong>: Sums relevance scores with logarithmic discount by position.</p><ul><li>Items at top positions contribute more</li><li>Formula: Σ (2^rel - 1) / log₂(i + 1)</li></ul><h3 id="IDCG-(Ideal-DCG)"><a class="docs-heading-anchor" href="#IDCG-(Ideal-DCG)">IDCG (Ideal DCG)</a><a id="IDCG-(Ideal-DCG)-1"></a><a class="docs-heading-anchor-permalink" href="#IDCG-(Ideal-DCG)" title="Permalink"></a></h3><pre><code class="language-julia hljs">idcg(relevance)</code></pre><p><strong>How it works</strong>: DCG of the best possible ranking (sorted by relevance descending).</p><h3 id="NDCG-(Normalized-DCG)"><a class="docs-heading-anchor" href="#NDCG-(Normalized-DCG)">NDCG (Normalized DCG)</a><a id="NDCG-(Normalized-DCG)-1"></a><a class="docs-heading-anchor-permalink" href="#NDCG-(Normalized-DCG)" title="Permalink"></a></h3><pre><code class="language-julia hljs">ndcg(relevance; k=nothing)</code></pre><p><strong>Interpretation</strong>:</p><ul><li>NDCG = 1: Perfect ranking</li><li>NDCG = 0: No relevant items retrieved</li></ul><p><strong>When to use</strong>:</p><ul><li>Search engine evaluation</li><li>When relevance is graded (not binary)</li><li>When position matters</li></ul><h3 id="Mean-NDCG"><a class="docs-heading-anchor" href="#Mean-NDCG">Mean NDCG</a><a id="Mean-NDCG-1"></a><a class="docs-heading-anchor-permalink" href="#Mean-NDCG" title="Permalink"></a></h3><pre><code class="language-julia hljs">mean_ndcg(relevances; k=nothing)</code></pre><p><strong>When to use</strong>: Evaluating across multiple queries.</p><h2 id="Reciprocal-Rank-Metrics"><a class="docs-heading-anchor" href="#Reciprocal-Rank-Metrics">Reciprocal Rank Metrics</a><a id="Reciprocal-Rank-Metrics-1"></a><a class="docs-heading-anchor-permalink" href="#Reciprocal-Rank-Metrics" title="Permalink"></a></h2><h3 id="Reciprocal-Rank"><a class="docs-heading-anchor" href="#Reciprocal-Rank">Reciprocal Rank</a><a id="Reciprocal-Rank-1"></a><a class="docs-heading-anchor-permalink" href="#Reciprocal-Rank" title="Permalink"></a></h3><pre><code class="language-julia hljs">reciprocal_rank(actual, predicted)</code></pre><p><strong>How it works</strong>: 1/position of first relevant item.</p><h3 id="Mean-Reciprocal-Rank-(MRR)"><a class="docs-heading-anchor" href="#Mean-Reciprocal-Rank-(MRR)">Mean Reciprocal Rank (MRR)</a><a id="Mean-Reciprocal-Rank-(MRR)-1"></a><a class="docs-heading-anchor-permalink" href="#Mean-Reciprocal-Rank-(MRR)" title="Permalink"></a></h3><pre><code class="language-julia hljs">mrr(actual_list, predicted_list)</code></pre><p><strong>When to use</strong>:</p><ul><li>Question answering systems</li><li>When only the first relevant result matters</li><li>Voice assistants, &quot;I&#39;m Feeling Lucky&quot; searches</li></ul><p><strong>Interpretation</strong>:</p><ul><li>MRR = 1: First result always relevant</li><li>MRR = 0.5: First relevant result is typically at position 2</li></ul><h2 id="Average-Precision"><a class="docs-heading-anchor" href="#Average-Precision">Average Precision</a><a id="Average-Precision-1"></a><a class="docs-heading-anchor-permalink" href="#Average-Precision" title="Permalink"></a></h2><h3 id="AP@K"><a class="docs-heading-anchor" href="#AP@K">AP@K</a><a id="AP@K-1"></a><a class="docs-heading-anchor-permalink" href="#AP@K" title="Permalink"></a></h3><pre><code class="language-julia hljs">apk(k, actual, predicted)</code></pre><p><strong>How it works</strong>: Average precision at each position where a relevant item is found.</p><h3 id="MAP@K-(Mean-Average-Precision)"><a class="docs-heading-anchor" href="#MAP@K-(Mean-Average-Precision)">MAP@K (Mean Average Precision)</a><a id="MAP@K-(Mean-Average-Precision)-1"></a><a class="docs-heading-anchor-permalink" href="#MAP@K-(Mean-Average-Precision)" title="Permalink"></a></h3><pre><code class="language-julia hljs">mapk(k, actual_list, predicted_list)</code></pre><p><strong>When to use</strong>:</p><ul><li>Standard metric for document retrieval</li><li>When both precision and recall matter</li><li>Benchmark datasets (TREC, MS MARCO)</li></ul><h2 id="Set-Based-Retrieval-Metrics"><a class="docs-heading-anchor" href="#Set-Based-Retrieval-Metrics">Set-Based Retrieval Metrics</a><a id="Set-Based-Retrieval-Metrics-1"></a><a class="docs-heading-anchor-permalink" href="#Set-Based-Retrieval-Metrics" title="Permalink"></a></h2><h3 id="F1-Score-(IR-Context)"><a class="docs-heading-anchor" href="#F1-Score-(IR-Context)">F1 Score (IR Context)</a><a id="F1-Score-(IR-Context)-1"></a><a class="docs-heading-anchor-permalink" href="#F1-Score-(IR-Context)" title="Permalink"></a></h3><pre><code class="language-julia hljs">f1(actual, predicted)</code></pre><h3 id="Precision@K"><a class="docs-heading-anchor" href="#Precision@K">Precision@K</a><a id="Precision@K-1"></a><a class="docs-heading-anchor-permalink" href="#Precision@K" title="Permalink"></a></h3><pre><code class="language-julia hljs">precision_at_k(actual, predicted; k)</code></pre><p><strong>Interpretation</strong>: Of the top K results, what fraction are relevant?</p><h3 id="Recall@K"><a class="docs-heading-anchor" href="#Recall@K">Recall@K</a><a id="Recall@K-1"></a><a class="docs-heading-anchor-permalink" href="#Recall@K" title="Permalink"></a></h3><pre><code class="language-julia hljs">recall_at_k(actual, predicted; k)</code></pre><p><strong>Interpretation</strong>: Of all relevant items, what fraction appear in top K?</p><h3 id="F1@K"><a class="docs-heading-anchor" href="#F1@K">F1@K</a><a id="F1@K-1"></a><a class="docs-heading-anchor-permalink" href="#F1@K" title="Permalink"></a></h3><pre><code class="language-julia hljs">f1_at_k(actual, predicted; k)</code></pre><h2 id="Hit-Rate"><a class="docs-heading-anchor" href="#Hit-Rate">Hit Rate</a><a id="Hit-Rate-1"></a><a class="docs-heading-anchor-permalink" href="#Hit-Rate" title="Permalink"></a></h2><pre><code class="language-julia hljs">hit_rate(actual_list, predicted_list; k)</code></pre><p><strong>When to use</strong>:</p><ul><li>Recommendation systems</li><li>When showing at least one good item is success</li><li>E-commerce, media streaming</li></ul><h2 id="Recommendation-System-Metrics"><a class="docs-heading-anchor" href="#Recommendation-System-Metrics">Recommendation System Metrics</a><a id="Recommendation-System-Metrics-1"></a><a class="docs-heading-anchor-permalink" href="#Recommendation-System-Metrics" title="Permalink"></a></h2><h3 id="Coverage"><a class="docs-heading-anchor" href="#Coverage">Coverage</a><a id="Coverage-1"></a><a class="docs-heading-anchor-permalink" href="#Coverage" title="Permalink"></a></h3><pre><code class="language-julia hljs">coverage(recommendations, catalog)</code></pre><p><strong>Interpretation</strong>: What fraction of the catalog gets recommended?</p><ul><li>High coverage: Diverse recommendations</li><li>Low coverage: Recommendations focus on popular items</li></ul><h3 id="Novelty"><a class="docs-heading-anchor" href="#Novelty">Novelty</a><a id="Novelty-1"></a><a class="docs-heading-anchor-permalink" href="#Novelty" title="Permalink"></a></h3><pre><code class="language-julia hljs">novelty(recommendations, popularity)</code></pre><p><strong>Interpretation</strong>: Are we recommending non-obvious items?</p><ul><li>High novelty: Recommending less popular (&quot;long-tail&quot;) items</li><li>Low novelty: Recommending already-popular items</li></ul><h2 id="Usage-Examples"><a class="docs-heading-anchor" href="#Usage-Examples">Usage Examples</a><a id="Usage-Examples-1"></a><a class="docs-heading-anchor-permalink" href="#Usage-Examples" title="Permalink"></a></h2><h3 id="Search-Engine-Evaluation"><a class="docs-heading-anchor" href="#Search-Engine-Evaluation">Search Engine Evaluation</a><a id="Search-Engine-Evaluation-1"></a><a class="docs-heading-anchor-permalink" href="#Search-Engine-Evaluation" title="Permalink"></a></h3><pre><code class="language-julia hljs">using UnifiedMetrics

# Graded relevance scores for top 6 results
# 3 = highly relevant, 2 = relevant, 1 = marginally relevant, 0 = not relevant
relevance = [3, 2, 1, 0, 2, 1]

println(&quot;DCG: &quot;, round(dcg(relevance), digits=3))
println(&quot;NDCG: &quot;, round(ndcg(relevance), digits=3))
println(&quot;NDCG@3: &quot;, round(ndcg(relevance, k=3), digits=3))

# Multiple queries
relevances = [
    [3, 2, 1, 0],    # Query 1
    [0, 1, 2, 3],    # Query 2 (poor ranking)
    [3, 3, 2, 1],    # Query 3 (good ranking)
]
println(&quot;Mean NDCG: &quot;, round(mean_ndcg(relevances), digits=3))
println(&quot;Mean NDCG@2: &quot;, round(mean_ndcg(relevances, k=2), digits=3))</code></pre><h3 id="Document-Retrieval-Evaluation"><a class="docs-heading-anchor" href="#Document-Retrieval-Evaluation">Document Retrieval Evaluation</a><a id="Document-Retrieval-Evaluation-1"></a><a class="docs-heading-anchor-permalink" href="#Document-Retrieval-Evaluation" title="Permalink"></a></h3><pre><code class="language-julia hljs">using UnifiedMetrics

# Multiple queries
actual_relevant = [
    [&quot;doc1&quot;, &quot;doc5&quot;, &quot;doc7&quot;],           # Relevant docs for query 1
    [&quot;doc2&quot;, &quot;doc3&quot;],                    # Relevant docs for query 2
    [&quot;doc4&quot;, &quot;doc6&quot;, &quot;doc8&quot;, &quot;doc9&quot;],   # Relevant docs for query 3
]

retrieved = [
    [&quot;doc1&quot;, &quot;doc2&quot;, &quot;doc5&quot;, &quot;doc3&quot;],   # Retrieved for query 1
    [&quot;doc1&quot;, &quot;doc2&quot;, &quot;doc4&quot;],           # Retrieved for query 2
    [&quot;doc4&quot;, &quot;doc5&quot;, &quot;doc6&quot;, &quot;doc7&quot;],   # Retrieved for query 3
]

println(&quot;MAP@3: &quot;, round(mapk(3, actual_relevant, retrieved), digits=3))
println(&quot;MRR: &quot;, round(mrr(actual_relevant, retrieved), digits=3))</code></pre><h3 id="Recommendation-System-Evaluation"><a class="docs-heading-anchor" href="#Recommendation-System-Evaluation">Recommendation System Evaluation</a><a id="Recommendation-System-Evaluation-1"></a><a class="docs-heading-anchor-permalink" href="#Recommendation-System-Evaluation" title="Permalink"></a></h3><pre><code class="language-julia hljs">using UnifiedMetrics

# User-item recommendations
actual_liked = [
    [&quot;item_a&quot;, &quot;item_c&quot;],              # User 1&#39;s liked items
    [&quot;item_b&quot;, &quot;item_d&quot;, &quot;item_e&quot;],    # User 2&#39;s liked items
    [&quot;item_a&quot;, &quot;item_f&quot;],              # User 3&#39;s liked items
]

recommended = [
    [&quot;item_a&quot;, &quot;item_b&quot;, &quot;item_g&quot;, &quot;item_c&quot;, &quot;item_h&quot;],
    [&quot;item_x&quot;, &quot;item_d&quot;, &quot;item_y&quot;, &quot;item_b&quot;, &quot;item_z&quot;],
    [&quot;item_f&quot;, &quot;item_a&quot;, &quot;item_m&quot;, &quot;item_n&quot;, &quot;item_o&quot;],
]

println(&quot;=== Recommendation Quality ===&quot;)
println(&quot;Hit Rate@3: &quot;, round(hit_rate(actual_liked, recommended, k=3), digits=3))
println(&quot;Hit Rate@5: &quot;, round(hit_rate(actual_liked, recommended, k=5), digits=3))
println(&quot;MRR: &quot;, round(mrr(actual_liked, recommended), digits=3))
println(&quot;MAP@5: &quot;, round(mapk(5, actual_liked, recommended), digits=3))

# Per-user metrics
for (i, (act, rec)) in enumerate(zip(actual_liked, recommended))
    println(&quot;User $i - P@3: $(round(precision_at_k(act, rec, k=3), digits=2)), &quot;,
            &quot;R@3: $(round(recall_at_k(act, rec, k=3), digits=2))&quot;)
end</code></pre><h3 id="Evaluating-Recommendation-Diversity"><a class="docs-heading-anchor" href="#Evaluating-Recommendation-Diversity">Evaluating Recommendation Diversity</a><a id="Evaluating-Recommendation-Diversity-1"></a><a class="docs-heading-anchor-permalink" href="#Evaluating-Recommendation-Diversity" title="Permalink"></a></h3><pre><code class="language-julia hljs">using UnifiedMetrics

# Full catalog of items
catalog = [&quot;item_&quot; * string(i) for i in 1:100]

# Recommendations for 50 users
recommendations = [[&quot;item_1&quot;, &quot;item_2&quot;, &quot;item_3&quot;, &quot;item_5&quot;, &quot;item_10&quot;],
                   [&quot;item_1&quot;, &quot;item_3&quot;, &quot;item_7&quot;, &quot;item_12&quot;, &quot;item_15&quot;],
                   # ... more users
                  ]

# What fraction of catalog was recommended?
cov = coverage(recommendations, catalog)
println(&quot;Catalog Coverage: $(round(cov*100, digits=1))%&quot;)

# Novelty (recommending less popular items)
popularity = Dict(&quot;item_$i&quot; =&gt; 1.0/i for i in 1:100)  # Power law popularity
nov = novelty(recommendations, popularity)
println(&quot;Novelty: &quot;, round(nov, digits=2))</code></pre><h3 id="Comparing-Ranking-Models"><a class="docs-heading-anchor" href="#Comparing-Ranking-Models">Comparing Ranking Models</a><a id="Comparing-Ranking-Models-1"></a><a class="docs-heading-anchor-permalink" href="#Comparing-Ranking-Models" title="Permalink"></a></h3><pre><code class="language-julia hljs">using UnifiedMetrics

# Ground truth relevance for 3 queries
actual = [
    [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;],
    [&quot;d&quot;, &quot;e&quot;],
    [&quot;f&quot;, &quot;g&quot;, &quot;h&quot;, &quot;i&quot;],
]

# Model A&#39;s rankings
model_a = [
    [&quot;a&quot;, &quot;x&quot;, &quot;b&quot;, &quot;y&quot;, &quot;c&quot;],
    [&quot;d&quot;, &quot;z&quot;, &quot;e&quot;, &quot;w&quot;],
    [&quot;f&quot;, &quot;g&quot;, &quot;x&quot;, &quot;h&quot;, &quot;i&quot;],
]

# Model B&#39;s rankings
model_b = [
    [&quot;x&quot;, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;y&quot;],
    [&quot;e&quot;, &quot;d&quot;, &quot;z&quot;, &quot;w&quot;],
    [&quot;x&quot;, &quot;y&quot;, &quot;f&quot;, &quot;g&quot;, &quot;h&quot;],
]

println(&quot;=== Model Comparison ===&quot;)
for (name, model) in [(&quot;Model A&quot;, model_a), (&quot;Model B&quot;, model_b)]
    println(&quot;$name:&quot;)
    println(&quot;  MAP@3: $(round(mapk(3, actual, model), digits=3))&quot;)
    println(&quot;  MRR: $(round(mrr(actual, model), digits=3))&quot;)
    println(&quot;  Hit Rate@3: $(round(hit_rate(actual, model, k=3), digits=3))&quot;)
end</code></pre><p>See the <a href="../api/#API-Reference">API Reference</a> for complete function documentation.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../binary_classification/">« Binary Classification</a><a class="docs-footer-nextpage" href="../api/">API Reference »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Friday 30 January 2026 11:30">Friday 30 January 2026</span>. Using Julia version 1.11.8.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
